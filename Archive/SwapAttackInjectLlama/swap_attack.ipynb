{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c0de5e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import gc\n",
    "from collections import defaultdict\n",
    "from typing import List, Dict, Any, Optional, Tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ed85bda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import login\n",
    "# login()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "996ac4a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device('cpu')#'cuda' if torch.cuda.is_available() else 'cpu')\n",
    "MODEL_PATH = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c302d7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = LlamaTokenizer.from_pretrained(MODEL_PATH)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2680ffe1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adf71357d37343f59ade3d97fe88d8b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=DEVICE\n",
    ")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6d95dd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Distance Metrics for Comparing Logit Distributions\n",
    "# =============================================================================\n",
    "\n",
    "def compute_l2_distance(original_logits: torch.Tensor, perturbed_logits: torch.Tensor) -> float:\n",
    "    # Compute L2 (Euclidean) distance between two logit vectors\n",
    "    return torch.norm(original_logits - perturbed_logits, p=2).item()\n",
    "\n",
    "def compute_cosine_distance(original_logits: torch.Tensor, perturbed_logits: torch.Tensor) -> float:\n",
    "    # Compute cosine distance (1 - cosine_similarity) between two logit vectors\n",
    "    cos_sim = F.cosine_similarity(original_logits.unsqueeze(0), perturbed_logits.unsqueeze(0))\n",
    "    return (1 - cos_sim).item()\n",
    "\n",
    "def compute_kl_divergence(original_logits: torch.Tensor, perturbed_logits: torch.Tensor) -> float:\n",
    "    # Compute KL divergence: KL(original || perturbed) after softmax\n",
    "    original_probs = F.softmax(original_logits, dim=-1)\n",
    "    perturbed_log_probs = F.log_softmax(perturbed_logits, dim=-1)\n",
    "    # KL(P || Q) = sum(P * log(P/Q)) = sum(P * (log_P - log_Q))\n",
    "    kl_div = F.kl_div(perturbed_log_probs, original_probs, reduction='sum')\n",
    "    return kl_div.item()\n",
    "\n",
    "def compute_js_divergence(original_logits: torch.Tensor, perturbed_logits: torch.Tensor) -> float:\n",
    "   # Compute Jensen-Shannon divergence: 0.5*KL(P||M) + 0.5*KL(Q||M) where M = 0.5*(P+Q).\n",
    "    P = F.softmax(original_logits, dim=-1)\n",
    "    Q = F.softmax(perturbed_logits, dim=-1)\n",
    "    M = 0.5 * (P + Q)\n",
    "    \n",
    "    # KL(P || M)\n",
    "    kl_pm = F.kl_div(M.log(), P, reduction='sum')\n",
    "    # KL(Q || M)\n",
    "    kl_qm = F.kl_div(M.log(), Q, reduction='sum')\n",
    "    \n",
    "    js_div = 0.5 * (kl_pm + kl_qm)\n",
    "    return js_div.item()\n",
    "\n",
    "def compute_all_distances(original_logits: torch.Tensor, perturbed_logits: torch.Tensor) -> Dict[str, float]:\n",
    "    # Compute all distance metrics between original and perturbed logits.\n",
    "    return {\n",
    "        'l2_distance': compute_l2_distance(original_logits, perturbed_logits),\n",
    "        'cosine_distance': compute_cosine_distance(original_logits, perturbed_logits),\n",
    "        'kl_divergence': compute_kl_divergence(original_logits, perturbed_logits),\n",
    "        'js_divergence': compute_js_divergence(original_logits, perturbed_logits),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "784f29b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables for detailed activation capture\n",
    "captured_activations = {}\n",
    "current_hooks = []\n",
    "hook_errors = []\n",
    "\n",
    "def clear_activations():\n",
    "    global captured_activations\n",
    "    captured_activations.clear()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "def remove_all_hooks():\n",
    "    global current_hooks\n",
    "    for hook in current_hooks:\n",
    "        try:\n",
    "            hook.remove()\n",
    "        except:\n",
    "            pass\n",
    "    current_hooks.clear()\n",
    "\n",
    "def get_activation_hook(name):\n",
    "    def hook(module, input, output):\n",
    "        global hook_errors\n",
    "        try:\n",
    "            # Handle different output types\n",
    "            if output is None:\n",
    "                activation = None\n",
    "            elif isinstance(output, tuple):\n",
    "                activation = output[0]\n",
    "            elif hasattr(output, 'last_hidden_state'):\n",
    "                # Handle model output objects\n",
    "                activation = output.last_hidden_state\n",
    "            else:\n",
    "                activation = output\n",
    "            \n",
    "            # Handle input\n",
    "            input_tensor = input[0] if isinstance(input, tuple) and len(input) > 0 else None\n",
    "\n",
    "            # Safely detach and move to CPU\n",
    "            def safe_detach_cpu(tensor):\n",
    "                if tensor is None:\n",
    "                    return None\n",
    "                try:\n",
    "                    # Check if tensor is on meta device\n",
    "                    if hasattr(tensor, 'device') and str(tensor.device) == 'meta':\n",
    "                        return None\n",
    "                    return tensor.detach().cpu()\n",
    "                except Exception as e:\n",
    "                    hook_errors.append(f\"Detach error in {name}: {str(e)}\")\n",
    "                    return None\n",
    "\n",
    "            captured_activations[name] = {\n",
    "                'output': safe_detach_cpu(activation),\n",
    "                'input': safe_detach_cpu(input_tensor),\n",
    "                'weight': safe_detach_cpu(module.weight) if hasattr(module, 'weight') and module.weight is not None else None,\n",
    "                'bias': safe_detach_cpu(module.bias) if hasattr(module, 'bias') and module.bias is not None else None\n",
    "            }\n",
    "        except Exception as e:\n",
    "            error_msg = f\"Hook error in {name}: {str(e)}\"\n",
    "            hook_errors.append(error_msg)\n",
    "            captured_activations[name] = {'output': None, 'input': None, 'weight': None, 'bias': None}\n",
    "    return hook\n",
    "\n",
    "def register_llama_hooks(model):\n",
    "    global current_hooks\n",
    "    remove_all_hooks() # clear any old hooks first\n",
    "    hook_errors.clear()\n",
    "\n",
    "    total_layers = len(model.model.layers)\n",
    "\n",
    "    for i in range(total_layers):\n",
    "        layer = model.model.layers[i]\n",
    "        layer_prefix = f\"layer_{i}\"\n",
    "        components = [\n",
    "            (layer.self_attn.q_proj, f\"{layer_prefix}_attention_q\"), (layer.self_attn.k_proj, f\"{layer_prefix}_attention_k\"),\n",
    "            (layer.self_attn.v_proj, f\"{layer_prefix}_attention_v\"), (layer.self_attn.o_proj, f\"{layer_prefix}_attention_output\"),\n",
    "            (layer.mlp.gate_proj, f\"{layer_prefix}_mlp_gate\"), (layer.mlp.up_proj, f\"{layer_prefix}_mlp_up\"),\n",
    "            (layer.mlp.down_proj, f\"{layer_prefix}_mlp_down\"), (layer.input_layernorm, f\"{layer_prefix}_input_norm\"),\n",
    "            (layer.post_attention_layernorm, f\"{layer_prefix}_post_attn_norm\"),\n",
    "        ]\n",
    "        for module, name in components:\n",
    "            current_hooks.append(module.register_forward_hook(get_activation_hook(name)))\n",
    "    \n",
    "    current_hooks.append(model.model.norm.register_forward_hook(get_activation_hook(\"final_norm\")))\n",
    "    current_hooks.append(model.lm_head.register_forward_hook(get_activation_hook(\"lm_head\")))\n",
    "    print(f\"Registered {len(current_hooks)} hooks.\")\n",
    "\n",
    "def run_model_and_capture_activations(model, inputs=None, inputs_embeds=None):\n",
    "    global hook_errors\n",
    "    clear_activations()\n",
    "    register_llama_hooks(model)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        if inputs is not None:\n",
    "            _ = model(**inputs)\n",
    "        elif inputs_embeds is not None:\n",
    "            _ = model(inputs_embeds=inputs_embeds)\n",
    "        else:\n",
    "            raise ValueError(\"Either inputs or inputs_embeds must be provided.\")\n",
    "            \n",
    "    remove_all_hooks()\n",
    "    \n",
    "    # Print any hook errors that occurred\n",
    "    if hook_errors:\n",
    "        print(f\"WARNING: {len(hook_errors)} hook errors occurred:\")\n",
    "        for err in hook_errors[:5]:\n",
    "            print(f\"  - {err}\")\n",
    "        if len(hook_errors) > 5:\n",
    "            print(f\"  ... and {len(hook_errors) - 5} more\")\n",
    "    \n",
    "    # return a copy of the captured activations\n",
    "    return captured_activations.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5c5a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neuron Perturbation Sensitivity Analysis\n",
    "\n",
    "def get_top_k_predictions(logits: torch.Tensor, tokenizer, k: int = 3) -> Dict[str, Any]:\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    top_logits, top_indices = torch.topk(logits, k)\n",
    "    top_probs = probs[top_indices]\n",
    "    \n",
    "    result = {}\n",
    "    for i in range(k):\n",
    "        idx = top_indices[i].item()\n",
    "        word = tokenizer.decode([idx])\n",
    "        result[f'top{i+1}_word'] = word\n",
    "        result[f'top{i+1}_index'] = idx\n",
    "        result[f'top{i+1}_logit'] = top_logits[i].item()\n",
    "        result[f'top{i+1}_softmax'] = top_probs[i].item()\n",
    "    \n",
    "    return result\n",
    "\n",
    "def find_boundary_delta_binary_search(\n",
    "    final_norm_activations: torch.Tensor,\n",
    "    original_last_logits: torch.Tensor,\n",
    "    neuron_indices: List[int],\n",
    "    last_token_pos: int,\n",
    "    lm_head_weight: torch.Tensor,\n",
    "    lm_head_bias: Optional[torch.Tensor],\n",
    "    distance_threshold: float,\n",
    "    distance_metric: str,\n",
    "    tokenizer,  # Added to compute top3 predictions\n",
    "    delta_min: float = 0.0,\n",
    "    delta_max: float = 100.0,\n",
    "    tolerance: float = 0.001,\n",
    ") -> Tuple[float, Dict[str, float], torch.Tensor, List[Dict]]:\n",
    "    \n",
    "    # Binary search to find the minimum delta where distance >= threshold.\n",
    "    # Returns (boundary_delta, distances_at_boundary, perturbed_logits_at_boundary, search_history)\n",
    "    \n",
    "    \n",
    "    low, high = delta_min, delta_max\n",
    "    best_delta = delta_max\n",
    "    best_distances = None\n",
    "    best_logits = None\n",
    "    search_history = []  # Track all (delta, distance) pairs\n",
    "    step = 0\n",
    "    \n",
    "    while high - low > tolerance:\n",
    "        mid = (low + high) / 2\n",
    "        step += 1\n",
    "        \n",
    "        # Apply perturbation with delta=mid\n",
    "        perturbed_activations = final_norm_activations.clone()\n",
    "        for idx in neuron_indices:\n",
    "            perturbed_activations[0, last_token_pos, idx] += mid\n",
    "        \n",
    "        # Compute logits\n",
    "        perturbed_last = perturbed_activations[0, last_token_pos, :].to(lm_head_weight.device)\n",
    "        perturbed_logits = F.linear(perturbed_last, lm_head_weight, lm_head_bias).float()\n",
    "        \n",
    "        # Compute distance\n",
    "        distances = compute_all_distances(original_last_logits.to(perturbed_logits.device), perturbed_logits)\n",
    "        current_distance = distances[distance_metric]\n",
    "        \n",
    "        # Get top-3 predictions for perturbed logits\n",
    "        perturbed_top3 = get_top_k_predictions(perturbed_logits, tokenizer, k=3)\n",
    "        perturbed_top3_prefixed = {f'pert_{key}': val for key, val in perturbed_top3.items()}\n",
    "        \n",
    "        # Record this step in search history\n",
    "        search_history.append({\n",
    "            'search_step': step,\n",
    "            'delta_tried': mid,\n",
    "            'exceeded_threshold': current_distance >= distance_threshold,\n",
    "            **distances,\n",
    "            **perturbed_top3_prefixed,\n",
    "        })\n",
    "        \n",
    "        if current_distance >= distance_threshold:\n",
    "            # Found a valid delta, try to find smaller\n",
    "            best_delta = mid\n",
    "            best_distances = distances\n",
    "            best_logits = perturbed_logits\n",
    "            high = mid\n",
    "        else:\n",
    "            # Need larger delta\n",
    "            low = mid\n",
    "    \n",
    "    return best_delta, best_distances, best_logits, search_history\n",
    "\n",
    "def analyze_neuron_perturbation_sensitivity(\n",
    "    model: \"LlamaForCausalLM\",\n",
    "    tokenizer: \"LlamaTokenizer\",\n",
    "    final_norm_activations: torch.Tensor,\n",
    "    original_logits: torch.Tensor,\n",
    "    k_values: List[int],\n",
    "    distance_threshold: float = 0.1,\n",
    "    distance_metric: str = \"js_divergence\",\n",
    "    delta_max: float = 100.0,\n",
    "    tolerance: float = 0.001,  # Binary search precision\n",
    "    num_trials_per_k: int = 10,\n",
    "    save_every: int = 1000,\n",
    "    input_text: str = \"\",\n",
    "    input_id: int = 0,\n",
    "    filename: str = \"neuron_perturbation_analysis_other.csv\",\n",
    ") -> Dict[int, List[float]]:\n",
    "    # For each K and each trial, find minimum delta where distance >= threshold using binary search.\n",
    "    # Returns: {K: [boundary_delta_trial_0, boundary_delta_trial_1, ...]}\n",
    "    \n",
    "    results = []\n",
    "    total_saved = 0\n",
    "    boundary_deltas = {}\n",
    "    \n",
    "    # Get dimensions\n",
    "    seq_len = final_norm_activations.shape[1]\n",
    "    hidden_size = final_norm_activations.shape[2] \n",
    "    last_token_pos = seq_len - 1\n",
    "    \n",
    "    # Get original logits for the last token\n",
    "    original_last_logits = original_logits[0, last_token_pos, :].float()\n",
    "    \n",
    "    # Get top-3 predictions for original logits\n",
    "    original_top3 = get_top_k_predictions(original_last_logits, tokenizer, k=3)\n",
    "    original_top3_prefixed = {f'orig_{key}': val for key, val in original_top3.items()}\n",
    "    \n",
    "    # Get lm_head weights\n",
    "    lm_head_weight = model.lm_head.weight.detach()\n",
    "    lm_head_bias = model.lm_head.bias.detach() if hasattr(model.lm_head, 'bias') and model.lm_head.bias is not None else None\n",
    "    \n",
    "    # Helper to save batch\n",
    "    def save_batch(batch):\n",
    "        nonlocal total_saved\n",
    "        if not batch:\n",
    "            return\n",
    "        df = pd.DataFrame(batch)\n",
    "        df.insert(0, 'input_id', input_id)\n",
    "        file_exists = os.path.exists(filename)\n",
    "        df.to_csv(filename, mode='a', header=not file_exists, index=False)\n",
    "        total_saved += len(batch)\n",
    "        print(f\"[Checkpoint] Saved {len(batch)} rows (total: {total_saved})\")\n",
    "    \n",
    "    # For each K value\n",
    "    for k in k_values:\n",
    "        if k > hidden_size:\n",
    "            print(f\"Warning: K={k} > hidden_size={hidden_size}, skipping.\")\n",
    "            continue\n",
    "        \n",
    "        boundary_deltas[k] = []\n",
    "        \n",
    "        for trial in range(num_trials_per_k):\n",
    "            # Randomly select K neurons (fixed for this trial)\n",
    "            neuron_indices = torch.randperm(hidden_size)[:k].tolist()\n",
    "            \n",
    "            # Binary search for boundary delta\n",
    "            boundary_delta, distances, perturbed_logits, search_history = find_boundary_delta_binary_search(\n",
    "                final_norm_activations=final_norm_activations,\n",
    "                original_last_logits=original_last_logits,\n",
    "                neuron_indices=neuron_indices,\n",
    "                last_token_pos=last_token_pos,\n",
    "                lm_head_weight=lm_head_weight,\n",
    "                lm_head_bias=lm_head_bias,\n",
    "                distance_threshold=distance_threshold,\n",
    "                distance_metric=distance_metric,\n",
    "                tokenizer=tokenizer,\n",
    "                delta_min=delta_max*-0.1,\n",
    "                delta_max=delta_max,\n",
    "                tolerance=tolerance,\n",
    "            )\n",
    "            \n",
    "            boundary_deltas[k].append(boundary_delta)\n",
    "            \n",
    "            # Record all search steps (each step is a row)\n",
    "            for step_data in search_history:\n",
    "                result = {\n",
    "                    'num_neurons_changed': k,\n",
    "                    'trial': trial,\n",
    "                    'boundary_delta': boundary_delta,  # Final boundary\n",
    "                    'neuron_indices': str(neuron_indices),\n",
    "                    'total_neurons': hidden_size,\n",
    "                    'soundness_ratio': k / hidden_size,\n",
    "                    **step_data,  # search_step, delta_tried, exceeded_threshold, all distances\n",
    "                    **original_top3_prefixed,\n",
    "                }\n",
    "                results.append(result)\n",
    "            \n",
    "            # Periodic save\n",
    "            if len(results) >= save_every:\n",
    "                save_batch(results)\n",
    "                results = []\n",
    "        \n",
    "        # Progress\n",
    "        avg_delta = sum(boundary_deltas[k]) / len(boundary_deltas[k])\n",
    "        print(f\"[K={k}] avg boundary_delta = {avg_delta:.4f}\")\n",
    "    \n",
    "    # Save remaining\n",
    "    if results:\n",
    "        save_batch(results)\n",
    "    \n",
    "    print(f\"--- Total saved: {total_saved} rows to {filename} ---\")\n",
    "    return boundary_deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "22eaf9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Main Workflow: Neuron Perturbation Sensitivity Analysis\n",
    "# =============================================================================\n",
    "\n",
    "def run_perturbation_analysis_workflow(\n",
    "    model: \"LlamaForCausalLM\",\n",
    "    tokenizer: \"LlamaTokenizer\",\n",
    "    string_input: List,  # [input_id, input_text]\n",
    "    k_values: List[int],\n",
    "    distance_threshold: float = 0.1,\n",
    "    distance_metric: str = \"js_divergence\",\n",
    "    delta_max: float = 100.0,\n",
    "    tolerance: float = 0.001,\n",
    "    num_trials_per_k: int = 10,\n",
    "    save_every: int = 1000,\n",
    "):\n",
    "\n",
    "    input_id, input_text = string_input\n",
    "    sample_input = tokenizer(input_text, return_tensors=\"pt\")\n",
    "    inputs_on_device = {k: v.to(model.device) for k, v in sample_input.items()}\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Input ID: {input_id}\")\n",
    "    print(f\"Input: '{tokenizer.decode(inputs_on_device['input_ids'][0])}'\")\n",
    "    print(f\"K values: {k_values}\")\n",
    "    print(f\"Threshold: {distance_threshold} ({distance_metric})\")\n",
    "    print(f\"Delta search: max={delta_max}, tolerance={tolerance} (binary search)\")\n",
    "    print(f\"Trials per K: {num_trials_per_k}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # --- Step 1: Get Original Logits and Activations ---\n",
    "    print(\"Running forward pass to capture original state...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        original_logits = model(**inputs_on_device).logits\n",
    "    \n",
    "    # Capture activations (we only need final_norm)\n",
    "    original_activations = run_model_and_capture_activations(model, inputs=inputs_on_device)\n",
    "    \n",
    "    # Get final_norm output (layer n-1, input to lm_head)\n",
    "    try:\n",
    "        final_norm_output = original_activations['final_norm']['output'].to(model.device)\n",
    "    except KeyError:\n",
    "        print(\"ERROR: Could not find 'final_norm' in activations.\")\n",
    "        return\n",
    "    \n",
    "    hidden_size = final_norm_output.shape[2]\n",
    "    print(f\"Final norm output shape: {final_norm_output.shape}\")\n",
    "    print(f\"Hidden size (N = total neurons): {hidden_size}\")\n",
    "    \n",
    "    # Show original prediction\n",
    "    last_token_logits = original_logits[0, -1, :]\n",
    "    top_token_idx = torch.argmax(last_token_logits).item()\n",
    "    print(f\"Original top prediction: '{tokenizer.decode(top_token_idx)}' (ID: {top_token_idx})\")\n",
    "    \n",
    "    # --- Step 2: Find boundary delta for each K (binary search) ---\n",
    "    print(f\"\\nFinding boundary delta for each K (binary search)...\")\n",
    "    \n",
    "    boundary_deltas = analyze_neuron_perturbation_sensitivity(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        final_norm_activations=final_norm_output,\n",
    "        original_logits=original_logits,\n",
    "        k_values=k_values,\n",
    "        distance_threshold=distance_threshold,\n",
    "        distance_metric=distance_metric,\n",
    "        delta_max=delta_max,\n",
    "        tolerance=tolerance,\n",
    "        num_trials_per_k=num_trials_per_k,\n",
    "        save_every=save_every,\n",
    "        input_text=input_text,\n",
    "        input_id=input_id,\n",
    "    )\n",
    "    \n",
    "    # Clean up\n",
    "    del original_activations, final_norm_output\n",
    "    clear_activations()\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"SUMMARY: Boundary Delta per K\")\n",
    "    for k, deltas in boundary_deltas.items():\n",
    "        avg = sum(deltas) / len(deltas)\n",
    "        print(f\"  K={k:4d}: avg={avg:.4f}, min={min(deltas):.4f}, max={max(deltas):.4f}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    return boundary_deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "56fa7cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_texts = [\n",
    "    [1,\"The capital of France is\"],\n",
    "    [2,\"The largest mammal on Earth is\"],\n",
    "    [3,\"The process of photosynthesis occurs in\"],\n",
    "    [4,\"The speed of light in a vacuum is\"],\n",
    "    [5,\"The chemical symbol for gold is\"],\n",
    "    [6,\"The human body has how many bones\"],\n",
    "    [7,\"The Great Wall of China was built to\"],\n",
    "    [8,\"Water boils at what temperature\"],\n",
    "    [9,\"The smallest unit of matter is\"],\n",
    "    [10,\"Shakespeare wrote the play\"],\n",
    "    [11,\"The currency of Japan is\"],\n",
    "    [12,\"Mount Everest is located in\"],\n",
    "    [13,\"The inventor of the telephone was\"],\n",
    "    [14,\"DNA stands for\"],\n",
    "    [15,\"The largest ocean on Earth is\"],\n",
    "    [16,\"The planet closest to the Sun is\"],\n",
    "    [17,\"Gravity was discovered by\"],\n",
    "    [18,\"The Amazon rainforest is primarily located in\"],\n",
    "    [19,\"The freezing point of water is\"],\n",
    "    [20,\"The most abundant gas in Earth's atmosphere is\"],\n",
    "    [21,\"The Mona Lisa was painted by\"],\n",
    "    [22,\"The longest river in the world is\"],\n",
    "    [23,\"Photosynthesis converts carbon dioxide and water into\"],\n",
    "    [24,\"The study of earthquakes is called\"],\n",
    "    [25,\"The first person to walk on the moon was\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670b639c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>>> Starting Analysis for Prompt 1 (Mode: min) <<<<\n",
      "\n",
      "============================================================\n",
      "Input: '<s> The capital of France is'\n",
      "============================================================\n",
      "Registered 290 hooks.\n",
      "--- Logit Swap Attack ---\n",
      "Original top prediction: 'Paris' (ID: 3681)\n",
      "Target swap token:     'textt' (ID: 16196)\n",
      "New top prediction after swap: 'a'\n",
      "\n",
      "\n",
      "--- [Recon 1/2] Starting reconstruction ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing Recon 1:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Run Perturbation Analysis on All Sample Texts\n",
    "# =============================================================================\n",
    "\n",
    "# Configuration\n",
    "K_VALUES = [i for i in range(1, 1024)] # K values to test\n",
    "DISTANCE_THRESHOLD = 0.15  # Stop when distance >= this\n",
    "DISTANCE_METRIC = \"js_divergence\"  # Options: l2_distance, cosine_distance, kl_divergence, js_divergence\n",
    "DELTA_MAX = 1000.0  # Maximum delta to search\n",
    "TOLERANCE = 0.001  # Binary search precision\n",
    "NUM_TRIALS_PER_K = 5000  # Trials per K\n",
    "SAVE_EVERY = 1000  # Checkpoint frequency\n",
    "\n",
    "# Store results\n",
    "all_results = {}\n",
    "\n",
    "# Loop through each prompt\n",
    "for i, prompt in enumerate(sample_texts):\n",
    "    print(f\"\\n>>>> Starting Perturbation Analysis for Prompt {i+1}/{len(sample_texts)} <<<<\")\n",
    "    \n",
    "    boundary_deltas = run_perturbation_analysis_workflow(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        string_input=prompt,\n",
    "        k_values=K_VALUES,\n",
    "        distance_threshold=DISTANCE_THRESHOLD,\n",
    "        distance_metric=DISTANCE_METRIC,\n",
    "        delta_max=DELTA_MAX,\n",
    "        tolerance=TOLERANCE,\n",
    "        num_trials_per_k=NUM_TRIALS_PER_K,\n",
    "        save_every=SAVE_EVERY,\n",
    "    )\n",
    "    \n",
    "    all_results[prompt[0]] = boundary_deltas\n",
    "\n",
    "print(\"\\n\\n<<<< ALL ANALYSES COMPLETE >>>>\")\n",
    "print(f\"Results saved to 'neuron_perturbation_analysis.csv'\")\n",
    "\n",
    "# Overall summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"OVERALL: Average Boundary Delta per K (across all prompts)\")\n",
    "print(\"=\"*60)\n",
    "for k in K_VALUES:\n",
    "    all_deltas = []\n",
    "    for deltas_dict in all_results.values():\n",
    "        if k in deltas_dict:\n",
    "            all_deltas.extend(deltas_dict[k])\n",
    "    if all_deltas:\n",
    "        print(f\"  K={k:4d}: avg={sum(all_deltas)/len(all_deltas):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ca1d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running quick test (binary search for boundary delta)...\n",
      "\n",
      "============================================================\n",
      "Input ID: 1\n",
      "Input: '<s> The capital of France is'\n",
      "K values: [1, 2]\n",
      "Threshold: 0.1 (js_divergence)\n",
      "Delta search: max=1000.0, tolerance=0.001 (binary search)\n",
      "Trials per K: 3\n",
      "============================================================\n",
      "Running forward pass to capture original state...\n",
      "Registered 290 hooks.\n",
      "Final norm output shape: torch.Size([1, 6, 4096])\n",
      "Hidden size (N = total neurons): 4096\n",
      "Original top prediction: 'Paris' (ID: 3681)\n",
      "\n",
      "Finding boundary delta for each K (binary search)...\n",
      "[K=1] avg boundary_delta = 184.5318\n",
      "[K=2] avg boundary_delta = 81.3653\n",
      "[Checkpoint] Saved 120 rows (total: 120)\n",
      "--- Total saved: 120 rows to neuron_perturbation_analysis_other.csv ---\n",
      "\n",
      "============================================================\n",
      "SUMMARY: Boundary Delta per K\n",
      "  K=   1: avg=184.5318, min=113.5941, max=238.9383\n",
      "  K=   2: avg=81.3653, min=67.4696, max=102.1566\n",
      "============================================================\n",
      "\n",
      "Test complete! Check 'neuron_perturbation_analysis.csv' for output.\n"
     ]
    }
   ],
   "source": [
    "# # =============================================================================\n",
    "# # Quick Test: Run on a single input to verify output\n",
    "# # =============================================================================\n",
    "\n",
    "# # Test configuration\n",
    "# TEST_PROMPT = [1, \"The capital of France is\"]\n",
    "# TEST_K_VALUES = [1, 2]  \n",
    "# TEST_THRESHOLD = 0.1  # Distance threshold\n",
    "# TEST_METRIC = \"js_divergence\"\n",
    "# TEST_DELTA_MAX = 1000.0\n",
    "# TEST_TOLERANCE = 0.001\n",
    "# TEST_TRIALS = 3  \n",
    "# TEST_SAVE_EVERY = 1000\n",
    "\n",
    "# print(\"Running quick test (binary search for boundary delta)...\")\n",
    "# boundary_deltas = run_perturbation_analysis_workflow(\n",
    "#     model=model,\n",
    "#     tokenizer=tokenizer,\n",
    "#     string_input=TEST_PROMPT,\n",
    "#     k_values=TEST_K_VALUES,\n",
    "#     distance_threshold=TEST_THRESHOLD,\n",
    "#     distance_metric=TEST_METRIC,\n",
    "#     delta_max=TEST_DELTA_MAX,\n",
    "#     tolerance=TEST_TOLERANCE,\n",
    "#     num_trials_per_k=TEST_TRIALS,\n",
    "#     save_every=TEST_SAVE_EVERY,\n",
    "# )\n",
    "# print(\"\\nTest complete! Check for output.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dea5844",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
