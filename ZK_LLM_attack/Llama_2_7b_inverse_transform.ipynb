{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948b3723",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39140d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "MODEL_PATH = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4135ce0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    torch_dtype=torch.float32,  # Use float32 for better numerical precision in inversions\n",
    "    device_map='auto' if torch.cuda.is_available() else None\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "print(\"Model loaded with full parameter access!\")\n",
    "print(f\"Number of layers: {len(model.model.layers)}\")\n",
    "print(f\"Hidden size: {model.config.hidden_size}\")\n",
    "print(f\"Number of attention heads: {model.config.num_attention_heads}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddd9196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive Inverse Transform with Full Parameter Access\n",
    "class PreciseLlamaInverseTransform:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.config = model.config\n",
    "        self.hidden_size = model.config.hidden_size\n",
    "        self.num_heads = model.config.num_attention_heads\n",
    "        self.head_dim = self.hidden_size // self.num_heads\n",
    "        \n",
    "        # Extract all parameters for direct access\n",
    "        self.extract_all_parameters()\n",
    "    \n",
    "    def extract_all_parameters(self):\n",
    "        \"\"\"Extract all model parameters for direct mathematical operations\"\"\"\n",
    "        print(\"Extracting all model parameters...\")\n",
    "        \n",
    "        # LM Head parameters\n",
    "        self.lm_head_weight = self.model.lm_head.weight.detach().clone()\n",
    "        \n",
    "        # Embedding parameters\n",
    "        self.embed_tokens_weight = self.model.model.embed_tokens.weight.detach().clone()\n",
    "        \n",
    "        # Layer parameters\n",
    "        self.layer_params = []\n",
    "        for i, layer in enumerate(self.model.model.layers):\n",
    "            layer_param = {\n",
    "                # Input LayerNorm\n",
    "                'input_layernorm_weight': layer.input_layernorm.weight.detach().clone(),\n",
    "                \n",
    "                # Self-Attention\n",
    "                'q_proj_weight': layer.self_attn.q_proj.weight.detach().clone(),\n",
    "                'k_proj_weight': layer.self_attn.k_proj.weight.detach().clone(), \n",
    "                'v_proj_weight': layer.self_attn.v_proj.weight.detach().clone(),\n",
    "                'o_proj_weight': layer.self_attn.o_proj.weight.detach().clone(),\n",
    "                \n",
    "                # Post-attention LayerNorm\n",
    "                'post_attention_layernorm_weight': layer.post_attention_layernorm.weight.detach().clone(),\n",
    "                \n",
    "                # MLP\n",
    "                'gate_proj_weight': layer.mlp.gate_proj.weight.detach().clone(),\n",
    "                'up_proj_weight': layer.mlp.up_proj.weight.detach().clone(),\n",
    "                'down_proj_weight': layer.mlp.down_proj.weight.detach().clone(),\n",
    "            }\n",
    "            self.layer_params.append(layer_param)\n",
    "        \n",
    "        # Final LayerNorm\n",
    "        self.final_layernorm_weight = self.model.model.norm.weight.detach().clone()\n",
    "        \n",
    "        print(f\"Extracted parameters for {len(self.layer_params)} layers\")\n",
    "    \n",
    "    def precise_inverse_lm_head(self, logits):\n",
    "        \"\"\"Precise inverse of LM head using exact weight matrix\"\"\"\n",
    "        # logits = hidden_states @ lm_head_weight.T\n",
    "        # hidden_states = logits @ pinv(lm_head_weight.T)\n",
    "        \n",
    "        lm_head_weight_T_pinv = torch.pinverse(self.lm_head_weight.T)\n",
    "        hidden_states = torch.matmul(logits, lm_head_weight_T_pinv)\n",
    "        \n",
    "        return hidden_states\n",
    "    \n",
    "    def precise_inverse_layernorm(self, normalized_output, weight, original_stats=None):\n",
    "        \"\"\"Precise inverse of layer normalization with known parameters\"\"\"\n",
    "        # LayerNorm: output = (input - mean) / sqrt(var + eps) * weight\n",
    "        # To invert: input = (output / weight) * sqrt(var + eps) + mean\n",
    "        \n",
    "        eps = 1e-5  # Standard epsilon for Llama\n",
    "        \n",
    "        if original_stats is not None:\n",
    "            mean, var = original_stats\n",
    "        else:\n",
    "            # Estimate statistics (this is approximate)\n",
    "            # In practice, we'd need to store these during forward pass\n",
    "            mean = torch.zeros_like(normalized_output.mean(dim=-1, keepdim=True))\n",
    "            var = torch.ones_like(normalized_output.var(dim=-1, keepdim=True))\n",
    "        \n",
    "        # Inverse transformation\n",
    "        std = torch.sqrt(var + eps)\n",
    "        unnormalized = (normalized_output / weight) * std + mean\n",
    "        \n",
    "        return unnormalized\n",
    "    \n",
    "    def precise_inverse_silu(self, silu_output, approximate=True):\n",
    "        \"\"\"Approximate inverse of SiLU activation\"\"\"\n",
    "        # SiLU(x) = x * sigmoid(x)\n",
    "        # This is not easily invertible, so we use approximation\n",
    "        \n",
    "        if approximate:\n",
    "            # Simple approximation: assume x ≈ silu_output for small values\n",
    "            # For larger values, use iterative method or lookup table\n",
    "            return silu_output  # Rough approximation\n",
    "        else:\n",
    "            # More precise but computationally expensive\n",
    "            # Could implement Newton-Raphson or other numerical methods\n",
    "            return silu_output\n",
    "    \n",
    "    def precise_inverse_mlp(self, mlp_output, layer_idx):\n",
    "        \"\"\"Precise inverse of MLP using exact weight matrices\"\"\"\n",
    "        params = self.layer_params[layer_idx]\n",
    "        \n",
    "        # MLP structure: gate_proj(x) * SiLU(up_proj(x)) -> down_proj -> output\n",
    "        # Inverse: output -> inv(down_proj) -> split gate/up -> inv(gate_proj), inv(up_proj)\n",
    "        \n",
    "        # Step 1: Inverse of down_proj\n",
    "        down_proj_weight_pinv = torch.pinverse(params['down_proj_weight'])\n",
    "        intermediate = torch.matmul(mlp_output, down_proj_weight_pinv.T)\n",
    "        \n",
    "        # Step 2: This is where we need to split the gate*SiLU(up) combination\n",
    "        # This is approximate since we lost information in the element-wise multiplication\n",
    "        \n",
    "        # Approximate split (this is the main limitation)\n",
    "        gate_proj_weight_pinv = torch.pinverse(params['gate_proj_weight'])\n",
    "        up_proj_weight_pinv = torch.pinverse(params['up_proj_weight'])\n",
    "        \n",
    "        # Rough approximation: assume gate ≈ up for inversion\n",
    "        sqrt_intermediate = torch.sqrt(torch.abs(intermediate) + 1e-8) * torch.sign(intermediate)\n",
    "        \n",
    "        # Get original input estimates\n",
    "        input_from_gate = torch.matmul(sqrt_intermediate, gate_proj_weight_pinv.T)\n",
    "        input_from_up = torch.matmul(sqrt_intermediate, up_proj_weight_pinv.T)\n",
    "        \n",
    "        # Average the estimates\n",
    "        mlp_input = (input_from_gate + input_from_up) / 2\n",
    "        \n",
    "        return mlp_input\n",
    "    \n",
    "    def precise_inverse_attention(self, attn_output, layer_idx, attention_mask=None):\n",
    "        \"\"\"Precise inverse of self-attention using exact weight matrices\"\"\"\n",
    "        params = self.layer_params[layer_idx]\n",
    "        \n",
    "        # Attention: Q@K.T -> softmax -> @V -> o_proj -> output\n",
    "        # Inverse: output -> inv(o_proj) -> approximate inv(attention mechanism)\n",
    "        \n",
    "        # Step 1: Inverse of output projection\n",
    "        o_proj_weight_pinv = torch.pinverse(params['o_proj_weight'])\n",
    "        attention_heads_output = torch.matmul(attn_output, o_proj_weight_pinv.T)\n",
    "        \n",
    "        # Step 2: Reshape to separate heads\n",
    "        batch_size, seq_len = attn_output.shape[:2]\n",
    "        attention_heads_output = attention_heads_output.view(\n",
    "            batch_size, seq_len, self.num_heads, self.head_dim\n",
    "        )\n",
    "        \n",
    "        # Step 3: Approximate inverse of attention mechanism\n",
    "        # This is highly approximate since attention is not easily invertible\n",
    "        \n",
    "        # Simple approximation: assume uniform attention weights\n",
    "        # In reality, this loses a lot of information\n",
    "        v_approx = attention_heads_output  # Assume V ≈ attention output\n",
    "        \n",
    "        # Reshape back\n",
    "        v_concat = v_approx.view(batch_size, seq_len, self.hidden_size)\n",
    "        \n",
    "        # Step 4: Inverse of V, K, Q projections to get original input\n",
    "        v_proj_weight_pinv = torch.pinverse(params['v_proj_weight'])  # V uses same weight as value\n",
    "        k_proj_weight_pinv = torch.pinverse(params['k_proj_weight'])\n",
    "        q_proj_weight_pinv = torch.pinverse(params['q_proj_weight'])\n",
    "        \n",
    "        # Get input estimates from each projection\n",
    "        input_from_v = torch.matmul(v_concat, v_proj_weight_pinv.T)\n",
    "        # For K and Q, we'd need the actual K,Q values which we don't have\n",
    "        # So we approximate\n",
    "        input_from_k = input_from_v  # Rough approximation\n",
    "        input_from_q = input_from_v  # Rough approximation\n",
    "        \n",
    "        # Average estimates\n",
    "        attention_input = (input_from_v + input_from_k + input_from_q) / 3\n",
    "        \n",
    "        return attention_input\n",
    "    \n",
    "    def precise_inverse_transformer_layer(self, layer_output, layer_idx, stored_residuals=None):\n",
    "        \"\"\"Precise inverse of complete transformer layer\"\"\"\n",
    "        params = self.layer_params[layer_idx]\n",
    "        \n",
    "        # Transformer layer structure:\n",
    "        # 1. input -> input_layernorm -> attention -> residual_1\n",
    "        # 2. residual_1 -> post_attention_layernorm -> mlp -> residual_2 (= layer_output)\n",
    "        \n",
    "        current = layer_output\n",
    "        \n",
    "        # Step 1: Remove final residual connection (approximate)\n",
    "        # residual_2 = residual_1 + mlp_output\n",
    "        # We need residual_1, but we approximate it\n",
    "        if stored_residuals and layer_idx in stored_residuals:\n",
    "            residual_1 = stored_residuals[layer_idx]['post_attention']\n",
    "        else:\n",
    "            # Approximate: assume residual_1 ≈ current / 2\n",
    "            residual_1 = current * 0.5\n",
    "        \n",
    "        mlp_output = current - residual_1\n",
    "        \n",
    "        # Step 2: Inverse of post-attention LayerNorm\n",
    "        post_attn_ln_input = self.precise_inverse_layernorm(\n",
    "            mlp_output, \n",
    "            params['post_attention_layernorm_weight']\n",
    "        )\n",
    "        \n",
    "        # Step 3: Inverse of MLP\n",
    "        mlp_input = self.precise_inverse_mlp(post_attn_ln_input, layer_idx)\n",
    "        \n",
    "        # mlp_input should equal residual_1, so we have residual_1\n",
    "        residual_1_recovered = mlp_input\n",
    "        \n",
    "        # Step 4: Remove first residual connection\n",
    "        # residual_1 = layer_input + attention_output\n",
    "        if stored_residuals and layer_idx in stored_residuals:\n",
    "            layer_input = stored_residuals[layer_idx]['input']\n",
    "        else:\n",
    "            # Approximate\n",
    "            layer_input = residual_1_recovered * 0.5\n",
    "        \n",
    "        attention_output = residual_1_recovered - layer_input\n",
    "        \n",
    "        # Step 5: Inverse of input LayerNorm\n",
    "        input_ln_input = self.precise_inverse_layernorm(\n",
    "            attention_output,\n",
    "            params['input_layernorm_weight']\n",
    "        )\n",
    "        \n",
    "        # Step 6: Inverse of attention\n",
    "        attention_input = self.precise_inverse_attention(input_ln_input, layer_idx)\n",
    "        \n",
    "        # attention_input should equal layer_input\n",
    "        return attention_input\n",
    "    \n",
    "    def full_inverse_transform(self, final_logits, target_layer_depth=3):\n",
    "        \"\"\"Complete inverse transform from logits to target layer activations\"\"\"\n",
    "        \n",
    "        current_activation = final_logits\n",
    "        \n",
    "        # Step 1: Inverse of LM head\n",
    "        current_activation = self.precise_inverse_lm_head(current_activation)\n",
    "        \n",
    "        # Step 2: Inverse of final layer norm\n",
    "        current_activation = self.precise_inverse_layernorm(\n",
    "            current_activation,\n",
    "            self.final_layernorm_weight\n",
    "        )\n",
    "        \n",
    "        # Step 3: Inverse through transformer layers (backwards)\n",
    "        reconstructed_layer_activations = {}\n",
    "        \n",
    "        # Go backwards from last layer to target depth\n",
    "        num_layers = len(self.layer_params)\n",
    "        for layer_idx in range(num_layers - 1, target_layer_depth - 1, -1):\n",
    "            current_activation = self.precise_inverse_transformer_layer(\n",
    "                current_activation, \n",
    "                layer_idx\n",
    "            )\n",
    "            \n",
    "            # Store activation for this layer\n",
    "            reconstructed_layer_activations[layer_idx] = current_activation.detach().clone()\n",
    "        \n",
    "        return reconstructed_layer_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6b0ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to capture original activations\n",
    "def get_activation(name, activations_dict):\n",
    "    def hook(module, input, output):\n",
    "        if isinstance(output, tuple):\n",
    "            activations_dict[name] = output[0].detach().clone()\n",
    "        else:\n",
    "            activations_dict[name] = output.detach().clone()\n",
    "    return hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8132554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample inputs\n",
    "def generate_sample_inputs(tokenizer, n_samples=20, seq_length=8):\n",
    "    \"\"\"Generate sample inputs - reduced size for computational efficiency\"\"\"\n",
    "    sample_texts = [\n",
    "        \"The cat sat on\",\n",
    "        \"Machine learning is\", \n",
    "        \"Climate change affects\",\n",
    "        \"Deep networks can\",\n",
    "        \"Natural language processing\",\n",
    "        \"Computer vision detects\",\n",
    "        \"Quantum computing enables\",\n",
    "        \"Blockchain provides secure\",\n",
    "        \"Renewable energy sources\",\n",
    "        \"Medical research shows\"\n",
    "    ]\n",
    "    \n",
    "    inputs = []\n",
    "    for i in range(n_samples):\n",
    "        base_text = sample_texts[i % len(sample_texts)]\n",
    "        tokenized = tokenizer(\n",
    "            base_text,\n",
    "            return_tensors=\"pt\",\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=seq_length\n",
    "        )\n",
    "        inputs.append(tokenized.input_ids.to(model.device))\n",
    "    \n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305a31cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main analysis function with precise inverse transforms\n",
    "def generate_activation_differences_precise_inverse(model, X_data, n_samples=10, n_reconstructions=5):\n",
    "    \"\"\"\n",
    "    Precise inverse transform reconstruction analysis\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    inverse_transformer = PreciseLlamaInverseTransform(model)\n",
    "    \n",
    "    # Target layers to analyze\n",
    "    target_layers = [0, 1, 2]\n",
    "    \n",
    "    for sample_idx in tqdm(range(min(n_samples, len(X_data))), desc=\"Processing samples\"):\n",
    "        original_input = X_data[sample_idx]\n",
    "        \n",
    "        # Get original activations for comparison\n",
    "        original_activations = {}\n",
    "        hooks = []\n",
    "        \n",
    "        layer_names = [f'model.layers.{i}' for i in target_layers]\n",
    "        for layer_name in layer_names:\n",
    "            layer_module = model\n",
    "            for attr in layer_name.split('.'):\n",
    "                layer_module = getattr(layer_module, attr)\n",
    "            hooks.append(layer_module.register_forward_hook(\n",
    "                get_activation(layer_name, original_activations)\n",
    "            ))\n",
    "        \n",
    "        # Original forward pass\n",
    "        with torch.no_grad():\n",
    "            original_output = model(original_input).logits\n",
    "        \n",
    "        # Remove hooks\n",
    "        for hook in hooks:\n",
    "            hook.remove()\n",
    "        \n",
    "        # Multiple reconstructions with different perturbations\n",
    "        for recon_idx in range(n_reconstructions):\n",
    "            try:\n",
    "                # Add controlled perturbation for different reconstructions\n",
    "                noise_scale = 0.001 * (recon_idx + 1)  # Very small perturbations\n",
    "                perturbed_logits = original_output + torch.randn_like(original_output) * noise_scale\n",
    "                \n",
    "                # PRECISE INVERSE TRANSFORM\n",
    "                reconstructed_activations = inverse_transformer.full_inverse_transform(\n",
    "                    perturbed_logits, \n",
    "                    target_layer_depth=max(target_layers)\n",
    "                )\n",
    "                \n",
    "                # Calculate differences\n",
    "                row = {'sample_idx': sample_idx, 'reconstruction_idx': recon_idx}\n",
    "                all_layer_max_diffs = []\n",
    "                \n",
    "                for layer_idx in target_layers:\n",
    "                    layer_name = f'model.layers.{layer_idx}'\n",
    "                    \n",
    "                    if (layer_name in original_activations and \n",
    "                        layer_idx in reconstructed_activations):\n",
    "                        \n",
    "                        orig_act = original_activations[layer_name].flatten().float()\n",
    "                        recon_act = reconstructed_activations[layer_idx].flatten().float()\n",
    "                        \n",
    "                        # Ensure same size\n",
    "                        min_size = min(orig_act.shape[0], recon_act.shape[0])\n",
    "                        orig_act = orig_act[:min_size]\n",
    "                        recon_act = recon_act[:min_size]\n",
    "                        \n",
    "                        abs_diff = torch.abs(orig_act - recon_act)\n",
    "                        \n",
    "                        row[f'layer_{layer_idx}_min_abs_diff'] = abs_diff.min().item()\n",
    "                        row[f'layer_{layer_idx}_mean_abs_diff'] = abs_diff.mean().item()\n",
    "                        row[f'layer_{layer_idx}_max_abs_diff'] = abs_diff.max().item()\n",
    "                        \n",
    "                        all_layer_max_diffs.append(abs_diff.max().item())\n",
    "                \n",
    "                # Aggregate metrics\n",
    "                if all_layer_max_diffs:\n",
    "                    row['all_layers_max_diff'] = max(all_layer_max_diffs)\n",
    "                    row['all_layers_min_of_max'] = min(all_layer_max_diffs)\n",
    "                \n",
    "                results.append(row)\n",
    "                \n",
    "                print(f\"Sample {sample_idx}, Recon {recon_idx}: Max diff = {row.get('all_layers_max_diff', 'N/A')}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error in sample {sample_idx}, reconstruction {recon_idx}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    return pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790e5532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the analysis\n",
    "print(\"Generating sample inputs...\")\n",
    "X_data = generate_sample_inputs(tokenizer, n_samples=8, seq_length=6)  # Small for testing\n",
    "print(f\"Generated {len(X_data)} samples\")\n",
    "\n",
    "print(\"\\nRunning precise inverse transform analysis...\")\n",
    "results = generate_activation_differences_precise_inverse(model, X_data, n_samples=5, n_reconstructions=3)\n",
    "\n",
    "# Save results\n",
    "results.to_csv('llama2_precise_inverse_results.csv', index=False)\n",
    "print(f\"\\nResults saved. Shape: {results.shape}\")\n",
    "if len(results) > 0:\n",
    "    print(\"\\nFirst few rows:\")\n",
    "    print(results.head())\n",
    "    print(f\"\\nSample statistics:\")\n",
    "    print(f\"Mean max diff across all layers: {results['all_layers_max_diff'].mean():.6f}\")\n",
    "    print(f\"Min max diff: {results['all_layers_max_diff'].min():.6f}\")\n",
    "    print(f\"Max max diff: {results['all_layers_max_diff'].max():.6f}\")\n",
    "else:\n",
    "    print(\"No results generated - check for errors above\")\n",
    "\n",
    "print(\"\\nPrecise inverse transform analysis with full parameter access completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6f868a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660984d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b38d79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b63ff48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1563b2af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
