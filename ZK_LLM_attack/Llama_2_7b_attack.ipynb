{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cb8346",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from tqdm import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf004e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "MODEL_PATH = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cc4b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map='auto' if torch.cuda.is_available() else None\n",
    ")\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b40becc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to capture activations\n",
    "def get_activation(name, activations_dict):\n",
    "    def hook(module, input, output):\n",
    "        # Handle different output types\n",
    "        if isinstance(output, tuple):\n",
    "            activations_dict[name] = output[0].detach().clone()\n",
    "        else:\n",
    "            activations_dict[name] = output.detach().clone()\n",
    "    return hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d376c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample inputs for analysis\n",
    "def generate_sample_inputs(tokenizer, n_samples=100, seq_length=32):    \n",
    "    sample_texts = [\n",
    "        \"The future of artificial intelligence\",\n",
    "        \"Climate change affects global weather\",\n",
    "        \"Machine learning algorithms can\",\n",
    "        \"Deep neural networks are\",\n",
    "        \"Natural language processing enables\",\n",
    "        \"Computer vision systems detect\",\n",
    "        \"Quantum computing will revolutionize\",\n",
    "        \"Blockchain technology provides\",\n",
    "        \"Renewable energy sources include\",\n",
    "        \"Medical research has shown\",\n",
    "        \"Space exploration reveals\",\n",
    "        \"Economic policies influence\",\n",
    "        \"Educational systems should\",\n",
    "        \"Transportation networks connect\",\n",
    "        \"Communication technologies enable\"\n",
    "    ]\n",
    "    \n",
    "    inputs = []\n",
    "    for i in range(n_samples):\n",
    "        # Cycle through sample texts and add variations\n",
    "        base_text = sample_texts[i % len(sample_texts)]\n",
    "        \n",
    "        # Add some randomness\n",
    "        if i > len(sample_texts):\n",
    "            base_text = base_text + f\" in {2020 + (i % 10)} with\"\n",
    "        \n",
    "        # Tokenize\n",
    "        tokenized = tokenizer(\n",
    "            base_text, \n",
    "            return_tensors=\"pt\", \n",
    "            padding='max_length', \n",
    "            truncation=True, \n",
    "            max_length=seq_length\n",
    "        )\n",
    "        inputs.append(tokenized.input_ids.to(model.device))\n",
    "    \n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885df2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate activation differences for Llama-2 layers\n",
    "def generate_activation_differences_llama(model, X_data, n_samples=50, n_reconstructions=3):\n",
    "    results = []\n",
    "    \n",
    "    # Select specific layers to analyze (first few transformer layers)\n",
    "    layer_names = [\n",
    "        'model.layers.0',  # First transformer layer\n",
    "        'model.layers.1',  # Second transformer layer  \n",
    "        'model.layers.2',  # Third transformer layer\n",
    "    ]\n",
    "    \n",
    "    for sample_idx in tqdm(range(min(n_samples, len(X_data))), desc=\"Processing samples\"):\n",
    "        original_input = X_data[sample_idx]\n",
    "        \n",
    "        # Get original activations\n",
    "        original_activations = {}\n",
    "        hooks = []\n",
    "        \n",
    "        # Register hooks for target layers\n",
    "        for layer_name in layer_names:\n",
    "            layer_module = model\n",
    "            for attr in layer_name.split('.'):\n",
    "                layer_module = getattr(layer_module, attr)\n",
    "            hooks.append(layer_module.register_forward_hook(\n",
    "                get_activation(layer_name, original_activations)\n",
    "            ))\n",
    "        \n",
    "        # Get original output and activations\n",
    "        with torch.no_grad():\n",
    "            original_output = model(original_input).logits\n",
    "        \n",
    "        # Remove hooks\n",
    "        for hook in hooks:\n",
    "            hook.remove()\n",
    "        \n",
    "        # Multiple reconstruction attempts\n",
    "        for recon_idx in range(n_reconstructions):\n",
    "            # Initialize random input embeddings for reconstruction\n",
    "            seq_length = original_input.shape[1]\n",
    "            embedding_dim = model.config.hidden_size\n",
    "            \n",
    "            # Use embeddings instead of token IDs for gradient-based optimization\n",
    "            reconstructed_embeddings = torch.randn(\n",
    "                1, seq_length, embedding_dim,\n",
    "                device=model.device,\n",
    "                dtype=torch.float32,\n",
    "                requires_grad=True\n",
    "            )\n",
    "            \n",
    "            optimizer = optim.Adam([reconstructed_embeddings], lr=0.01)\n",
    "            \n",
    "            # Reconstruction optimization\n",
    "            for iteration in range(500):  # Reduced iterations for efficiency\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Forward pass with embeddings\n",
    "                embeddings_model_dtype = reconstructed_embeddings.to(model.dtype)\n",
    "                output = model(inputs_embeds=embeddings_model_dtype).logits\n",
    "                \n",
    "                # Loss: match original output\n",
    "                loss = nn.functional.mse_loss(output.float(), original_output.float())\n",
    "                \n",
    "                # Add regularization\n",
    "                reg_loss = 0.001 * torch.mean(reconstructed_embeddings ** 2)\n",
    "                total_loss = loss + reg_loss\n",
    "                \n",
    "                total_loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                if iteration % 100 == 0:\n",
    "                    print(f\"Sample {sample_idx}, Recon {recon_idx}, Iter {iteration}, Loss: {total_loss.item():.6f}\")\n",
    "            \n",
    "            # Get reconstructed activations\n",
    "            reconstructed_activations = {}\n",
    "            hooks = []\n",
    "            \n",
    "            for layer_name in layer_names:\n",
    "                layer_module = model\n",
    "                for attr in layer_name.split('.'):\n",
    "                    layer_module = getattr(layer_module, attr)\n",
    "                hooks.append(layer_module.register_forward_hook(\n",
    "                    get_activation(layer_name, reconstructed_activations)\n",
    "                ))\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                embeddings_model_dtype = reconstructed_embeddings.to(model.dtype)\n",
    "                _ = model(inputs_embeds=embeddings_model_dtype)\n",
    "            \n",
    "            # Remove hooks\n",
    "            for hook in hooks:\n",
    "                hook.remove()\n",
    "            \n",
    "            # Calculate differences for each layer\n",
    "            row = {'sample_idx': sample_idx, 'reconstruction_idx': recon_idx}\n",
    "            \n",
    "            # Store individual layer metrics\n",
    "            all_layer_max_diffs = []\n",
    "            \n",
    "            for layer_name in layer_names:\n",
    "                if layer_name in original_activations and layer_name in reconstructed_activations:\n",
    "                    orig_act = original_activations[layer_name].flatten().float()\n",
    "                    recon_act = reconstructed_activations[layer_name].flatten().float()\n",
    "                    \n",
    "                    abs_diff = torch.abs(orig_act - recon_act)\n",
    "                    \n",
    "                    layer_short = layer_name.split('.')[-1]  # Get layer number\n",
    "                    row[f'layer_{layer_short}_min_abs_diff'] = abs_diff.min().item()\n",
    "                    row[f'layer_{layer_short}_mean_abs_diff'] = abs_diff.mean().item()\n",
    "                    row[f'layer_{layer_short}_max_abs_diff'] = abs_diff.max().item()\n",
    "                    \n",
    "                    all_layer_max_diffs.append(abs_diff.max().item())\n",
    "            \n",
    "            # Store the maximum difference across ALL layers\n",
    "            if all_layer_max_diffs:\n",
    "                row['all_layers_max_diff'] = max(all_layer_max_diffs)\n",
    "                row['all_layers_min_of_max'] = min(all_layer_max_diffs)\n",
    "            \n",
    "            results.append(row)\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5481000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample data\n",
    "print(\"Generating sample inputs...\")\n",
    "X_data = generate_sample_inputs(tokenizer, n_samples=20, seq_length=16) \n",
    "print(f\"Generated {len(X_data)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbe1a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate results\n",
    "print(\"Generating activation differences for Llama-2 layers...\")\n",
    "results = generate_activation_differences_llama(model, X_data, n_samples=10, n_reconstructions=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ca85c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "results.to_csv('llama2_activation_diff_results.csv', index=False)\n",
    "print(f\"Results saved. Shape: {results.shape}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(results.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff6167c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define threshold values to test\n",
    "thresholds = np.logspace(-6, 0, 100)  # From 1e-6 to 1\n",
    "\n",
    "# Create visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('Llama-2 Activation Reconstruction Analysis', fontsize=16)\n",
    "\n",
    "layers = ['0', '1', '2']  # Layer numbers\n",
    "metrics = ['min_abs_diff', 'mean_abs_diff', 'max_abs_diff']\n",
    "colors = {'min_abs_diff': 'blue', 'mean_abs_diff': 'green', 'max_abs_diff': 'red'}\n",
    "\n",
    "# Plot for each layer\n",
    "for idx, layer in enumerate(layers):\n",
    "    if idx < 3:  # Only plot first 3 layers\n",
    "        ax = axes[idx//2, idx%2]\n",
    "        \n",
    "        for metric in metrics:\n",
    "            column = f'layer_{layer}_{metric}'\n",
    "            if column in results.columns:\n",
    "                values = results[column].values\n",
    "                \n",
    "                # Calculate percentage passing each threshold\n",
    "                percentages = []\n",
    "                for threshold in thresholds:\n",
    "                    passing = np.sum(values <= threshold) / len(values) * 100\n",
    "                    percentages.append(passing)\n",
    "                \n",
    "                # Plot cumulative distribution\n",
    "                ax.semilogx(thresholds, percentages, \n",
    "                           label=f'Layer {layer} - {metric.replace(\"_abs_diff\", \"\").capitalize()}',\n",
    "                           color=colors[metric], linewidth=2, alpha=0.7)\n",
    "        \n",
    "        # Add reference lines\n",
    "        ax.axvline(x=0.007, color='black', linestyle='--', alpha=0.5, label='0.007 threshold')\n",
    "        ax.axhline(y=90, color='gray', linestyle=':', alpha=0.5)\n",
    "        ax.axhline(y=95, color='gray', linestyle=':', alpha=0.5)\n",
    "        ax.axhline(y=99, color='gray', linestyle=':', alpha=0.5)\n",
    "        \n",
    "        ax.set_xlabel('Threshold Value', fontsize=12)\n",
    "        ax.set_ylabel('Percentage Passing (%)', fontsize=12)\n",
    "        ax.set_title(f'Transformer Layer {layer}', fontsize=14)\n",
    "        ax.legend(fontsize=8)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.set_ylim(0, 105)\n",
    "\n",
    "# Plot ALL LAYERS simultaneous check\n",
    "ax = axes[1, 1]\n",
    "\n",
    "if 'all_layers_max_diff' in results.columns:\n",
    "    values = results['all_layers_max_diff'].values\n",
    "    percentages_all = []\n",
    "    for threshold in thresholds:\n",
    "        passing = np.sum(values <= threshold) / len(values) * 100\n",
    "        percentages_all.append(passing)\n",
    "    \n",
    "    ax.semilogx(thresholds, percentages_all, \n",
    "               label='ALL Layers Must Pass (Worst Case)',\n",
    "               color='darkred', linewidth=3)\n",
    "    \n",
    "    # Also plot individual layer maximums for comparison\n",
    "    for layer in layers:\n",
    "        column = f'layer_{layer}_max_abs_diff'\n",
    "        if column in results.columns:\n",
    "            values = results[column].values\n",
    "            percentages = []\n",
    "            for threshold in thresholds:\n",
    "                passing = np.sum(values <= threshold) / len(values) * 100\n",
    "                percentages.append(passing)\n",
    "            ax.semilogx(thresholds, percentages, \n",
    "                       label=f'Layer {layer} only',\n",
    "                       linewidth=1, alpha=0.5, linestyle='--')\n",
    "\n",
    "ax.axvline(x=0.007, color='black', linestyle='--', alpha=0.5, label='0.007 threshold')\n",
    "ax.axhline(y=90, color='gray', linestyle=':', alpha=0.5)\n",
    "ax.axhline(y=95, color='gray', linestyle=':', alpha=0.5)\n",
    "ax.axhline(y=99, color='gray', linestyle=':', alpha=0.5)\n",
    "\n",
    "ax.set_xlabel('Threshold Value', fontsize=12)\n",
    "ax.set_ylabel('Percentage Passing (%)', fontsize=12)\n",
    "ax.set_title('ALL Layers Simultaneous Pass Rate', fontsize=14)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_ylim(0, 105)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('llama2_activation_reconstruction_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# %%\n",
    "# Create threshold analysis table\n",
    "threshold_values = [0.001, 0.005, 0.007, 0.01, 0.05, 0.1]\n",
    "analysis_results = []\n",
    "\n",
    "# Individual layer analysis\n",
    "for layer in layers:\n",
    "    for threshold in threshold_values:\n",
    "        row = {'Layer': f'Layer {layer}', 'Threshold': threshold}\n",
    "        \n",
    "        for metric in metrics:\n",
    "            column = f'layer_{layer}_{metric}'\n",
    "            if column in results.columns:\n",
    "                values = results[column].values\n",
    "                passing_percentage = np.sum(values <= threshold) / len(values) * 100\n",
    "                row[f'{metric.replace(\"_abs_diff\", \"\").capitalize()} Pass %'] = f'{passing_percentage:.1f}%'\n",
    "        \n",
    "        analysis_results.append(row)\n",
    "\n",
    "# ALL LAYERS analysis\n",
    "if 'all_layers_max_diff' in results.columns:\n",
    "    for threshold in threshold_values:\n",
    "        row = {'Layer': 'ALL LAYERS', 'Threshold': threshold}\n",
    "        \n",
    "        values = results['all_layers_max_diff'].values\n",
    "        passing_percentage = np.sum(values <= threshold) / len(values) * 100\n",
    "        row['Min Pass %'] = '-'\n",
    "        row['Mean Pass %'] = '-'\n",
    "        row['Max Pass %'] = f'{passing_percentage:.1f}%'\n",
    "        \n",
    "        analysis_results.append(row)\n",
    "\n",
    "# Create DataFrame and display\n",
    "threshold_df = pd.DataFrame(analysis_results)\n",
    "print(\"\\nLlama-2 Threshold Analysis Table:\")\n",
    "print(\"=\"*80)\n",
    "print(threshold_df.to_string(index=False))\n",
    "\n",
    "# Save to CSV\n",
    "threshold_df.to_csv('llama2_threshold_analysis.csv', index=False)\n",
    "\n",
    "# %%\n",
    "# Summary statistics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LLAMA-2 ACTIVATION RECONSTRUCTION ANALYSIS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if 'all_layers_max_diff' in results.columns:\n",
    "    values = results['all_layers_max_diff'].values\n",
    "    \n",
    "    print(f\"\\nDataset size: {len(results)} reconstruction attempts\")\n",
    "    print(f\"Layers analyzed: {layers}\")\n",
    "    \n",
    "    # Find threshold for different pass rates\n",
    "    pass_rates = [90, 95, 99]\n",
    "    print(\"\\nThreshold needed for target pass rates (ALL LAYERS):\")\n",
    "    \n",
    "    for rate in pass_rates:\n",
    "        if len(values) > 0:\n",
    "            threshold_for_rate = np.percentile(values, rate)\n",
    "            print(f\"  {rate}% pass rate: {threshold_for_rate:.6f}\")\n",
    "    \n",
    "    # Statistics at specific thresholds\n",
    "    print(\"\\nPass rates at specific thresholds (ALL LAYERS):\")\n",
    "    for threshold in [0.001, 0.007, 0.01, 0.1]:\n",
    "        pass_rate = np.sum(values <= threshold) / len(values) * 100\n",
    "        print(f\"  Threshold {threshold}: {pass_rate:.1f}% pass rate\")\n",
    "\n",
    "print(\"\\nAnalysis completed! Check the generated plots and CSV files for detailed results.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6916e25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b4a867",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec680b5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
