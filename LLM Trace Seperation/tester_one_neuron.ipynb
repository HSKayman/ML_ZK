{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10562b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "import os\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import json\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8863042",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "MODEL_1_PATH = \"meta-llama/Llama-2-7b-chat-hf\" \n",
    "MODEL_2_PATH = \"meta-llama/Llama-2-7b-hf\"       \n",
    "\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3911279d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.58s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:49<00:00, 24.82s/it]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# %%\n",
    "tokenizer = LlamaTokenizer.from_pretrained(MODEL_1_PATH)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model_1 = LlamaForCausalLM.from_pretrained(\n",
    "    MODEL_1_PATH,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "model_2 = LlamaForCausalLM.from_pretrained(\n",
    "    MODEL_2_PATH,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "print(\"Models loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ea8373",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = model_1.to(DEVICE)\n",
    "model_2 = model_2.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3bf55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables for activation capture\n",
    "activations_model_1 = {}\n",
    "activations_model_2 = {}\n",
    "shared_weights = {}  # Store weights from model_1 only\n",
    "current_hooks = []\n",
    "hook_errors = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854f9fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_activations():\n",
    "    global activations_model_1, activations_model_2, shared_weights\n",
    "    activations_model_1.clear()\n",
    "    activations_model_2.clear()\n",
    "    shared_weights.clear()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "def remove_all_hooks():\n",
    "    global current_hooks\n",
    "    for hook in current_hooks:\n",
    "        try:\n",
    "            hook.remove()\n",
    "        except:\n",
    "            pass\n",
    "    current_hooks.clear()\n",
    "\n",
    "def get_activation_hook(name, model_name):\n",
    "    def hook(module, input, output):\n",
    "        global hook_errors, shared_weights\n",
    "        try:\n",
    "            # Handle output\n",
    "            if isinstance(output, tuple):\n",
    "                activation = output[0] if len(output) > 0 and output[0] is not None else None\n",
    "            else:\n",
    "                activation = output\n",
    "            \n",
    "            # Handle input\n",
    "            input_tensor = None\n",
    "            if input is not None and isinstance(input, tuple) and len(input) > 0:\n",
    "                input_tensor = input[0] if input[0] is not None else None\n",
    "            \n",
    "            # Store activation data (without weights)\n",
    "            activation_data = {\n",
    "                'output': activation.detach().cpu() if activation is not None else None,\n",
    "                'input': input_tensor.detach().cpu() if input_tensor is not None else None,\n",
    "            }\n",
    "            \n",
    "            # Only capture weights from Model_1 and store in shared dictionary\n",
    "            if model_name == \"Model_1\":\n",
    "                if name not in shared_weights:\n",
    "                    shared_weights[name] = {\n",
    "                        'weight': module.weight.detach().cpu() if hasattr(module, 'weight') and module.weight is not None else None,\n",
    "                        'bias': module.bias.detach().cpu() if hasattr(module, 'bias') and module.bias is not None else None\n",
    "                    }\n",
    "                activations_model_1[name] = activation_data\n",
    "            else:\n",
    "                activations_model_2[name] = activation_data\n",
    "                \n",
    "        except Exception as e:\n",
    "            error_msg = f\"Hook error in {name} ({model_name}): {str(e)}\"\n",
    "            hook_errors.append(error_msg)\n",
    "            print(f\"WARNING: {error_msg}\")\n",
    "            \n",
    "            # Store None data to prevent missing keys\n",
    "            activation_data = {\n",
    "                'output': None,\n",
    "                'input': None,\n",
    "            }\n",
    "            \n",
    "            if model_name == \"Model_1\":\n",
    "                activations_model_1[name] = activation_data\n",
    "                if name not in shared_weights:\n",
    "                    shared_weights[name] = {'weight': None, 'bias': None}\n",
    "            else:\n",
    "                activations_model_2[name] = activation_data\n",
    "            \n",
    "    return hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf4d8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def register_llama_hooks(model, model_name, layer_range=None, max_layers=None):\n",
    "    global current_hooks, hook_errors\n",
    "    hooks = []\n",
    "    successful_hooks = 0\n",
    "    failed_hooks = 0\n",
    "    \n",
    "    hook_errors.clear()\n",
    "    \n",
    "    total_layers = len(model.model.layers)\n",
    "    if max_layers is not None:\n",
    "        total_layers = min(total_layers, max_layers)\n",
    "    \n",
    "    if layer_range is None:\n",
    "        layer_range = range(total_layers)\n",
    "    \n",
    "    print(f\"Registering hooks for {model_name}: {len(layer_range)} layers\")\n",
    "    \n",
    "    for i in layer_range:\n",
    "        if i >= len(model.model.layers):\n",
    "            continue\n",
    "            \n",
    "        layer = model.model.layers[i]\n",
    "        layer_prefix = f\"layer_{i}\"\n",
    "        \n",
    "        components_to_hook = [\n",
    "            (layer.self_attn.q_proj, f\"{layer_prefix}_attention_q\"),\n",
    "            (layer.self_attn.k_proj, f\"{layer_prefix}_attention_k\"),\n",
    "            (layer.self_attn.v_proj, f\"{layer_prefix}_attention_v\"),\n",
    "            (layer.self_attn.o_proj, f\"{layer_prefix}_attention_output\"),\n",
    "            (layer.mlp.gate_proj, f\"{layer_prefix}_mlp_gate\"),\n",
    "            (layer.mlp.up_proj, f\"{layer_prefix}_mlp_up\"),\n",
    "            (layer.mlp.down_proj, f\"{layer_prefix}_mlp_down\"),\n",
    "            (layer.input_layernorm, f\"{layer_prefix}_input_norm\"),\n",
    "            (layer.post_attention_layernorm, f\"{layer_prefix}_post_attn_norm\"),\n",
    "        ]\n",
    "        \n",
    "        for module, hook_name in components_to_hook:\n",
    "            try:\n",
    "                hook = module.register_forward_hook(\n",
    "                    get_activation_hook(hook_name, model_name)\n",
    "                )\n",
    "                hooks.append(hook)\n",
    "                successful_hooks += 1\n",
    "            except Exception as e:\n",
    "                error_msg = f\"Failed to register {hook_name}: {str(e)}\"\n",
    "                hook_errors.append(error_msg)\n",
    "                failed_hooks += 1\n",
    "    \n",
    "    # Register final components\n",
    "    try:\n",
    "        hooks.append(model.model.norm.register_forward_hook(\n",
    "            get_activation_hook(\"final_norm\", model_name)\n",
    "        ))\n",
    "        hooks.append(model.lm_head.register_forward_hook(\n",
    "            get_activation_hook(\"lm_head\", model_name)\n",
    "        ))\n",
    "        successful_hooks += 2\n",
    "    except Exception as e:\n",
    "        error_msg = f\"Failed to register final components: {str(e)}\"\n",
    "        hook_errors.append(error_msg)\n",
    "        failed_hooks += 2\n",
    "    \n",
    "    current_hooks.extend(hooks)\n",
    "    \n",
    "    print(f\"Hook registration complete for {model_name}:\")\n",
    "    print(f\"  ✓ Successful: {successful_hooks}\")\n",
    "    print(f\"  ✗ Failed: {failed_hooks}\")\n",
    "    \n",
    "    return hooks\n",
    "\n",
    "def select_neurons_per_token_position(activations1, activations2, shared_weights, mode='min', seed=42):\n",
    "    \"\"\"\n",
    "    Select neurons per token position based on calculation error.\n",
    "    \n",
    "    Args:\n",
    "        activations1: Activations from model 1\n",
    "        activations2: Activations from model 2\n",
    "        shared_weights: Shared weights from model 1\n",
    "        mode: 'min' (minimum calculation error) or 'random' (random selection)\n",
    "        seed: Random seed for reproducibility\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    selected_neurons = {}\n",
    "    \n",
    "    print(f\"Selecting neurons per token position from {len(activations1)} layers...\")\n",
    "    print(f\"Selection mode: {mode}\")\n",
    "    \n",
    "    for layer_name, layer_data in activations1.items():\n",
    "        if not isinstance(layer_data, dict):\n",
    "            continue\n",
    "            \n",
    "        activation1 = layer_data.get('output')\n",
    "        activation2 = activations2.get(layer_name, {}).get('output')\n",
    "        weight_data = shared_weights.get(layer_name, {})\n",
    "\n",
    "        if activation1 is None or activation2 is None:\n",
    "            print(f\"Skipping {layer_name}: Missing activation data\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            if len(activation1.shape) == 3:  # [batch, seq_len, hidden_size]\n",
    "                batch_size, seq_len, hidden_size = activation1.shape\n",
    "                \n",
    "                if hidden_size == 0:\n",
    "                    continue\n",
    "                \n",
    "                # Select neurons for EACH token position separately\n",
    "                token_selections = {}\n",
    "                \n",
    "                for token_pos in range(seq_len):\n",
    "                    # Get activations for this specific token position\n",
    "                    token_act1 = activation1[0, token_pos, :]  # [hidden_size]\n",
    "                    token_act2 = activation2[0, token_pos, :]  # [hidden_size]\n",
    "                    \n",
    "                    # Select neuron based on mode\n",
    "                    if mode == 'random':\n",
    "                        neuron_idx = torch.randint(0, hidden_size, (1,)).item()\n",
    "                        diff_value = torch.abs(token_act1[neuron_idx] - token_act2[neuron_idx]).item()\n",
    "                    elif mode == 'min':\n",
    "                        # Calculate differences for this token\n",
    "                        diff = torch.abs(token_act1 - token_act2)\n",
    "                        neuron_idx = torch.argmin(diff).item()\n",
    "                        diff_value = diff[neuron_idx].item()\n",
    "                    else:\n",
    "                        # Default to min\n",
    "                        diff = torch.abs(token_act1 - token_act2)\n",
    "                        neuron_idx = torch.argmin(diff).item()\n",
    "                        diff_value = diff[neuron_idx].item()\n",
    "                    \n",
    "                    token_selections[token_pos] = {\n",
    "                        'neuron_index': neuron_idx,\n",
    "                        'difference': diff_value,\n",
    "                        'activation1_value': token_act1[neuron_idx].item(),\n",
    "                        'activation2_value': token_act2[neuron_idx].item(),\n",
    "                        'abs_activation1': abs(token_act1[neuron_idx].item()),\n",
    "                        'abs_activation2': abs(token_act2[neuron_idx].item()),\n",
    "                        'selection_mode': mode\n",
    "                    }\n",
    "                \n",
    "                selected_neurons[layer_name] = {\n",
    "                    'per_token_selections': token_selections,\n",
    "                    'sequence_length': seq_len,\n",
    "                    'hidden_size': hidden_size,\n",
    "                    'activation_shape': list(activation1.shape),\n",
    "                    'layer_type': get_component_type(layer_name)\n",
    "                }\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error selecting neurons for {layer_name}: {e}\")\n",
    "            continue\n",
    "            \n",
    "    print(f\"Successfully selected neurons from {len(selected_neurons)} layers\")\n",
    "    return selected_neurons\n",
    "\n",
    "def get_component_type(layer_name):\n",
    "    if 'attention' in layer_name:\n",
    "        return 'attention'\n",
    "    elif 'mlp' in layer_name:\n",
    "        return 'mlp'\n",
    "    elif 'norm' in layer_name:\n",
    "        return 'normalization'\n",
    "    elif 'lm_head' in layer_name:\n",
    "        return 'output'\n",
    "    elif 'embed' in layer_name:\n",
    "        return 'embedding'\n",
    "    else:\n",
    "        return 'other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9d201c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_single_token_neuron(layer_name, neuron_idx, token_pos, \n",
    "                                 layer_1_data, layer_2_data, shared_weight_data):\n",
    "\n",
    "    # Get inputs for this specific token from both models\n",
    "    input_tensor_1 = layer_1_data.get('input')\n",
    "    input_tensor_2 = layer_2_data.get('input')\n",
    "    \n",
    "    if input_tensor_1 is None or token_pos >= input_tensor_1.shape[1]:\n",
    "        return {'error': 'Missing or invalid input data from model 1'}\n",
    "    if input_tensor_2 is None or token_pos >= input_tensor_2.shape[1]:\n",
    "        return {'error': 'Missing or invalid input data from model 2'}\n",
    "    \n",
    "    # Get input for this specific token from both models\n",
    "    token_input_1 = input_tensor_1[0, token_pos, :]  # [hidden_size]\n",
    "    token_input_2 = input_tensor_2[0, token_pos, :]  # [hidden_size]\n",
    "    \n",
    "    # Get weights from model_1 ONLY (shared weights)\n",
    "    w1 = shared_weight_data.get('weight')\n",
    "    b1 = shared_weight_data.get('bias')\n",
    "    \n",
    "    if w1 is None:\n",
    "        return {'error': 'Missing weight data'}\n",
    "    \n",
    "    try:\n",
    "        # Calculate for this specific token and neuron\n",
    "        # IMPORTANT: Both calculations use model_1 weights (w1, b1)\n",
    "        # but with different inputs (token_input_1 vs token_input_2)\n",
    "        if 'norm' in layer_name:\n",
    "            # Layer norm calculation: weight * normalized_input + bias\n",
    "            if neuron_idx >= w1.shape[0] or neuron_idx >= token_input_1.shape[0]:\n",
    "                return {'error': 'Index out of bounds for layer norm'}\n",
    "                \n",
    "            # Both use w1 weights, different inputs\n",
    "            calc_1 = w1[neuron_idx].item() * token_input_1[neuron_idx].item()\n",
    "            calc_2 = w1[neuron_idx].item() * token_input_2[neuron_idx].item()\n",
    "            \n",
    "            if b1 is not None and neuron_idx < b1.shape[0]:\n",
    "                calc_1 += b1[neuron_idx].item()\n",
    "                calc_2 += b1[neuron_idx].item()\n",
    "                \n",
    "        else:\n",
    "            # Linear layer calculation: input @ weight.T + bias\n",
    "            if neuron_idx >= w1.shape[0]:\n",
    "                return {'error': 'Neuron index out of bounds'}\n",
    "                \n",
    "            # Both use w1 weights, different inputs\n",
    "            calc_1 = torch.matmul(token_input_1, w1[neuron_idx, :]).item()\n",
    "            calc_2 = torch.matmul(token_input_2, w1[neuron_idx, :]).item()\n",
    "            \n",
    "            if b1 is not None and neuron_idx < b1.shape[0]:\n",
    "                calc_1 += b1[neuron_idx].item()\n",
    "                calc_2 += b1[neuron_idx].item()\n",
    "            \n",
    "            # Apply activation function for MLP components\n",
    "            if 'mlp_gate' in layer_name or 'mlp_up' in layer_name:\n",
    "                calc_1 = F.silu(torch.tensor(calc_1)).item()\n",
    "                calc_2 = F.silu(torch.tensor(calc_2)).item()\n",
    "        \n",
    "        # Get actual outputs from the models\n",
    "        actual_1 = layer_1_data.get('output')\n",
    "        actual_2 = layer_2_data.get('output')\n",
    "        \n",
    "        actual_1_val = None\n",
    "        actual_2_val = None\n",
    "        \n",
    "        if actual_1 is not None and token_pos < actual_1.shape[1] and neuron_idx < actual_1.shape[2]:\n",
    "            actual_1_val = actual_1[0, token_pos, neuron_idx].item()\n",
    "        if actual_2 is not None and token_pos < actual_2.shape[1] and neuron_idx < actual_2.shape[2]:\n",
    "            actual_2_val = actual_2[0, token_pos, neuron_idx].item()\n",
    "        \n",
    "        # Calculate errors between our calculations and actual outputs\n",
    "        calc_error_1 = abs(calc_1 - actual_1_val) if actual_1_val is not None else None\n",
    "        calc_error_2 = abs(calc_2 - actual_2_val) if actual_2_val is not None else None\n",
    "        \n",
    "        return {\n",
    "            'token_position': token_pos,\n",
    "            'neuron_index': neuron_idx,\n",
    "            'model_1_calculated': calc_1,\n",
    "            'model_2_calculated': calc_2,\n",
    "            'calculation_difference': calc_1 - calc_2,\n",
    "            'model_1_actual': actual_1_val,\n",
    "            'model_2_actual': actual_2_val,\n",
    "            'actual_difference': (actual_1_val - actual_2_val) if (actual_1_val is not None and actual_2_val is not None) else None,\n",
    "            'calculation_error_1': calc_error_1,\n",
    "            'calculation_error_2': calc_error_2,\n",
    "            'layer_type': get_component_type(layer_name)\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {'error': f'Calculation failed: {str(e)}'}\n",
    "\n",
    "def compare_neuron_calculations_per_token(model_1_activations, model_2_activations, \n",
    "                                        selected_neurons, shared_weights):\n",
    "    comparison_results = {}\n",
    "    \n",
    "    print(f\"Comparing calculations for {len(selected_neurons)} layers...\")\n",
    "    \n",
    "    for layer_name, neuron_info in selected_neurons.items():\n",
    "        if 'per_token_selections' not in neuron_info:\n",
    "            continue\n",
    "            \n",
    "        results = {\n",
    "            'layer_type': neuron_info.get('layer_type', get_component_type(layer_name)),\n",
    "            'sequence_length': neuron_info['sequence_length'],\n",
    "            'hidden_size': neuron_info['hidden_size'],\n",
    "            'token_analyses': {},\n",
    "            'summary_stats': {}\n",
    "        }\n",
    "        \n",
    "        # Get layer data (activations only)\n",
    "        layer_1_data = model_1_activations.get(layer_name, {})\n",
    "        layer_2_data = model_2_activations.get(layer_name, {})\n",
    "        # Get shared weights from model_1\n",
    "        shared_weight_data = shared_weights.get(layer_name, {})\n",
    "        \n",
    "        if not isinstance(layer_1_data, dict) or not isinstance(layer_2_data, dict):\n",
    "            print(f\"Skipping {layer_name}: Invalid layer data\")\n",
    "            continue\n",
    "        \n",
    "        # Analyze each token position with its selected neuron\n",
    "        valid_analyses = 0\n",
    "        calc_diffs = []\n",
    "        actual_diffs = []\n",
    "        calc_errors_1 = []\n",
    "        calc_errors_2 = []\n",
    "        \n",
    "        for token_pos, token_data in neuron_info['per_token_selections'].items():\n",
    "            neuron_idx = token_data['neuron_index']\n",
    "            \n",
    "            # Calculate for this specific token and neuron using shared weights\n",
    "            token_analysis = calculate_single_token_neuron(\n",
    "                layer_name, neuron_idx, token_pos,\n",
    "                layer_1_data, layer_2_data, shared_weight_data\n",
    "            )\n",
    "            \n",
    "            # Add selection info to analysis\n",
    "            if 'error' not in token_analysis:\n",
    "                token_analysis.update({\n",
    "                    'selected_activation1': token_data['activation1_value'],\n",
    "                    'selected_activation2': token_data['activation2_value'],\n",
    "                    'selection_difference': token_data['difference'],\n",
    "                    'selection_mode': token_data.get('selection_mode', 'unknown')\n",
    "                })\n",
    "                \n",
    "                valid_analyses += 1\n",
    "                calc_diffs.append(token_analysis['calculation_difference'])\n",
    "                \n",
    "                if token_analysis['actual_difference'] is not None:\n",
    "                    actual_diffs.append(token_analysis['actual_difference'])\n",
    "                if token_analysis['calculation_error_1'] is not None:\n",
    "                    calc_errors_1.append(token_analysis['calculation_error_1'])\n",
    "                if token_analysis['calculation_error_2'] is not None:\n",
    "                    calc_errors_2.append(token_analysis['calculation_error_2'])\n",
    "            \n",
    "            results['token_analyses'][token_pos] = token_analysis\n",
    "        \n",
    "        # Calculate summary statistics\n",
    "        if valid_analyses > 0:\n",
    "            results['summary_stats'] = {\n",
    "                'valid_analyses': valid_analyses,\n",
    "                'total_tokens': len(neuron_info['per_token_selections']),\n",
    "                'mean_calc_difference': np.mean(calc_diffs) if calc_diffs else None,\n",
    "                'std_calc_difference': np.std(calc_diffs) if calc_diffs else None,\n",
    "                'max_abs_calc_difference': max([abs(d) for d in calc_diffs]) if calc_diffs else None,\n",
    "                'mean_actual_difference': np.mean(actual_diffs) if actual_diffs else None,\n",
    "                'mean_calc_error_1': np.mean(calc_errors_1) if calc_errors_1 else None,\n",
    "                'mean_calc_error_2': np.mean(calc_errors_2) if calc_errors_2 else None,\n",
    "                'unique_neurons_selected': len(set(td['neuron_index'] for td in neuron_info['per_token_selections'].values()))\n",
    "            }\n",
    "        \n",
    "        comparison_results[layer_name] = results\n",
    "    \n",
    "    print(f\"Completed comparisons for {len(comparison_results)} layers\")\n",
    "    return comparison_results\n",
    "\n",
    "def save_detailed_results_per_token(comparison_results, filename=\"per_token_neuron_analysis.csv\"):\n",
    "    rows = []\n",
    "    \n",
    "    input_text = comparison_results.get('input_text', 'Unknown')\n",
    "    \n",
    "    for layer_name, layer_data in comparison_results.get('layer_comparisons', {}).items():\n",
    "        if 'token_analyses' not in layer_data:\n",
    "            continue\n",
    "            \n",
    "        for token_pos, token_analysis in layer_data['token_analyses'].items():\n",
    "            if 'error' in token_analysis:\n",
    "                # Save error rows too\n",
    "                row = {\n",
    "                    'input_text': input_text[:100],\n",
    "                    'layer_name': layer_name,\n",
    "                    'layer_type': layer_data.get('layer_type', 'unknown'),\n",
    "                    'token_position': token_pos,\n",
    "                    'neuron_index': None,\n",
    "                    'error': token_analysis['error'],\n",
    "                    'model_1_calculated': None,\n",
    "                    'model_2_calculated': None,\n",
    "                    'calculation_difference': None,\n",
    "                    'model_1_actual': None,\n",
    "                    'model_2_actual': None,\n",
    "                    'actual_difference': None,\n",
    "                    'calculation_error_1': None,\n",
    "                    'calculation_error_2': None,\n",
    "                    'selected_activation1': None,\n",
    "                    'selected_activation2': None,\n",
    "                    'selection_difference': None,\n",
    "                    'selection_method': None\n",
    "                }\n",
    "            else:\n",
    "                row = {\n",
    "                    'input_text': input_text[:100],\n",
    "                    'layer_name': layer_name,\n",
    "                    'layer_type': token_analysis.get('layer_type', 'unknown'),\n",
    "                    'token_position': token_analysis['token_position'],\n",
    "                    'neuron_index': token_analysis['neuron_index'],\n",
    "                    'error': None,\n",
    "                    'model_1_calculated': token_analysis['model_1_calculated'],\n",
    "                    'model_2_calculated': token_analysis['model_2_calculated'],\n",
    "                    'calculation_difference': token_analysis['calculation_difference'],\n",
    "                    'abs_calculation_difference': abs(token_analysis['calculation_difference']),\n",
    "                    'model_1_actual': token_analysis['model_1_actual'],\n",
    "                    'model_2_actual': token_analysis['model_2_actual'],\n",
    "                    'actual_difference': token_analysis['actual_difference'],\n",
    "                    'abs_actual_difference': abs(token_analysis['actual_difference']) if token_analysis['actual_difference'] is not None else None,\n",
    "                    'calculation_error_1': token_analysis['calculation_error_1'],\n",
    "                    'calculation_error_2': token_analysis['calculation_error_2'],\n",
    "                    'selected_activation1': token_analysis.get('selected_activation1'),\n",
    "                    'selected_activation2': token_analysis.get('selected_activation2'),\n",
    "                    'selection_difference': token_analysis.get('selection_difference'),\n",
    "                    'selection_mode': token_analysis.get('selection_mode')\n",
    "                }\n",
    "            \n",
    "            rows.append(row)\n",
    "    \n",
    "    df = pd.DataFrame(rows)\n",
    "    \n",
    "    # Save to CSV\n",
    "    if os.path.exists(filename):\n",
    "        df.to_csv(filename, mode='a', header=False, index=False)\n",
    "    else:\n",
    "        df.to_csv(filename, index=False)\n",
    "    \n",
    "    print(f\"Saved {len(rows)} rows to {filename}\")\n",
    "    return df\n",
    "\n",
    "def run_comparison_per_token(text_input, mode='min', seed=42, max_layers=None):\n",
    "    \"\"\"\n",
    "    Run comparison between model 1 and model 2 using model 1 weights.\n",
    "    \n",
    "    Args:\n",
    "        text_input: Input text to process\n",
    "        mode: 'min' (minimum error) or 'random' (random neuron selection)\n",
    "        seed: Random seed\n",
    "        max_layers: Maximum number of layers to process (None for all)\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Processing: {text_input[:50]}...\")\n",
    "    print(f\"Selection mode: {mode}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Clear previous data and free memory\n",
    "    clear_activations()\n",
    "    remove_all_hooks()\n",
    "    \n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(\n",
    "        text_input, \n",
    "        return_tensors=\"pt\", \n",
    "        padding=True, \n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "    inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n",
    "    \n",
    "    print(f\"Input tokens: {inputs['input_ids'].shape[1]}\")\n",
    "    \n",
    "    try:\n",
    "        # Register hooks\n",
    "        print(\"\\n1. Registering hooks...\")\n",
    "        hooks_1 = register_llama_hooks(model_1, \"Model_1\", max_layers=max_layers)\n",
    "        hooks_2 = register_llama_hooks(model_2, \"Model_2\", max_layers=max_layers)\n",
    "        \n",
    "        if len(hooks_1) == 0 or len(hooks_2) == 0:\n",
    "            raise Exception(\"Failed to register hooks\")\n",
    "        \n",
    "        # Run models\n",
    "        print(\"\\n2. Running models...\")\n",
    "        with torch.no_grad():\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            \n",
    "            outputs_1 = model_1(**inputs)\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            \n",
    "            outputs_2 = model_2(**inputs)\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        print(f\"\\n3. Activation capture results:\")\n",
    "        print(f\"   Model 1: {len(activations_model_1)} layers captured\")\n",
    "        print(f\"   Model 2: {len(activations_model_2)} layers captured\")\n",
    "        \n",
    "        # Select neurons per token position\n",
    "        print(\"\\n4. Selecting neurons per token position...\")\n",
    "        selected_neurons = select_neurons_per_token_position(\n",
    "            activations_model_1, activations_model_2, shared_weights,\n",
    "            mode=mode, seed=seed\n",
    "        )\n",
    "        \n",
    "        # Compare activations using shared weights from model_1\n",
    "        print(\"\\n5. Comparing activations...\")\n",
    "        comparison_results = compare_neuron_calculations_per_token(\n",
    "            activations_model_1,\n",
    "            activations_model_2,\n",
    "            selected_neurons,\n",
    "            shared_weights\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n6. Results summary:\")\n",
    "        print(f\"   Layers with comparisons: {len(comparison_results)}\")\n",
    "        \n",
    "        # Calculate overall statistics\n",
    "        total_valid = sum(r['summary_stats'].get('valid_analyses', 0) for r in comparison_results.values())\n",
    "        total_tokens = sum(r['summary_stats'].get('total_tokens', 0) for r in comparison_results.values())\n",
    "        \n",
    "        print(f\"   Total valid analyses: {total_valid}\")\n",
    "        print(f\"   Total token positions: {total_tokens}\")\n",
    "        \n",
    "        return {\n",
    "            'input_text': text_input,\n",
    "            'tokenized_input': inputs,\n",
    "            'model_1_output': outputs_1.logits,\n",
    "            'model_2_output': outputs_2.logits,\n",
    "            'layer_comparisons': comparison_results,\n",
    "            'selected_neurons': selected_neurons,\n",
    "            'hook_errors': hook_errors.copy(),\n",
    "            'layers_captured_1': len(activations_model_1),\n",
    "            'layers_captured_2': len(activations_model_2),\n",
    "            'shared_weights_captured': len(shared_weights),\n",
    "            'selection_mode': mode,\n",
    "            'total_valid_analyses': total_valid,\n",
    "            'total_token_positions': total_tokens\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"\\nERROR in run_comparison_per_token: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "        return {\n",
    "            'input_text': text_input,\n",
    "            'error': str(e),\n",
    "            'layer_comparisons': {},\n",
    "            'selected_neurons': {},\n",
    "            'hook_errors': hook_errors.copy(),\n",
    "            'layers_captured_1': len(activations_model_1),\n",
    "            'layers_captured_2': len(activations_model_2),\n",
    "            'shared_weights_captured': len(shared_weights),\n",
    "            'selection_mode': mode\n",
    "        }\n",
    "    \n",
    "    finally:\n",
    "        remove_all_hooks()\n",
    "        clear_activations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbfda25",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_TEXTS = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Artificial intelligence is transforming the world of technology.\",\n",
    "    \"In a hole in the ground there lived a hobbit.\",\n",
    "    \"To be or not to be, that is the question Shakespeare posed.\",\n",
    "    \"Machine learning models require large datasets for training.\",\n",
    "    \"The mitochondria is the powerhouse of the cell in biology.\",\n",
    "    \"Climate change is causing unprecedented shifts in global weather patterns.\",\n",
    "    \"Mozart composed his first symphony at the age of eight years old.\",\n",
    "    \"The stock market experienced significant volatility during the pandemic crisis.\",\n",
    "    \"Quantum physics reveals the strange behavior of particles at subatomic levels.\",\n",
    "    \"Professional chefs recommend using fresh herbs to enhance flavor profiles.\",\n",
    "    \"Ancient Egyptian pyramids were built using sophisticated engineering techniques.\",\n",
    "    \"Regular exercise and proper nutrition are essential for maintaining good health.\",\n",
    "    \"The International Space Station orbits Earth approximately every ninety minutes.\",\n",
    "    \"Cryptocurrency markets operate twenty-four hours a day across global exchanges.\",\n",
    "    \"Vincent van Gogh painted Starry Night while staying at an asylum.\",\n",
    "    \"Professional athletes must maintain strict training regimens throughout their careers.\",\n",
    "    \"The Amazon rainforest produces twenty percent of the world's oxygen supply.\",\n",
    "    \"Modern architecture emphasizes clean lines and functional design principles.\",\n",
    "    \"Forensic scientists use DNA analysis to solve complex criminal investigations.\",\n",
    "    \"Traditional Japanese tea ceremonies follow centuries-old ritualistic practices.\",\n",
    "    \"Marine biologists study coral reef ecosystems threatened by ocean acidification.\",\n",
    "    \"The Renaissance period marked a cultural rebirth in European art and science.\",\n",
    "    \"Cybersecurity experts work tirelessly to protect digital infrastructure from threats.\",\n",
    "    \"Sustainable agriculture practices help preserve soil quality for future generations.\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb31afe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing text 1/5 ===\n",
      "Processing: The quick brown fox jumps over the lazy dog....\n",
      "Input tokens: 13\n",
      "Registering hooks...\n",
      "Running models...\n",
      "Captured 290 activations from Model 1\n",
      "Captured 290 activations from Model 2\n",
      "Selecting random neurons...\n",
      "Selected neurons from 290 layers\n",
      "Comparing activations...\n",
      "Visualization saved to neuron_comparison_text_0.png\n",
      "Completed text 1\n",
      "\n",
      "=== Processing text 2/5 ===\n",
      "Processing: Artificial intelligence is transforming the world ...\n",
      "Input tokens: 13\n",
      "Registering hooks...\n",
      "Running models...\n",
      "Captured 290 activations from Model 1\n",
      "Captured 290 activations from Model 2\n",
      "Selecting random neurons...\n",
      "Selected neurons from 290 layers\n",
      "Comparing activations...\n",
      "Visualization saved to neuron_comparison_text_1.png\n",
      "Completed text 2\n",
      "\n",
      "=== Processing text 3/5 ===\n",
      "Processing: In a hole in the ground there lived a hobbit....\n",
      "Input tokens: 14\n",
      "Registering hooks...\n",
      "Running models...\n",
      "Captured 290 activations from Model 1\n",
      "Captured 290 activations from Model 2\n",
      "Selecting random neurons...\n",
      "Selected neurons from 290 layers\n",
      "Comparing activations...\n",
      "Visualization saved to neuron_comparison_text_2.png\n",
      "Completed text 3\n",
      "\n",
      "=== Processing text 4/5 ===\n",
      "Processing: To be or not to be, that is the question Shakespea...\n",
      "Input tokens: 16\n",
      "Registering hooks...\n",
      "Running models...\n",
      "Captured 290 activations from Model 1\n",
      "Captured 290 activations from Model 2\n",
      "Selecting random neurons...\n",
      "Selected neurons from 290 layers\n",
      "Comparing activations...\n",
      "Visualization saved to neuron_comparison_text_3.png\n",
      "Completed text 4\n",
      "\n",
      "=== Processing text 5/5 ===\n",
      "Processing: Machine learning models require large datasets for...\n",
      "Input tokens: 10\n",
      "Registering hooks...\n",
      "Running models...\n",
      "Captured 290 activations from Model 1\n",
      "Captured 290 activations from Model 2\n",
      "Selecting random neurons...\n",
      "Selected neurons from 290 layers\n",
      "Comparing activations...\n",
      "Visualization saved to neuron_comparison_text_4.png\n",
      "Completed text 5\n"
     ]
    }
   ],
   "source": [
    "# Run with your preferred mode\n",
    "PREFERRED_MODE = 'min'  # Options: 'min' or 'random'\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Running full analysis with mode: {PREFERRED_MODE}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for i, text in enumerate(TEST_TEXTS):\n",
    "    print(f\"\\n=== Processing text {i+1}/{len(TEST_TEXTS)} ===\")\n",
    "    \n",
    "    try:\n",
    "        result = run_comparison_per_token(\n",
    "            text, \n",
    "            mode=PREFERRED_MODE,\n",
    "            seed=42+i,\n",
    "            max_layers=None  # Use all layers\n",
    "        )\n",
    "        \n",
    "        all_results.append(result)\n",
    "        \n",
    "        # Save detailed results\n",
    "        save_detailed_results_per_token(\n",
    "            result, \n",
    "            filename=f\"all_texts_per_token_{PREFERRED_MODE}.csv\"\n",
    "        )\n",
    "        \n",
    "        print(f\"✓ Completed text {i+1}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error processing text {i+1}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\n{'='*60}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
