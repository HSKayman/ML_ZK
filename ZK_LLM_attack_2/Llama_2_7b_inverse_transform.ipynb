{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "948b3723",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exouser/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39140d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "MODEL_PATH = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4135ce0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded with full parameter access!\n",
      "Number of layers: 32\n",
      "Hidden size: 4096\n",
      "Number of attention heads: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    torch_dtype=torch.float32,  # Use float32 for better numerical precision in inversions\n",
    "    device_map='auto' if torch.cuda.is_available() else None\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "print(\"Model loaded with full parameter access!\")\n",
    "print(f\"Number of layers: {len(model.model.layers)}\")\n",
    "print(f\"Hidden size: {model.config.hidden_size}\")\n",
    "print(f\"Number of attention heads: {model.config.num_attention_heads}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ddd9196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Comprehensive Inverse Transform with Memory-Efficient Parameter Access\n",
    "class PreciseLlamaInverseTransform:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.config = model.config\n",
    "        self.hidden_size = model.config.hidden_size\n",
    "        self.num_heads = model.config.num_attention_heads\n",
    "        self.head_dim = self.hidden_size // self.num_heads\n",
    "        \n",
    "        # Don't extract all parameters - use direct access to save memory\n",
    "        print(\"Initialized with direct parameter access (memory efficient)\")\n",
    "    \n",
    "    def precise_inverse_lm_head(self, logits):\n",
    "        # Access weight directly without cloning\n",
    "        with torch.no_grad():\n",
    "            lm_head_weight_T_pinv = torch.pinverse(self.model.lm_head.weight.T)\n",
    "            hidden_states = torch.matmul(logits, lm_head_weight_T_pinv)\n",
    "        return hidden_states\n",
    "    \n",
    "    def precise_inverse_layernorm(self, normalized_output, weight, original_stats=None):\n",
    "        # LayerNorm: output = (input - mean) / sqrt(var + eps) * weight\n",
    "        # To invert: input = (output / weight) * sqrt(var + eps) + mean\n",
    "        \n",
    "        eps = 1e-5  # Standard epsilon for Llama\n",
    "        \n",
    "        if original_stats is not None:\n",
    "            mean, var = original_stats\n",
    "        else:\n",
    "            # Estimate statistics (this is approximate)\n",
    "            mean = torch.zeros_like(normalized_output.mean(dim=-1, keepdim=True))\n",
    "            var = torch.ones_like(normalized_output.var(dim=-1, keepdim=True))\n",
    "        \n",
    "        # Inverse transformation\n",
    "        std = torch.sqrt(var + eps)\n",
    "        unnormalized = (normalized_output / weight) * std + mean\n",
    "        \n",
    "        return unnormalized\n",
    "    \n",
    "    def precise_inverse_silu(self, silu_output, approximate=True):\n",
    "        # SiLU(x) = x * sigmoid(x)\n",
    "        # This is not easily invertible, so we use approximation\n",
    "        if approximate:\n",
    "            return silu_output  # Rough approximation\n",
    "        else:\n",
    "            return silu_output\n",
    "    \n",
    "    def precise_inverse_mlp(self, mlp_output, layer_idx):\n",
    "        # Access layer directly\n",
    "        layer = self.model.model.layers[layer_idx]\n",
    "        \n",
    "        # MLP structure: gate_proj(x) * SiLU(up_proj(x)) -> down_proj -> output\n",
    "        # Inverse: output -> inv(down_proj) -> split gate/up -> inv(gate_proj), inv(up_proj)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Step 1: Inverse of down_proj\n",
    "            down_proj_weight_pinv = torch.pinverse(layer.mlp.down_proj.weight)\n",
    "            intermediate = torch.matmul(mlp_output, down_proj_weight_pinv.T)\n",
    "            \n",
    "            # Step 2: Approximate split of gate*SiLU(up) combination\n",
    "            gate_proj_weight_pinv = torch.pinverse(layer.mlp.gate_proj.weight)\n",
    "            up_proj_weight_pinv = torch.pinverse(layer.mlp.up_proj.weight)\n",
    "            \n",
    "            # Rough approximation: assume gate ≈ up for inversion\n",
    "            sqrt_intermediate = torch.sqrt(torch.abs(intermediate) + 1e-8) * torch.sign(intermediate)\n",
    "            \n",
    "            # Get original input estimates\n",
    "            input_from_gate = torch.matmul(sqrt_intermediate, gate_proj_weight_pinv.T)\n",
    "            input_from_up = torch.matmul(sqrt_intermediate, up_proj_weight_pinv.T)\n",
    "            \n",
    "            # Average the estimates\n",
    "            mlp_input = (input_from_gate + input_from_up) / 2\n",
    "        \n",
    "        return mlp_input\n",
    "    \n",
    "    def precise_inverse_attention(self, attn_output, layer_idx, attention_mask=None):\n",
    "        \"\"\"Precise inverse of self-attention using direct weight access\"\"\"\n",
    "        layer = self.model.model.layers[layer_idx]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Step 1: Inverse of output projection\n",
    "            o_proj_weight_pinv = torch.pinverse(layer.self_attn.o_proj.weight)\n",
    "            attention_heads_output = torch.matmul(attn_output, o_proj_weight_pinv.T)\n",
    "            \n",
    "            # Step 2: Reshape to separate heads\n",
    "            batch_size, seq_len = attn_output.shape[:2]\n",
    "            attention_heads_output = attention_heads_output.view(\n",
    "                batch_size, seq_len, self.num_heads, self.head_dim\n",
    "            )\n",
    "            \n",
    "            # Step 3: Approximate inverse of attention mechanism\n",
    "            v_approx = attention_heads_output  # Assume V ≈ attention output\n",
    "            \n",
    "            # Reshape back\n",
    "            v_concat = v_approx.view(batch_size, seq_len, self.hidden_size)\n",
    "            \n",
    "            # Step 4: Inverse of V projection to get original input\n",
    "            v_proj_weight_pinv = torch.pinverse(layer.self_attn.v_proj.weight)\n",
    "            input_from_v = torch.matmul(v_concat, v_proj_weight_pinv.T)\n",
    "            \n",
    "            # For simplicity, use V projection estimate\n",
    "            attention_input = input_from_v\n",
    "        \n",
    "        return attention_input\n",
    "    \n",
    "    def precise_inverse_transformer_layer(self, layer_output, layer_idx, stored_residuals=None):\n",
    "        layer = self.model.model.layers[layer_idx]\n",
    "        \n",
    "        current = layer_output\n",
    "        \n",
    "        # Step 1: Approximate residual split\n",
    "        # residual_2 = residual_1 + mlp_output\n",
    "        if stored_residuals and layer_idx in stored_residuals:\n",
    "            residual_1 = stored_residuals[layer_idx]['post_attention']\n",
    "        else:\n",
    "            residual_1 = current * 0.5  # Rough approximation\n",
    "        \n",
    "        mlp_output = current - residual_1\n",
    "        \n",
    "        # Step 2: Inverse of post-attention LayerNorm\n",
    "        post_attn_ln_input = self.precise_inverse_layernorm(\n",
    "            mlp_output, \n",
    "            layer.post_attention_layernorm.weight\n",
    "        )\n",
    "        \n",
    "        # Step 3: Inverse of MLP\n",
    "        mlp_input = self.precise_inverse_mlp(post_attn_ln_input, layer_idx)\n",
    "        \n",
    "        # Step 4: Approximate first residual split\n",
    "        if stored_residuals and layer_idx in stored_residuals:\n",
    "            layer_input = stored_residuals[layer_idx]['input']\n",
    "        else:\n",
    "            layer_input = mlp_input * 0.5  # Rough approximation\n",
    "        \n",
    "        attention_output = mlp_input - layer_input\n",
    "        \n",
    "        # Step 5: Inverse of input LayerNorm\n",
    "        input_ln_input = self.precise_inverse_layernorm(\n",
    "            attention_output,\n",
    "            layer.input_layernorm.weight\n",
    "        )\n",
    "        \n",
    "        # Step 6: Inverse of attention\n",
    "        attention_input = self.precise_inverse_attention(input_ln_input, layer_idx)\n",
    "        \n",
    "        return attention_input\n",
    "    \n",
    "    def full_inverse_transform(self, final_logits, target_layer_depth=3):\n",
    "        current_activation = final_logits\n",
    "        \n",
    "        # Step 1: Inverse of LM head\n",
    "        current_activation = self.precise_inverse_lm_head(current_activation)\n",
    "        \n",
    "        # Step 2: Inverse of final layer norm\n",
    "        current_activation = self.precise_inverse_layernorm(\n",
    "            current_activation,\n",
    "            self.model.model.norm.weight\n",
    "        )\n",
    "        \n",
    "        # Step 3: Inverse through transformer layers (backwards)\n",
    "        reconstructed_layer_activations = {}\n",
    "        \n",
    "        # Go backwards from last layer to target depth\n",
    "        num_layers = len(self.model.model.layers)\n",
    "        for layer_idx in range(num_layers - 1, target_layer_depth - 1, -1):\n",
    "            current_activation = self.precise_inverse_transformer_layer(\n",
    "                current_activation, \n",
    "                layer_idx\n",
    "            )\n",
    "            \n",
    "            # Store activation for this layer\n",
    "            reconstructed_layer_activations[layer_idx] = current_activation.detach().clone()\n",
    "            \n",
    "            # Clear cache periodically\n",
    "            if layer_idx % 5 == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        return reconstructed_layer_activations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f6b0ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to capture original activations\n",
    "def get_activation(name, activations_dict):\n",
    "    def hook(module, input, output):\n",
    "        if isinstance(output, tuple):\n",
    "            activations_dict[name] = output[0].detach().clone()\n",
    "        else:\n",
    "            activations_dict[name] = output.detach().clone()\n",
    "    return hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8132554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample inputs\n",
    "def generate_sample_inputs(tokenizer, n_samples=20, seq_length=8):\n",
    "    sample_texts = [\n",
    "        \"The cat sat on\",\n",
    "        \"Machine learning is\", \n",
    "        \"Climate change affects\",\n",
    "        \"Deep networks can\",\n",
    "        \"Natural language processing\",\n",
    "        \"Computer vision detects\",\n",
    "        \"Quantum computing enables\",\n",
    "        \"Blockchain provides secure\",\n",
    "        \"Renewable energy sources\",\n",
    "        \"Medical research shows\"\n",
    "    ]\n",
    "    \n",
    "    inputs = []\n",
    "    for i in range(n_samples):\n",
    "        base_text = sample_texts[i % len(sample_texts)]\n",
    "        tokenized = tokenizer(\n",
    "            base_text,\n",
    "            return_tensors=\"pt\",\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=seq_length\n",
    "        )\n",
    "        inputs.append(tokenized.input_ids.to(model.device))\n",
    "    \n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "305a31cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Main analysis function with memory management\n",
    "def generate_activation_differences_precise_inverse(model, X_data, n_samples=10, n_reconstructions=5):\n",
    "    results = []\n",
    "    \n",
    "    # Clear memory before starting\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Initialize inverse transformer (now memory efficient)\n",
    "    inverse_transformer = PreciseLlamaInverseTransform(model)\n",
    "    \n",
    "    # Target layers to analyze\n",
    "    target_layers = [0, 1, 2]\n",
    "    \n",
    "    for sample_idx in tqdm(range(min(n_samples, len(X_data))), desc=\"Processing samples\"):\n",
    "        original_input = X_data[sample_idx]\n",
    "        \n",
    "        # Get original activations for comparison\n",
    "        original_activations = {}\n",
    "        hooks = []\n",
    "        \n",
    "        layer_names = [f'model.layers.{i}' for i in target_layers]\n",
    "        for layer_name in layer_names:\n",
    "            layer_module = model\n",
    "            for attr in layer_name.split('.'):\n",
    "                layer_module = getattr(layer_module, attr)\n",
    "            hooks.append(layer_module.register_forward_hook(\n",
    "                get_activation(layer_name, original_activations)\n",
    "            ))\n",
    "        \n",
    "        # Original forward pass\n",
    "        with torch.no_grad():\n",
    "            original_output = model(original_input).logits\n",
    "        \n",
    "        # Remove hooks\n",
    "        for hook in hooks:\n",
    "            hook.remove()\n",
    "        \n",
    "        # Multiple reconstructions with different perturbations\n",
    "        for recon_idx in range(n_reconstructions):\n",
    "            try:\n",
    "                # Add controlled perturbation for different reconstructions\n",
    "                noise_scale = 0.001 * (recon_idx + 1)\n",
    "                perturbed_logits = original_output + torch.randn_like(original_output) * noise_scale\n",
    "                \n",
    "                # PRECISE INVERSE TRANSFORM (now memory efficient)\n",
    "                reconstructed_activations = inverse_transformer.full_inverse_transform(\n",
    "                    perturbed_logits, \n",
    "                    target_layer_depth=max(target_layers)\n",
    "                )\n",
    "                \n",
    "                # Calculate differences\n",
    "                row = {'sample_idx': sample_idx, 'reconstruction_idx': recon_idx}\n",
    "                all_layer_max_diffs = []\n",
    "                \n",
    "                for layer_idx in target_layers:\n",
    "                    layer_name = f'model.layers.{layer_idx}'\n",
    "                    \n",
    "                    if (layer_name in original_activations and \n",
    "                        layer_idx in reconstructed_activations):\n",
    "                        \n",
    "                        orig_act = original_activations[layer_name].flatten().float()\n",
    "                        recon_act = reconstructed_activations[layer_idx].flatten().float()\n",
    "                        \n",
    "                        # Ensure same size\n",
    "                        min_size = min(orig_act.shape[0], recon_act.shape[0])\n",
    "                        orig_act = orig_act[:min_size]\n",
    "                        recon_act = recon_act[:min_size]\n",
    "                        \n",
    "                        abs_diff = torch.abs(orig_act - recon_act)\n",
    "                        \n",
    "                        row[f'layer_{layer_idx}_min_abs_diff'] = abs_diff.min().item()\n",
    "                        row[f'layer_{layer_idx}_mean_abs_diff'] = abs_diff.mean().item()\n",
    "                        row[f'layer_{layer_idx}_max_abs_diff'] = abs_diff.max().item()\n",
    "                        \n",
    "                        all_layer_max_diffs.append(abs_diff.max().item())\n",
    "                \n",
    "                # Aggregate metrics\n",
    "                if all_layer_max_diffs:\n",
    "                    row['all_layers_max_diff'] = max(all_layer_max_diffs)\n",
    "                    row['all_layers_min_of_max'] = min(all_layer_max_diffs)\n",
    "                \n",
    "                results.append(row)\n",
    "                \n",
    "                print(f\"Sample {sample_idx}, Recon {recon_idx}: Max diff = {row.get('all_layers_max_diff', 'N/A')}\")\n",
    "                \n",
    "                # Clear cache after each reconstruction\n",
    "                torch.cuda.empty_cache()\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error in sample {sample_idx}, reconstruction {recon_idx}: {e}\")\n",
    "                torch.cuda.empty_cache()\n",
    "                continue\n",
    "    \n",
    "    return pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790e5532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating sample inputs...\n",
      "Generated 8 samples\n",
      "\n",
      "Running precise inverse transform analysis...\n",
      "Initialized with direct parameter access (memory efficient)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing samples:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# Run the analysis\n",
    "print(\"Generating sample inputs...\")\n",
    "X_data = generate_sample_inputs(tokenizer, n_samples=8, seq_length=6)  # Small for testing\n",
    "print(f\"Generated {len(X_data)} samples\")\n",
    "\n",
    "print(\"\\nRunning precise inverse transform analysis...\")\n",
    "results = generate_activation_differences_precise_inverse(model, X_data, n_samples=5, n_reconstructions=3)\n",
    "\n",
    "# Save results\n",
    "results.to_csv('llama2_precise_inverse_results.csv', index=False)\n",
    "print(f\"\\nResults saved. Shape: {results.shape}\")\n",
    "if len(results) > 0:\n",
    "    print(\"\\nFirst few rows:\")\n",
    "    print(results.head())\n",
    "    print(f\"\\nSample statistics:\")\n",
    "    print(f\"Mean max diff across all layers: {results['all_layers_max_diff'].mean():.6f}\")\n",
    "    print(f\"Min max diff: {results['all_layers_max_diff'].min():.6f}\")\n",
    "    print(f\"Max max diff: {results['all_layers_max_diff'].max():.6f}\")\n",
    "else:\n",
    "    print(\"No results generated - check for errors above\")\n",
    "\n",
    "print(\"\\nPrecise inverse transform analysis with full parameter access completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6f868a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "254"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "660984d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory allocated: 0.00 GB\n",
      "GPU memory cached: 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Clear memory and set memory management\n",
    "import gc\n",
    "import os\n",
    "import torch\n",
    "# Set PyTorch memory management\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "# Clear all caches\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# Check memory\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU memory allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "    print(f\"GPU memory cached: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b38d79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b63ff48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1563b2af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
