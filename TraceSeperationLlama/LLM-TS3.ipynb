{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e10562b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hskay\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "import os\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "import json\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8863042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "MODEL_1_PATH = \"meta-llama/Llama-2-7b-chat-hf\" \n",
    "MODEL_2_PATH = \"meta-llama/Llama-2-7b-hf\"       \n",
    "\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3911279d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  6.57it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  3.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models loaded successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# %%\n",
    "tokenizer = LlamaTokenizer.from_pretrained(MODEL_1_PATH)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model_1 = LlamaForCausalLM.from_pretrained(\n",
    "    MODEL_1_PATH,\n",
    "    dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "model_2 = LlamaForCausalLM.from_pretrained(\n",
    "    MODEL_2_PATH,\n",
    "    dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "print(\"Models loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2ea8373",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = model_1.to(DEVICE)\n",
    "model_2 = model_2.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "854f9fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables for detailed activation capture\n",
    "captured_activations = {}\n",
    "current_hooks = []\n",
    "hook_errors = []\n",
    "\n",
    "def clear_activations():\n",
    "    global captured_activations\n",
    "    captured_activations.clear()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "def remove_all_hooks():\n",
    "    global current_hooks\n",
    "    for hook in current_hooks:\n",
    "        try:\n",
    "            hook.remove()\n",
    "        except:\n",
    "            pass\n",
    "    current_hooks.clear()\n",
    "\n",
    "def get_activation_hook(name):\n",
    "    def hook(module, input, output):\n",
    "        global hook_errors\n",
    "        try:\n",
    "            activation = output[0] if isinstance(output, tuple) else output\n",
    "            input_tensor = input[0] if isinstance(input, tuple) and len(input) > 0 else None\n",
    "\n",
    "            captured_activations[name] = {\n",
    "                'output': activation.detach().cpu() if activation is not None else None,\n",
    "                'input': input_tensor.detach().cpu() if input_tensor is not None else None,\n",
    "                'weight': module.weight.detach().cpu() if hasattr(module, 'weight') and module.weight is not None else None,\n",
    "                'bias': module.bias.detach().cpu() if hasattr(module, 'bias') and module.bias is not None else None\n",
    "            }\n",
    "        except Exception as e:\n",
    "            error_msg = f\"Hook error in {name}: {str(e)}\"\n",
    "            hook_errors.append(error_msg)\n",
    "            captured_activations[name] = {'output': None, 'input': None, 'weight': None, 'bias': None}\n",
    "    return hook\n",
    "\n",
    "def register_llama_hooks(model):\n",
    "    global current_hooks\n",
    "    remove_all_hooks() # clear any old hooks first\n",
    "    hook_errors.clear()\n",
    "\n",
    "    total_layers = len(model.model.layers)\n",
    "\n",
    "    for i in range(total_layers):\n",
    "        layer = model.model.layers[i]\n",
    "        layer_prefix = f\"layer_{i}\"\n",
    "        components = [\n",
    "            (layer.self_attn.q_proj, f\"{layer_prefix}_attention_q\"), (layer.self_attn.k_proj, f\"{layer_prefix}_attention_k\"),\n",
    "            (layer.self_attn.v_proj, f\"{layer_prefix}_attention_v\"), (layer.self_attn.o_proj, f\"{layer_prefix}_attention_output\"),\n",
    "            (layer.mlp.gate_proj, f\"{layer_prefix}_mlp_gate\"), (layer.mlp.up_proj, f\"{layer_prefix}_mlp_up\"),\n",
    "            (layer.mlp.down_proj, f\"{layer_prefix}_mlp_down\"), (layer.input_layernorm, f\"{layer_prefix}_input_norm\"),\n",
    "            (layer.post_attention_layernorm, f\"{layer_prefix}_post_attn_norm\"),\n",
    "        ]\n",
    "        for module, name in components:\n",
    "            current_hooks.append(module.register_forward_hook(get_activation_hook(name)))\n",
    "    \n",
    "    current_hooks.append(model.model.norm.register_forward_hook(get_activation_hook(\"final_norm\")))\n",
    "    current_hooks.append(model.lm_head.register_forward_hook(get_activation_hook(\"lm_head\")))\n",
    "    print(f\"Registered {len(current_hooks)} hooks.\")\n",
    "\n",
    "def run_model_and_capture_activations(model, inputs=None, inputs_embeds=None):\n",
    "    clear_activations()\n",
    "    register_llama_hooks(model)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        if inputs is not None:\n",
    "            _ = model(**inputs)\n",
    "        elif inputs_embeds is not None:\n",
    "            _ = model(inputs_embeds=inputs_embeds)\n",
    "        else:\n",
    "            raise ValueError(\"Either inputs or inputs_embeds must be provided.\")\n",
    "            \n",
    "    remove_all_hooks()\n",
    "    \n",
    "    # return a copy of the captured activations\n",
    "    return captured_activations.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7769aea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def get_component_type(layer_name):\n",
    "    if 'attention' in layer_name:\n",
    "        return 'attention'\n",
    "    elif 'mlp' in layer_name:\n",
    "        return 'mlp'\n",
    "    elif 'norm' in layer_name:\n",
    "        return 'normalization'\n",
    "    elif 'lm_head' in layer_name:\n",
    "        return 'output'\n",
    "    elif 'embed' in layer_name:\n",
    "        return 'embedding'\n",
    "    else:\n",
    "        return 'other'\n",
    "    \n",
    "def calculate_layer_output(\n",
    "    layer_name: str,\n",
    "    token_input: torch.Tensor,\n",
    "    weight: torch.Tensor,\n",
    "    bias: Optional[torch.Tensor]\n",
    ") -> Tuple[Optional[torch.Tensor], str]:\n",
    "\n",
    "    if token_input is None or weight is None:\n",
    "        return None, \"Missing input or weight\"\n",
    "\n",
    "    try:\n",
    "        # Case 1: normalization layer (LayerNorm/RMSNorm)\n",
    "        if 'norm' in layer_name:\n",
    "            # PyTorch's functional LayerNorm which handles the formula:\n",
    "            # y = (x - E[x]) / sqrt(Var[x] + eps) * gamma + beta\n",
    "            calculated_output = F.layer_norm(\n",
    "                token_input,\n",
    "                normalized_shape=[token_input.shape[0]],\n",
    "                weight=weight,\n",
    "                bias=bias,\n",
    "                eps=1e-5 # standard epsilon for Llama models\n",
    "            )\n",
    "            return calculated_output, \"Success\"\n",
    "\n",
    "        # Case 2: linear projection (Attention, MLP, etc.)\n",
    "        else:\n",
    "            # y = x @ W^T + b\n",
    "            calculated_output = F.linear(token_input, weight, bias)\n",
    "\n",
    "            # Apply the SiLU activation function for specific MLP layers\n",
    "            if 'mlp_gate' in layer_name: #or 'mlp_up' in layer_name:\n",
    "                calculated_output = F.silu(calculated_output)\n",
    "\n",
    "            return calculated_output, \"Success\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return None, f\"Calculation failed: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d7c659",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_calculation_vs_real_outputs(\n",
    "    original_activations: Dict[str, Dict[str, torch.Tensor]],\n",
    "    other_activations: Dict[str, Dict[str, torch.Tensor]],\n",
    "    mode:str,\n",
    "    n_rounds: int,\n",
    "    token_pos: int\n",
    ") -> List[Dict[str, Any]]:    \n",
    "    all_results = []\n",
    "    if mode == 'min':\n",
    "        for layer_name in original_activations.keys():\n",
    "            \n",
    "            if layer_name not in other_activations:\n",
    "                continue\n",
    "                \n",
    "            orig_data = original_activations[layer_name]\n",
    "            recon_data = other_activations[layer_name]\n",
    "            \n",
    "            if not all(k in orig_data and orig_data[k] is not None for k in ['input', 'weight', 'output']) or \\\n",
    "            not all(k in recon_data and recon_data[k] is not None for k in ['weight', 'output']):\n",
    "                continue\n",
    "\n",
    "            # Get the single input vector from the ORIGINAL data\n",
    "            # token_pos = orig_data['input'].shape[1] - 1  # will need to update here dont forgetttttttt !HSK!\n",
    "            recon_token_input = recon_data['input'][0, token_pos, :]\n",
    "            orig_token_input = orig_data['input'][0, token_pos, :]\n",
    "\n",
    "            # --- 1. Analyze the Original Run ---\n",
    "            calc_orig, status_orig = calculate_layer_output(\n",
    "                layer_name, orig_token_input, orig_data['weight'], orig_data.get('bias')\n",
    "            )\n",
    "            if calc_orig is None: \n",
    "                print(f\"Skipping {layer_name} (original): {status_orig}\")\n",
    "                continue\n",
    "            \n",
    "            real_orig = orig_data['output'][0, token_pos, :]\n",
    "            error_vector_orig = calc_orig - real_orig\n",
    "\n",
    "            # --- 2. Analyze the Reconstructed Run (using ORIGINAL input) ---\n",
    "            calc_recon, status_recon = calculate_layer_output(\n",
    "                layer_name, recon_token_input, orig_data['weight'], orig_data.get('bias')\n",
    "            )\n",
    "            if calc_recon is None: continue\n",
    "            \n",
    "            real_recon = recon_data['output'][0, token_pos, :]\n",
    "            # This error shows how much the reconstructed model deviates from its\n",
    "            # own hooked output when given the original benign input.\n",
    "            error_vector_recon = calc_recon - real_recon\n",
    "            \n",
    "            # --- 3. Process results for both runs ---\n",
    "            \n",
    "            # Find min and random indices for the ORIGINAL run's error\n",
    "            min_err_idx_orig = torch.argmin(error_vector_orig.abs()).item()\n",
    "            #rand_idx_orig = torch.randint(0, len(error_vector_orig), (1,)).item() REMOVEW\n",
    "            \n",
    "            all_results.append({\n",
    "                'round': -1,\n",
    "                'layer_name': layer_name, 'run_type': 'original',\n",
    "                'error_index': min_err_idx_orig,\n",
    "                'error_real_value': real_orig[min_err_idx_orig].item(),\n",
    "                'error_calc_value': calc_orig[min_err_idx_orig].item(),\n",
    "            })\n",
    "\n",
    "            # Find min and random indices for the RECONSTRUCTED run's error\n",
    "            min_err_idx_recon = torch.argmin(error_vector_recon.abs()).item()\n",
    "            \n",
    "            all_results.append({\n",
    "                'round': -1,\n",
    "                'layer_name': layer_name, 'run_type': 'other',\n",
    "                'error_index': min_err_idx_recon,\n",
    "                'error_real_value': real_recon[min_err_idx_recon].item(),\n",
    "                'error_calc_value': calc_recon[min_err_idx_recon].item(),\n",
    "            })\n",
    "    else:\n",
    "        for round in range(n_rounds):\n",
    "            print(f\"Analysis round {round+1}/{n_rounds}...\"+\"\\r\",end=\"\")\n",
    "            for layer_name in original_activations.keys():\n",
    "                if layer_name not in other_activations:\n",
    "                    continue\n",
    "                    \n",
    "                orig_data = original_activations[layer_name]\n",
    "                recon_data = other_activations[layer_name]\n",
    "                \n",
    "                if not all(k in orig_data and orig_data[k] is not None for k in ['input', 'weight', 'output']) or \\\n",
    "                not all(k in recon_data and recon_data[k] is not None for k in ['weight', 'output']):\n",
    "                    continue\n",
    "\n",
    "                #token_pos = orig_data['input'].shape[1] - 1\n",
    "                recon_token_input = recon_data['input'][0, token_pos, :]\n",
    "                orig_token_input = orig_data['input'][0, token_pos, :]\n",
    "                num_neurons = orig_data['output'].shape[2]\n",
    "                rand_idx = torch.randint(0, num_neurons, (1,)).item()\n",
    "                \n",
    "                # --- Handle Norm layers separately, as they need the full input context ---\n",
    "                if 'norm' in layer_name:\n",
    "                    calc_orig, _ = calculate_layer_output(layer_name, orig_token_input, orig_data['weight'], orig_data.get('bias'))\n",
    "                    calc_recon, _ = calculate_layer_output(layer_name, recon_token_input, orig_data['weight'], orig_data.get('bias'))\n",
    "\n",
    "                    calc_orig = calc_orig[rand_idx].item() if calc_orig is not None else None\n",
    "                    calc_recon = calc_recon[rand_idx].item() if calc_recon is not None else None\n",
    "                \n",
    "                else:\n",
    "                    # Slice the weight and bias for the randomly selected neuron\n",
    "                    single_row_weight_orig = orig_data['weight'][rand_idx, :].unsqueeze(0) # Shape: [1, in_features]\n",
    "                    single_row_weight_recon = recon_data['weight'][rand_idx, :].unsqueeze(0)\n",
    "                    \n",
    "                    bias_orig = orig_data.get('bias')\n",
    "                    single_value_bias_orig = bias_orig[rand_idx].unsqueeze(0) if bias_orig is not None else None # Shape: [1]\n",
    "                    \n",
    "                    bias_recon = recon_data.get('bias')\n",
    "                    single_value_bias_recon = bias_recon[rand_idx].unsqueeze(0) if bias_recon is not None else None\n",
    "\n",
    "                    # Calculate output for the single neuron by passing its sliced weights\n",
    "                    calc_orig_tensor, _ = calculate_layer_output(layer_name, orig_token_input, single_row_weight_orig, single_value_bias_orig)\n",
    "                    calc_recon_tensor, _ = calculate_layer_output(layer_name, recon_token_input, single_row_weight_orig, single_value_bias_orig)\n",
    "                    \n",
    "                    # The result is a tensor with one value, so we extract it\n",
    "                    calc_orig = calc_orig_tensor.item() if calc_orig_tensor is not None else None\n",
    "                    calc_recon = calc_recon_tensor.item() if calc_recon_tensor is not None else None\n",
    "\n",
    "                # --- Append results for the single random neuron ---\n",
    "                if calc_orig is not None:\n",
    "                    real_orig = orig_data['output'][0, token_pos, rand_idx].item()\n",
    "                    all_results.append({\n",
    "                        'round': round,\n",
    "                        'layer_name': layer_name, 'run_type': 'original',\n",
    "                        'error_index': rand_idx,\n",
    "                        'error_real_value': real_orig,\n",
    "                        'error_calc_value': calc_orig,\n",
    "                    })\n",
    "\n",
    "                if calc_recon is not None:\n",
    "                    real_recon = recon_data['output'][0, token_pos, rand_idx].item()\n",
    "                    all_results.append({\n",
    "                        'round': round,\n",
    "                        'layer_name': layer_name, 'run_type': 'other', \n",
    "                        'error_index': rand_idx,\n",
    "                        'error_real_value': real_recon,\n",
    "                        'error_calc_value': calc_recon,\n",
    "                    })\n",
    "        print()    \n",
    "\n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6cc76ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def save_analysis_results(\n",
    "    results_list: List[Dict[str, Any]],\n",
    "    input: str,\n",
    "    token_pos: int,\n",
    "    filename: str = \"formula_analysis.csv\"\n",
    "):\n",
    "    if not results_list:\n",
    "        return\n",
    "        \n",
    "    df = pd.DataFrame(results_list)\n",
    "    df.insert(0, 'token_pos', token_pos)\n",
    "    df.insert(0, 'input', input)\n",
    "    \n",
    "    # Append to the file if it exists, otherwise create it\n",
    "    if os.path.exists(filename):\n",
    "        df.to_csv(filename, mode='a', header=False, index=False)\n",
    "    else:\n",
    "        df.to_csv(filename, mode='w', header=True, index=False)\n",
    "    \n",
    "    print(f\"--- Saved {len(df)} analysis rows to {filename} ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e3e929",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test_and_analysis_workflow(\n",
    "    original_model: \"LlamaForCausalLM\",\n",
    "    other_model: \"LlamaForCausalLM\",\n",
    "    tokenizer: \"LlamaTokenizer\",\n",
    "    string_input: str,\n",
    "    n_test_rounds: int = 1,\n",
    "):\n",
    "    sample_input = tokenizer(string_input[1],return_tensors=\"pt\")\n",
    "    inputs_on_device = {k: v.to(original_model.device) for k, v in sample_input.items()}\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Input: '{tokenizer.decode(inputs_on_device['input_ids'][0])}'\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # --- Step 1: Get Original State ---\n",
    "    original_activations = run_model_and_capture_activations(\n",
    "        original_model, inputs=inputs_on_device\n",
    "    )\n",
    "\n",
    "    other_activations = run_model_and_capture_activations(\n",
    "        other_model, inputs=inputs_on_device\n",
    "    )\n",
    "   \n",
    "    for token_pos in range(inputs_on_device[\"input_ids\"].shape[1]):\n",
    "        # --- Step 2: Run Deep Analysis (using original input) ---\n",
    "        print(\"Running deep calculation analysis...\")\n",
    "        analysis_results = analyze_calculation_vs_real_outputs(\n",
    "                original_activations,\n",
    "                other_activations,\n",
    "                mode='min',\n",
    "                n_rounds=-1,\n",
    "                token_pos=token_pos\n",
    "            )\n",
    "        \n",
    "        analysis_results.extend(analyze_calculation_vs_real_outputs(\n",
    "            original_activations,\n",
    "            other_activations,\n",
    "            mode='random',\n",
    "            n_rounds=n_test_rounds,\n",
    "            token_pos=token_pos\n",
    "        ))\n",
    "        # --- Step 6: Save Results ---\n",
    "        save_analysis_results(analysis_results, string_input[0],token_pos)\n",
    "    \n",
    "        # Clean up memory\n",
    "        del analysis_results\n",
    "        if 'clear_activations' in globals():\n",
    "            globals()['clear_activations']()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbfda25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "sample_texts = [\n",
    "    [1,\"The capital of France is\"],\n",
    "    [2,\"The largest mammal on Earth is\"],\n",
    "    [3,\"The process of photosynthesis occurs in\"],\n",
    "    [4,\"The speed of light in a vacuum is\"],\n",
    "    [5,\"The chemical symbol for gold is\"],\n",
    "    [6,\"The human body has how many bones\"],\n",
    "    [7,\"The Great Wall of China was built to\"],\n",
    "    [8,\"Water boils at what temperature\"],\n",
    "    [9,\"The smallest unit of matter is\"],\n",
    "    [10,\"Shakespeare wrote the play\"],\n",
    "    [11,\"The currency of Japan is\"],\n",
    "    [12,\"Mount Everest is located in\"],\n",
    "    [13,\"The inventor of the telephone was\"],\n",
    "    [14,\"DNA stands for\"],\n",
    "    [15,\"The largest ocean on Earth is\"],\n",
    "    [16,\"The planet closest to the Sun is\"],\n",
    "    [17,\"Gravity was discovered by\"],\n",
    "    [18,\"The Amazon rainforest is primarily located in\"],\n",
    "    [19,\"The freezing point of water is\"],\n",
    "    [20,\"The most abundant gas in Earth's atmosphere is\"],\n",
    "    [21,\"The Mona Lisa was painted by\"],\n",
    "    [22,\"The longest river in the world is\"],\n",
    "    [23,\"Photosynthesis converts carbon dioxide and water into\"],\n",
    "    [24,\"The study of earthquakes is called\"],\n",
    "    [25,\"The first person to walk on the moon was\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb31afe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing text 1/5 ===\n",
      "Processing: The quick brown fox jumps over the lazy dog....\n",
      "Input tokens: 13\n",
      "Registering hooks...\n",
      "Running models...\n",
      "Captured 290 activations from Model 1\n",
      "Captured 290 activations from Model 2\n",
      "Selecting random neurons...\n",
      "Selected neurons from 290 layers\n",
      "Comparing activations...\n",
      "Visualization saved to neuron_comparison_text_0.png\n",
      "Completed text 1\n",
      "\n",
      "=== Processing text 2/5 ===\n",
      "Processing: Artificial intelligence is transforming the world ...\n",
      "Input tokens: 13\n",
      "Registering hooks...\n",
      "Running models...\n",
      "Captured 290 activations from Model 1\n",
      "Captured 290 activations from Model 2\n",
      "Selecting random neurons...\n",
      "Selected neurons from 290 layers\n",
      "Comparing activations...\n",
      "Visualization saved to neuron_comparison_text_1.png\n",
      "Completed text 2\n",
      "\n",
      "=== Processing text 3/5 ===\n",
      "Processing: In a hole in the ground there lived a hobbit....\n",
      "Input tokens: 14\n",
      "Registering hooks...\n",
      "Running models...\n",
      "Captured 290 activations from Model 1\n",
      "Captured 290 activations from Model 2\n",
      "Selecting random neurons...\n",
      "Selected neurons from 290 layers\n",
      "Comparing activations...\n",
      "Visualization saved to neuron_comparison_text_2.png\n",
      "Completed text 3\n",
      "\n",
      "=== Processing text 4/5 ===\n",
      "Processing: To be or not to be, that is the question Shakespea...\n",
      "Input tokens: 16\n",
      "Registering hooks...\n",
      "Running models...\n",
      "Captured 290 activations from Model 1\n",
      "Captured 290 activations from Model 2\n",
      "Selecting random neurons...\n",
      "Selected neurons from 290 layers\n",
      "Comparing activations...\n",
      "Visualization saved to neuron_comparison_text_3.png\n",
      "Completed text 4\n",
      "\n",
      "=== Processing text 5/5 ===\n",
      "Processing: Machine learning models require large datasets for...\n",
      "Input tokens: 10\n",
      "Registering hooks...\n",
      "Running models...\n",
      "Captured 290 activations from Model 1\n",
      "Captured 290 activations from Model 2\n",
      "Selecting random neurons...\n",
      "Selected neurons from 290 layers\n",
      "Comparing activations...\n",
      "Visualization saved to neuron_comparison_text_4.png\n",
      "Completed text 5\n"
     ]
    }
   ],
   "source": [
    "for i, prompt in enumerate(sample_texts):\n",
    "    \n",
    "    print(f\"\\n>>>> Starting Analysis for Prompt {i+1}<<<<\")\n",
    "    run_test_and_analysis_workflow(\n",
    "        original_model=model_1,\n",
    "        other_model=model_2,\n",
    "        tokenizer=tokenizer,\n",
    "        string_input=prompt,\n",
    "        n_test_rounds=5000,\n",
    "    )\n",
    "\n",
    "print(\"\\n\\n<<<< ALL TESTS COMPLETE >>>>\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
