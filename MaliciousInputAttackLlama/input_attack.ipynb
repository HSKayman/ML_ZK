{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0de5e5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hskay\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import gc\n",
    "from collections import defaultdict\n",
    "from typing import List, Dict, Any, Optional, Tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "996ac4a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "MODEL_PATH = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c302d7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = LlamaTokenizer.from_pretrained(MODEL_PATH)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2680ffe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.37s/it]\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = LlamaTokenizer.from_pretrained(MODEL_PATH)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f158c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAYER EXECUTION ORDER:\n",
      "====================================================================================================\n",
      "  1. model.embed_tokens                                 | Embedding            | torch.Size([1, 3]) → torch.Size([1, 3, 4096])\n",
      "  2. model.rotary_emb                                   | LlamaRotaryEmbedding | torch.Size([1, 3, 4096]) → tuple\n",
      "  3. model.layers.0.input_layernorm                     | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "  4. model.layers.0.self_attn.q_proj                    | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "  5. model.layers.0.self_attn.k_proj                    | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "  6. model.layers.0.self_attn.v_proj                    | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "  7. model.layers.0.self_attn.o_proj                    | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "  8. model.layers.0.post_attention_layernorm            | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "  9. model.layers.0.mlp.gate_proj                       | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      " 10. model.layers.0.mlp.act_fn                          | SiLUActivation       | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 11008])\n",
      " 11. model.layers.0.mlp.up_proj                         | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      " 12. model.layers.0.mlp.down_proj                       | Linear               | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 4096])\n",
      " 13. model.layers.1.input_layernorm                     | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      " 14. model.layers.1.self_attn.q_proj                    | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      " 15. model.layers.1.self_attn.k_proj                    | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      " 16. model.layers.1.self_attn.v_proj                    | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      " 17. model.layers.1.self_attn.o_proj                    | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      " 18. model.layers.1.post_attention_layernorm            | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      " 19. model.layers.1.mlp.gate_proj                       | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      " 20. model.layers.1.mlp.act_fn                          | SiLUActivation       | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 11008])\n",
      " 21. model.layers.1.mlp.up_proj                         | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      " 22. model.layers.1.mlp.down_proj                       | Linear               | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 4096])\n",
      " 23. model.layers.2.input_layernorm                     | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      " 24. model.layers.2.self_attn.q_proj                    | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      " 25. model.layers.2.self_attn.k_proj                    | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      " 26. model.layers.2.self_attn.v_proj                    | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      " 27. model.layers.2.self_attn.o_proj                    | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      " 28. model.layers.2.post_attention_layernorm            | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      " 29. model.layers.2.mlp.gate_proj                       | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      " 30. model.layers.2.mlp.act_fn                          | SiLUActivation       | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 11008])\n",
      " 31. model.layers.2.mlp.up_proj                         | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      " 32. model.layers.2.mlp.down_proj                       | Linear               | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 4096])\n",
      " 33. model.layers.3.input_layernorm                     | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      " 34. model.layers.3.self_attn.q_proj                    | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      " 35. model.layers.3.self_attn.k_proj                    | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      " 36. model.layers.3.self_attn.v_proj                    | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      " 37. model.layers.3.self_attn.o_proj                    | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      " 38. model.layers.3.post_attention_layernorm            | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      " 39. model.layers.3.mlp.gate_proj                       | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      " 40. model.layers.3.mlp.act_fn                          | SiLUActivation       | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 11008])\n",
      " 41. model.layers.3.mlp.up_proj                         | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      " 42. model.layers.3.mlp.down_proj                       | Linear               | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 4096])\n",
      " 43. model.layers.4.input_layernorm                     | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      " 44. model.layers.4.self_attn.q_proj                    | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      " 45. model.layers.4.self_attn.k_proj                    | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      " 46. model.layers.4.self_attn.v_proj                    | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      " 47. model.layers.4.self_attn.o_proj                    | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      " 48. model.layers.4.post_attention_layernorm            | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      " 49. model.layers.4.mlp.gate_proj                       | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      " 50. model.layers.4.mlp.act_fn                          | SiLUActivation       | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 11008])\n",
      " 51. model.layers.4.mlp.up_proj                         | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      " 52. model.layers.4.mlp.down_proj                       | Linear               | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 4096])\n",
      " 53. model.layers.5.input_layernorm                     | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      " 54. model.layers.5.self_attn.q_proj                    | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      " 55. model.layers.5.self_attn.k_proj                    | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      " 56. model.layers.5.self_attn.v_proj                    | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      " 57. model.layers.5.self_attn.o_proj                    | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      " 58. model.layers.5.post_attention_layernorm            | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      " 59. model.layers.5.mlp.gate_proj                       | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      " 60. model.layers.5.mlp.act_fn                          | SiLUActivation       | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 11008])\n",
      " 61. model.layers.5.mlp.up_proj                         | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      " 62. model.layers.5.mlp.down_proj                       | Linear               | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 4096])\n",
      " 63. model.layers.6.input_layernorm                     | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      " 64. model.layers.6.self_attn.q_proj                    | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      " 65. model.layers.6.self_attn.k_proj                    | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      " 66. model.layers.6.self_attn.v_proj                    | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      " 67. model.layers.6.self_attn.o_proj                    | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      " 68. model.layers.6.post_attention_layernorm            | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      " 69. model.layers.6.mlp.gate_proj                       | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      " 70. model.layers.6.mlp.act_fn                          | SiLUActivation       | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 11008])\n",
      " 71. model.layers.6.mlp.up_proj                         | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      " 72. model.layers.6.mlp.down_proj                       | Linear               | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 4096])\n",
      " 73. model.layers.7.input_layernorm                     | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      " 74. model.layers.7.self_attn.q_proj                    | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      " 75. model.layers.7.self_attn.k_proj                    | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      " 76. model.layers.7.self_attn.v_proj                    | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      " 77. model.layers.7.self_attn.o_proj                    | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      " 78. model.layers.7.post_attention_layernorm            | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      " 79. model.layers.7.mlp.gate_proj                       | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      " 80. model.layers.7.mlp.act_fn                          | SiLUActivation       | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 11008])\n",
      " 81. model.layers.7.mlp.up_proj                         | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      " 82. model.layers.7.mlp.down_proj                       | Linear               | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 4096])\n",
      " 83. model.layers.8.input_layernorm                     | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      " 84. model.layers.8.self_attn.q_proj                    | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      " 85. model.layers.8.self_attn.k_proj                    | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      " 86. model.layers.8.self_attn.v_proj                    | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      " 87. model.layers.8.self_attn.o_proj                    | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      " 88. model.layers.8.post_attention_layernorm            | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      " 89. model.layers.8.mlp.gate_proj                       | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      " 90. model.layers.8.mlp.act_fn                          | SiLUActivation       | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 11008])\n",
      " 91. model.layers.8.mlp.up_proj                         | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      " 92. model.layers.8.mlp.down_proj                       | Linear               | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 4096])\n",
      " 93. model.layers.9.input_layernorm                     | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      " 94. model.layers.9.self_attn.q_proj                    | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      " 95. model.layers.9.self_attn.k_proj                    | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      " 96. model.layers.9.self_attn.v_proj                    | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      " 97. model.layers.9.self_attn.o_proj                    | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      " 98. model.layers.9.post_attention_layernorm            | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      " 99. model.layers.9.mlp.gate_proj                       | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      "100. model.layers.9.mlp.act_fn                          | SiLUActivation       | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 11008])\n",
      "101. model.layers.9.mlp.up_proj                         | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      "102. model.layers.9.mlp.down_proj                       | Linear               | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 4096])\n",
      "103. model.layers.10.input_layernorm                    | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "104. model.layers.10.self_attn.q_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "105. model.layers.10.self_attn.k_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "106. model.layers.10.self_attn.v_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "107. model.layers.10.self_attn.o_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "108. model.layers.10.post_attention_layernorm           | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "109. model.layers.10.mlp.gate_proj                      | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      "110. model.layers.10.mlp.act_fn                         | SiLUActivation       | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 11008])\n",
      "111. model.layers.10.mlp.up_proj                        | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      "112. model.layers.10.mlp.down_proj                      | Linear               | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 4096])\n",
      "113. model.layers.11.input_layernorm                    | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "114. model.layers.11.self_attn.q_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "115. model.layers.11.self_attn.k_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "116. model.layers.11.self_attn.v_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "117. model.layers.11.self_attn.o_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "118. model.layers.11.post_attention_layernorm           | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "119. model.layers.11.mlp.gate_proj                      | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      "120. model.layers.11.mlp.act_fn                         | SiLUActivation       | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 11008])\n",
      "121. model.layers.11.mlp.up_proj                        | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      "122. model.layers.11.mlp.down_proj                      | Linear               | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 4096])\n",
      "123. model.layers.12.input_layernorm                    | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "124. model.layers.12.self_attn.q_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "125. model.layers.12.self_attn.k_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "126. model.layers.12.self_attn.v_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "127. model.layers.12.self_attn.o_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "128. model.layers.12.post_attention_layernorm           | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "129. model.layers.12.mlp.gate_proj                      | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      "130. model.layers.12.mlp.act_fn                         | SiLUActivation       | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 11008])\n",
      "131. model.layers.12.mlp.up_proj                        | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      "132. model.layers.12.mlp.down_proj                      | Linear               | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 4096])\n",
      "133. model.layers.13.input_layernorm                    | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "134. model.layers.13.self_attn.q_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "135. model.layers.13.self_attn.k_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "136. model.layers.13.self_attn.v_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "137. model.layers.13.self_attn.o_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "138. model.layers.13.post_attention_layernorm           | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "139. model.layers.13.mlp.gate_proj                      | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      "140. model.layers.13.mlp.act_fn                         | SiLUActivation       | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 11008])\n",
      "141. model.layers.13.mlp.up_proj                        | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      "142. model.layers.13.mlp.down_proj                      | Linear               | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 4096])\n",
      "143. model.layers.14.input_layernorm                    | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "144. model.layers.14.self_attn.q_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "145. model.layers.14.self_attn.k_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "146. model.layers.14.self_attn.v_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "147. model.layers.14.self_attn.o_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "148. model.layers.14.post_attention_layernorm           | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "149. model.layers.14.mlp.gate_proj                      | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      "150. model.layers.14.mlp.act_fn                         | SiLUActivation       | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 11008])\n",
      "151. model.layers.14.mlp.up_proj                        | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      "152. model.layers.14.mlp.down_proj                      | Linear               | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 4096])\n",
      "153. model.layers.15.input_layernorm                    | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "154. model.layers.15.self_attn.q_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "155. model.layers.15.self_attn.k_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "156. model.layers.15.self_attn.v_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "157. model.layers.15.self_attn.o_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "158. model.layers.15.post_attention_layernorm           | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "159. model.layers.15.mlp.gate_proj                      | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      "160. model.layers.15.mlp.act_fn                         | SiLUActivation       | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 11008])\n",
      "161. model.layers.15.mlp.up_proj                        | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      "162. model.layers.15.mlp.down_proj                      | Linear               | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 4096])\n",
      "163. model.layers.16.input_layernorm                    | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "164. model.layers.16.self_attn.q_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "165. model.layers.16.self_attn.k_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "166. model.layers.16.self_attn.v_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "167. model.layers.16.self_attn.o_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "168. model.layers.16.post_attention_layernorm           | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "169. model.layers.16.mlp.gate_proj                      | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      "170. model.layers.16.mlp.act_fn                         | SiLUActivation       | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 11008])\n",
      "171. model.layers.16.mlp.up_proj                        | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      "172. model.layers.16.mlp.down_proj                      | Linear               | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 4096])\n",
      "173. model.layers.17.input_layernorm                    | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "174. model.layers.17.self_attn.q_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "175. model.layers.17.self_attn.k_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "176. model.layers.17.self_attn.v_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "177. model.layers.17.self_attn.o_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "178. model.layers.17.post_attention_layernorm           | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "179. model.layers.17.mlp.gate_proj                      | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      "180. model.layers.17.mlp.act_fn                         | SiLUActivation       | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 11008])\n",
      "181. model.layers.17.mlp.up_proj                        | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      "182. model.layers.17.mlp.down_proj                      | Linear               | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 4096])\n",
      "183. model.layers.18.input_layernorm                    | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "184. model.layers.18.self_attn.q_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "185. model.layers.18.self_attn.k_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "186. model.layers.18.self_attn.v_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "187. model.layers.18.self_attn.o_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "188. model.layers.18.post_attention_layernorm           | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "189. model.layers.18.mlp.gate_proj                      | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      "190. model.layers.18.mlp.act_fn                         | SiLUActivation       | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 11008])\n",
      "191. model.layers.18.mlp.up_proj                        | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      "192. model.layers.18.mlp.down_proj                      | Linear               | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 4096])\n",
      "193. model.layers.19.input_layernorm                    | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "194. model.layers.19.self_attn.q_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "195. model.layers.19.self_attn.k_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "196. model.layers.19.self_attn.v_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "197. model.layers.19.self_attn.o_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "198. model.layers.19.post_attention_layernorm           | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "199. model.layers.19.mlp.gate_proj                      | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      "200. model.layers.19.mlp.act_fn                         | SiLUActivation       | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 11008])\n",
      "201. model.layers.19.mlp.up_proj                        | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      "202. model.layers.19.mlp.down_proj                      | Linear               | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 4096])\n",
      "203. model.layers.20.input_layernorm                    | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "204. model.layers.20.self_attn.q_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "205. model.layers.20.self_attn.k_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "206. model.layers.20.self_attn.v_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "207. model.layers.20.self_attn.o_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "208. model.layers.20.post_attention_layernorm           | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "209. model.layers.20.mlp.gate_proj                      | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      "210. model.layers.20.mlp.act_fn                         | SiLUActivation       | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 11008])\n",
      "211. model.layers.20.mlp.up_proj                        | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      "212. model.layers.20.mlp.down_proj                      | Linear               | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 4096])\n",
      "213. model.layers.21.input_layernorm                    | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "214. model.layers.21.self_attn.q_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "215. model.layers.21.self_attn.k_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "216. model.layers.21.self_attn.v_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "217. model.layers.21.self_attn.o_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "218. model.layers.21.post_attention_layernorm           | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "219. model.layers.21.mlp.gate_proj                      | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      "220. model.layers.21.mlp.act_fn                         | SiLUActivation       | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 11008])\n",
      "221. model.layers.21.mlp.up_proj                        | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      "222. model.layers.21.mlp.down_proj                      | Linear               | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 4096])\n",
      "223. model.layers.22.input_layernorm                    | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "224. model.layers.22.self_attn.q_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "225. model.layers.22.self_attn.k_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "226. model.layers.22.self_attn.v_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "227. model.layers.22.self_attn.o_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "228. model.layers.22.post_attention_layernorm           | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "229. model.layers.22.mlp.gate_proj                      | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      "230. model.layers.22.mlp.act_fn                         | SiLUActivation       | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 11008])\n",
      "231. model.layers.22.mlp.up_proj                        | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      "232. model.layers.22.mlp.down_proj                      | Linear               | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 4096])\n",
      "233. model.layers.23.input_layernorm                    | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "234. model.layers.23.self_attn.q_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "235. model.layers.23.self_attn.k_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "236. model.layers.23.self_attn.v_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "237. model.layers.23.self_attn.o_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "238. model.layers.23.post_attention_layernorm           | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "239. model.layers.23.mlp.gate_proj                      | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      "240. model.layers.23.mlp.act_fn                         | SiLUActivation       | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 11008])\n",
      "241. model.layers.23.mlp.up_proj                        | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      "242. model.layers.23.mlp.down_proj                      | Linear               | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 4096])\n",
      "243. model.layers.24.input_layernorm                    | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "244. model.layers.24.self_attn.q_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "245. model.layers.24.self_attn.k_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "246. model.layers.24.self_attn.v_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "247. model.layers.24.self_attn.o_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "248. model.layers.24.post_attention_layernorm           | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "249. model.layers.24.mlp.gate_proj                      | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      "250. model.layers.24.mlp.act_fn                         | SiLUActivation       | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 11008])\n",
      "251. model.layers.24.mlp.up_proj                        | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      "252. model.layers.24.mlp.down_proj                      | Linear               | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 4096])\n",
      "253. model.layers.25.input_layernorm                    | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "254. model.layers.25.self_attn.q_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "255. model.layers.25.self_attn.k_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "256. model.layers.25.self_attn.v_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "257. model.layers.25.self_attn.o_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "258. model.layers.25.post_attention_layernorm           | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "259. model.layers.25.mlp.gate_proj                      | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      "260. model.layers.25.mlp.act_fn                         | SiLUActivation       | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 11008])\n",
      "261. model.layers.25.mlp.up_proj                        | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      "262. model.layers.25.mlp.down_proj                      | Linear               | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 4096])\n",
      "263. model.layers.26.input_layernorm                    | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "264. model.layers.26.self_attn.q_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "265. model.layers.26.self_attn.k_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "266. model.layers.26.self_attn.v_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "267. model.layers.26.self_attn.o_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "268. model.layers.26.post_attention_layernorm           | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "269. model.layers.26.mlp.gate_proj                      | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      "270. model.layers.26.mlp.act_fn                         | SiLUActivation       | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 11008])\n",
      "271. model.layers.26.mlp.up_proj                        | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      "272. model.layers.26.mlp.down_proj                      | Linear               | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 4096])\n",
      "273. model.layers.27.input_layernorm                    | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "274. model.layers.27.self_attn.q_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "275. model.layers.27.self_attn.k_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "276. model.layers.27.self_attn.v_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "277. model.layers.27.self_attn.o_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "278. model.layers.27.post_attention_layernorm           | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "279. model.layers.27.mlp.gate_proj                      | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      "280. model.layers.27.mlp.act_fn                         | SiLUActivation       | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 11008])\n",
      "281. model.layers.27.mlp.up_proj                        | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      "282. model.layers.27.mlp.down_proj                      | Linear               | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 4096])\n",
      "283. model.layers.28.input_layernorm                    | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "284. model.layers.28.self_attn.q_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "285. model.layers.28.self_attn.k_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "286. model.layers.28.self_attn.v_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "287. model.layers.28.self_attn.o_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "288. model.layers.28.post_attention_layernorm           | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "289. model.layers.28.mlp.gate_proj                      | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      "290. model.layers.28.mlp.act_fn                         | SiLUActivation       | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 11008])\n",
      "291. model.layers.28.mlp.up_proj                        | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      "292. model.layers.28.mlp.down_proj                      | Linear               | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 4096])\n",
      "293. model.layers.29.input_layernorm                    | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "294. model.layers.29.self_attn.q_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "295. model.layers.29.self_attn.k_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "296. model.layers.29.self_attn.v_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "297. model.layers.29.self_attn.o_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "298. model.layers.29.post_attention_layernorm           | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "299. model.layers.29.mlp.gate_proj                      | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      "300. model.layers.29.mlp.act_fn                         | SiLUActivation       | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 11008])\n",
      "301. model.layers.29.mlp.up_proj                        | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      "302. model.layers.29.mlp.down_proj                      | Linear               | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 4096])\n",
      "303. model.layers.30.input_layernorm                    | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "304. model.layers.30.self_attn.q_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "305. model.layers.30.self_attn.k_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "306. model.layers.30.self_attn.v_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "307. model.layers.30.self_attn.o_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "308. model.layers.30.post_attention_layernorm           | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "309. model.layers.30.mlp.gate_proj                      | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      "310. model.layers.30.mlp.act_fn                         | SiLUActivation       | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 11008])\n",
      "311. model.layers.30.mlp.up_proj                        | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      "312. model.layers.30.mlp.down_proj                      | Linear               | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 4096])\n",
      "313. model.layers.31.input_layernorm                    | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "314. model.layers.31.self_attn.q_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "315. model.layers.31.self_attn.k_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "316. model.layers.31.self_attn.v_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "317. model.layers.31.self_attn.o_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "318. model.layers.31.post_attention_layernorm           | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "319. model.layers.31.mlp.gate_proj                      | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      "320. model.layers.31.mlp.act_fn                         | SiLUActivation       | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 11008])\n",
      "321. model.layers.31.mlp.up_proj                        | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      "322. model.layers.31.mlp.down_proj                      | Linear               | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 4096])\n",
      "323. model.norm                                         | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "324. lm_head                                            | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 32000])\n"
     ]
    }
   ],
   "source": [
    "# class ExecutionTracer:\n",
    "#     def __init__(self):\n",
    "#         self.execution_order = []\n",
    "#         self.counter = 0\n",
    "    \n",
    "#     def __call__(self, module, input, output):\n",
    "#         self.counter += 1\n",
    "#         module_name = None\n",
    "#         for name, mod in model.named_modules():\n",
    "#             if mod is module:\n",
    "#                 module_name = name\n",
    "#                 break\n",
    "        \n",
    "#         self.execution_order.append({\n",
    "#             'order': self.counter,\n",
    "#             'name': module_name,\n",
    "#             'type': module.__class__.__name__,\n",
    "#             'input_shape': input[0].shape if isinstance(input, tuple) and len(input) > 0 else 'special',\n",
    "#             'output_shape': output.shape if hasattr(output, 'shape') else type(output).__name__\n",
    "#         })\n",
    "\n",
    "# # Create tracer and register hooks\n",
    "# tracer = ExecutionTracer()\n",
    "# hooks = []\n",
    "# for name, module in model.named_modules():\n",
    "#     # Skip container modules\n",
    "#     if len(list(module.children())) == 0:\n",
    "#         hooks.append(module.register_forward_hook(tracer))\n",
    "\n",
    "# # Run a forward pass\n",
    "# input_ids = tokenizer(\"Hello world\", return_tensors=\"pt\").input_ids\n",
    "# with torch.no_grad():\n",
    "#     output = model(input_ids)\n",
    "\n",
    "# # Print execution order\n",
    "# print(\"LAYER EXECUTION ORDER:\")\n",
    "# print(\"=\"*100)\n",
    "# for item in tracer.execution_order:\n",
    "#     print(f\"{item['order']:3d}. {item['name']:<50} | {item['type']:<20} | {item['input_shape']} → {item['output_shape']}\")\n",
    "\n",
    "# # Clean up hooks\n",
    "# for hook in hooks:\n",
    "#     hook.remove()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "784f29b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables for detailed activation capture\n",
    "captured_activations = {}\n",
    "current_hooks = []\n",
    "hook_errors = []\n",
    "\n",
    "def clear_activations():\n",
    "    global captured_activations\n",
    "    captured_activations.clear()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "def remove_all_hooks():\n",
    "    global current_hooks\n",
    "    for hook in current_hooks:\n",
    "        try:\n",
    "            hook.remove()\n",
    "        except:\n",
    "            pass\n",
    "    current_hooks.clear()\n",
    "\n",
    "def get_activation_hook(name):\n",
    "    def hook(module, input, output):\n",
    "        global hook_errors\n",
    "        try:\n",
    "            activation = output[0] if isinstance(output, tuple) else output\n",
    "            input_tensor = input[0] if isinstance(input, tuple) and len(input) > 0 else None\n",
    "\n",
    "            captured_activations[name] = {\n",
    "                'output': activation.detach().cpu() if activation is not None else None,\n",
    "                'input': input_tensor.detach().cpu() if input_tensor is not None else None,\n",
    "                'weight': module.weight.detach().cpu() if hasattr(module, 'weight') and module.weight is not None else None,\n",
    "                'bias': module.bias.detach().cpu() if hasattr(module, 'bias') and module.bias is not None else None\n",
    "            }\n",
    "        except Exception as e:\n",
    "            error_msg = f\"Hook error in {name}: {str(e)}\"\n",
    "            hook_errors.append(error_msg)\n",
    "            captured_activations[name] = {'output': None, 'input': None, 'weight': None, 'bias': None}\n",
    "    return hook\n",
    "\n",
    "def register_llama_hooks(model):\n",
    "    global current_hooks\n",
    "    remove_all_hooks() # clear any old hooks first\n",
    "    hook_errors.clear()\n",
    "\n",
    "    total_layers = len(model.model.layers)\n",
    "\n",
    "    for i in range(total_layers):\n",
    "        layer = model.model.layers[i]\n",
    "        layer_prefix = f\"layer_{i}\"\n",
    "        components = [\n",
    "            (layer.self_attn.q_proj, f\"{layer_prefix}_attention_q\"), (layer.self_attn.k_proj, f\"{layer_prefix}_attention_k\"),\n",
    "            (layer.self_attn.v_proj, f\"{layer_prefix}_attention_v\"), (layer.self_attn.o_proj, f\"{layer_prefix}_attention_output\"),\n",
    "            (layer.mlp.gate_proj, f\"{layer_prefix}_mlp_gate\"), (layer.mlp.up_proj, f\"{layer_prefix}_mlp_up\"),\n",
    "            (layer.mlp.down_proj, f\"{layer_prefix}_mlp_down\"), (layer.input_layernorm, f\"{layer_prefix}_input_norm\"),\n",
    "            (layer.post_attention_layernorm, f\"{layer_prefix}_post_attn_norm\"),\n",
    "        ]\n",
    "        for module, name in components:\n",
    "            current_hooks.append(module.register_forward_hook(get_activation_hook(name)))\n",
    "    \n",
    "    current_hooks.append(model.model.norm.register_forward_hook(get_activation_hook(\"final_norm\")))\n",
    "    current_hooks.append(model.lm_head.register_forward_hook(get_activation_hook(\"lm_head\")))\n",
    "    print(f\"Registered {len(current_hooks)} hooks.\")\n",
    "\n",
    "def run_model_and_capture_activations(model, inputs=None, inputs_embeds=None):\n",
    "    clear_activations()\n",
    "    register_llama_hooks(model)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        if inputs is not None:\n",
    "            _ = model(**inputs)\n",
    "        elif inputs_embeds is not None:\n",
    "            _ = model(inputs_embeds=inputs_embeds)\n",
    "        else:\n",
    "            raise ValueError(\"Either inputs or inputs_embeds must be provided.\")\n",
    "            \n",
    "    remove_all_hooks()\n",
    "    \n",
    "    # return a copy of the captured activations\n",
    "    return captured_activations.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5c5a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_component_type(layer_name):\n",
    "    if 'attention' in layer_name:\n",
    "        return 'attention'\n",
    "    elif 'mlp' in layer_name:\n",
    "        return 'mlp'\n",
    "    elif 'norm' in layer_name:\n",
    "        return 'normalization'\n",
    "    elif 'lm_head' in layer_name:\n",
    "        return 'output'\n",
    "    elif 'embed' in layer_name:\n",
    "        return 'embedding'\n",
    "    else:\n",
    "        return 'other'\n",
    "    \n",
    "def calculate_layer_output(\n",
    "    layer_name: str,\n",
    "    token_input: torch.Tensor,\n",
    "    weight: torch.Tensor,\n",
    "    bias: Optional[torch.Tensor]\n",
    ") -> Tuple[Optional[torch.Tensor], str]:\n",
    "\n",
    "    if token_input is None or weight is None:\n",
    "        return None, \"Missing input or weight\"\n",
    "\n",
    "    try:\n",
    "        # Case 1: normalization layer (LayerNorm/RMSNorm)\n",
    "        if 'norm' in layer_name:\n",
    "            # PyTorch's functional LayerNorm which handles the formula:\n",
    "            # y = (x - E[x]) / sqrt(Var[x] + eps) * gamma + beta\n",
    "            calculated_output = F.layer_norm(\n",
    "                token_input,\n",
    "                normalized_shape=[token_input.shape[0]],\n",
    "                weight=weight,\n",
    "                bias=bias,\n",
    "                eps=1e-5 # standard epsilon for Llama models\n",
    "            )\n",
    "            return calculated_output, \"Success\"\n",
    "\n",
    "        # Case 2: linear projection (Attention, MLP, etc.)\n",
    "        else:\n",
    "            # y = x @ W^T + b\n",
    "            calculated_output = F.linear(token_input, weight, bias)\n",
    "\n",
    "            # Apply the SiLU activation function for specific MLP layers\n",
    "            if 'mlp_gate' in layer_name: #or 'mlp_up' in layer_name:\n",
    "                calculated_output = F.silu(calculated_output)\n",
    "\n",
    "            return calculated_output, \"Success\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return None, f\"Calculation failed: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fe7f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def analyze_calculation_vs_real_outputs(\n",
    "    original_activations: Dict[str, Dict[str, torch.Tensor]],\n",
    "    reconstructed_activations: Dict[str, Dict[str, torch.Tensor]],\n",
    "    mode:str,\n",
    "    n_rounds: int\n",
    ") -> List[Dict[str, Any]]:    \n",
    "    all_results = []\n",
    "    if mode == 'min':\n",
    "        for layer_name in original_activations.keys():\n",
    "            \n",
    "            if layer_name not in reconstructed_activations:\n",
    "                continue\n",
    "                \n",
    "            orig_data = original_activations[layer_name]\n",
    "            recon_data = reconstructed_activations[layer_name]\n",
    "            \n",
    "            if not all(k in orig_data and orig_data[k] is not None for k in ['input', 'weight', 'output']) or \\\n",
    "            not all(k in recon_data and recon_data[k] is not None for k in ['weight', 'output']):\n",
    "                continue\n",
    "\n",
    "            # Get the single input vector from the ORIGINAL data\n",
    "            token_pos = orig_data['input'].shape[1] - 1  # will need to update here dont forgetttttttt !HSK!\n",
    "            recon_token_input = recon_data['input'][0, token_pos, :]\n",
    "            orig_token_input = orig_data['input'][0, token_pos, :]\n",
    "\n",
    "            # --- 1. Analyze the Original Run ---\n",
    "            calc_orig, status_orig = calculate_layer_output(\n",
    "                layer_name, orig_token_input, orig_data['weight'], orig_data.get('bias')\n",
    "            )\n",
    "            if calc_orig is None: \n",
    "                print(f\"Skipping {layer_name} (original): {status_orig}\")\n",
    "                continue\n",
    "            \n",
    "            real_orig = orig_data['output'][0, token_pos, :]\n",
    "            error_vector_orig = calc_orig - real_orig\n",
    "\n",
    "            # --- 2. Analyze the Reconstructed Run (using ORIGINAL input) ---\n",
    "            calc_recon, status_recon = calculate_layer_output(\n",
    "                layer_name, recon_token_input, orig_data['weight'], orig_data.get('bias')\n",
    "            )\n",
    "            if calc_recon is None: continue\n",
    "            \n",
    "            real_recon = recon_data['output'][0, token_pos, :]\n",
    "            # This error shows how much the reconstructed model deviates from its\n",
    "            # own hooked output when given the original benign input.\n",
    "            error_vector_recon = calc_recon - real_recon\n",
    "            \n",
    "            # --- 3. Process results for both runs ---\n",
    "            \n",
    "            # Find min and random indices for the ORIGINAL run's error\n",
    "            min_err_idx_orig = torch.argmin(error_vector_orig.abs()).item()\n",
    "            #rand_idx_orig = torch.randint(0, len(error_vector_orig), (1,)).item() REMOVEW\n",
    "            \n",
    "            all_results.append({\n",
    "                'round': -1,\n",
    "                'layer_name': layer_name, 'run_type': 'original',\n",
    "                'error_index': min_err_idx_orig,\n",
    "                'error_real_value': real_orig[min_err_idx_orig].item(),\n",
    "                'error_calc_value': calc_orig[min_err_idx_orig].item(),\n",
    "            })\n",
    "\n",
    "            # Find min and random indices for the RECONSTRUCTED run's error\n",
    "            min_err_idx_recon = torch.argmin(error_vector_recon.abs()).item()\n",
    "            \n",
    "            all_results.append({\n",
    "                'round': -1,\n",
    "                'layer_name': layer_name, 'run_type': 'reconstructed',\n",
    "                'error_index': min_err_idx_recon,\n",
    "                'error_real_value': real_recon[min_err_idx_recon].item(),\n",
    "                'error_calc_value': calc_recon[min_err_idx_recon].item(),\n",
    "            })\n",
    "    else:\n",
    "        for round in range(n_rounds):\n",
    "            print(f\"Analysis round {round+1}/{n_rounds}...\")\n",
    "            for layer_name in original_activations.keys():\n",
    "                if layer_name not in reconstructed_activations:\n",
    "                    continue\n",
    "                    \n",
    "                orig_data = original_activations[layer_name]\n",
    "                recon_data = reconstructed_activations[layer_name]\n",
    "                \n",
    "                if not all(k in orig_data and orig_data[k] is not None for k in ['input', 'weight', 'output']) or \\\n",
    "                not all(k in recon_data and recon_data[k] is not None for k in ['weight', 'output']):\n",
    "                    continue\n",
    "\n",
    "                token_pos = orig_data['input'].shape[1] - 1\n",
    "                recon_token_input = recon_data['input'][0, token_pos, :]\n",
    "                orig_token_input = orig_data['input'][0, token_pos, :]\n",
    "                num_neurons = orig_data['output'].shape[2]\n",
    "                rand_idx = torch.randint(0, num_neurons, (1,)).item()\n",
    "                \n",
    "                # --- Handle Norm layers separately, as they need the full input context ---\n",
    "                if 'norm' in layer_name:\n",
    "                    calc_orig, _ = calculate_layer_output(layer_name, orig_token_input, orig_data['weight'], orig_data.get('bias'))\n",
    "                    calc_recon, _ = calculate_layer_output(layer_name, recon_token_input, orig_data['weight'], orig_data.get('bias'))\n",
    "\n",
    "                    calc_orig = calc_orig[rand_idx].item() if calc_orig is not None else None\n",
    "                    calc_recon = calc_recon[rand_idx].item() if calc_recon is not None else None\n",
    "                \n",
    "                else:\n",
    "                    # Slice the weight and bias for the randomly selected neuron\n",
    "                    single_row_weight_orig = orig_data['weight'][rand_idx, :].unsqueeze(0) # Shape: [1, in_features]\n",
    "                    single_row_weight_recon = recon_data['weight'][rand_idx, :].unsqueeze(0)\n",
    "                    \n",
    "                    bias_orig = orig_data.get('bias')\n",
    "                    single_value_bias_orig = bias_orig[rand_idx].unsqueeze(0) if bias_orig is not None else None # Shape: [1]\n",
    "                    \n",
    "                    bias_recon = recon_data.get('bias')\n",
    "                    single_value_bias_recon = bias_recon[rand_idx].unsqueeze(0) if bias_recon is not None else None\n",
    "\n",
    "                    # Calculate output for the single neuron by passing its sliced weights\n",
    "                    calc_orig_tensor, _ = calculate_layer_output(layer_name, orig_token_input, single_row_weight_orig, single_value_bias_orig)\n",
    "                    calc_recon_tensor, _ = calculate_layer_output(layer_name, recon_token_input, single_row_weight_orig, single_value_bias_orig)\n",
    "                    \n",
    "                    # The result is a tensor with one value, so we extract it\n",
    "                    calc_orig = calc_orig_tensor.item() if calc_orig_tensor is not None else None\n",
    "                    calc_recon = calc_recon_tensor.item() if calc_recon_tensor is not None else None\n",
    "\n",
    "                # --- Append results for the single random neuron ---\n",
    "                if calc_orig is not None:\n",
    "                    real_orig = orig_data['output'][0, token_pos, rand_idx].item()\n",
    "                    all_results.append({\n",
    "                        'round': round,\n",
    "                        'layer_name': layer_name, 'run_type': 'original',\n",
    "                        'error_index': rand_idx,\n",
    "                        'error_real_value': real_orig,\n",
    "                        'error_calc_value': calc_orig,\n",
    "                    })\n",
    "\n",
    "                if calc_recon is not None:\n",
    "                    real_recon = recon_data['output'][0, token_pos, rand_idx].item()\n",
    "                    all_results.append({\n",
    "                        'round': round,\n",
    "                        'layer_name': layer_name, 'run_type': 'reconstructed', \n",
    "                        'error_index': rand_idx,\n",
    "                        'error_real_value': real_recon,\n",
    "                        'error_calc_value': calc_recon,\n",
    "                    })\n",
    "\n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e7f45d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_analysis_results(\n",
    "    results_list: List[Dict[str, Any]],\n",
    "    input: str,\n",
    "    recon_idx: int,\n",
    "    filename: str = \"attack_calc_error_analysis.csv\"\n",
    "):\n",
    "    if not results_list:\n",
    "        return\n",
    "        \n",
    "    df = pd.DataFrame(results_list)\n",
    "    df.insert(0, 'reconstruction_idx', recon_idx)\n",
    "    df.insert(0, 'input', input)\n",
    "    \n",
    "    # Append to the file if it exists, otherwise create it\n",
    "    if os.path.exists(filename):\n",
    "        df.to_csv(filename, mode='a', header=False, index=False)\n",
    "    else:\n",
    "        df.to_csv(filename, mode='w', header=True, index=False)\n",
    "    \n",
    "    print(f\"--- Saved {len(df)} analysis rows to {filename} ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22eaf9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_attack_and_analysis_workflow(\n",
    "    model: \"LlamaForCausalLM\",\n",
    "    tokenizer: \"LlamaTokenizer\",\n",
    "    string_input: str,\n",
    "    n_reconstructions: int = 3,\n",
    "    n_test_rounds: int = 1,\n",
    "    optimization_steps: int = 1500,\n",
    "    learning_rate: float = 0.01,\n",
    "    reg_loss_factor: float = 0.001\n",
    "):\n",
    "    sample_input = tokenizer(string_input[1],return_tensors=\"pt\")\n",
    "    inputs_on_device = {k: v.to(model.device) for k, v in sample_input.items()}\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Input: '{tokenizer.decode(inputs_on_device['input_ids'][0])}'\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # --- Step 1: Get Original State ---\n",
    "    with torch.no_grad():\n",
    "        original_logits = model(**inputs_on_device).logits\n",
    "        # Get original embeddings from the embedding layer\n",
    "        original_embeddings = model.model.embed_tokens(inputs_on_device['input_ids'])\n",
    "    \n",
    "    original_activations = run_model_and_capture_activations(\n",
    "        model, inputs=inputs_on_device\n",
    "    )\n",
    "    \n",
    "    # --- Step 2: Loop Through Reconstructions (Input Reconstruction Attack) ---\n",
    "    for recon_idx in range(n_reconstructions):\n",
    "        print(f\"\\n--- [Recon {recon_idx+1}/{n_reconstructions}] Starting input reconstruction ---\")\n",
    "        \n",
    "        # Initialize random embeddings to optimize\n",
    "        seq_len = inputs_on_device['input_ids'].shape[1]\n",
    "        embed_dim = model.config.hidden_size\n",
    "        reconstructed_embeds = torch.randn(\n",
    "            1, seq_len, embed_dim,\n",
    "            device=model.device, dtype=torch.float32, requires_grad=True\n",
    "        )\n",
    "        optimizer = optim.Adam([reconstructed_embeds], lr=learning_rate)\n",
    "\n",
    "        # Hook to inject optimized embeddings at the embedding layer\n",
    "        def embedding_injection_hook(module, input, output):\n",
    "            # Replace the embedding layer's output with our optimized embeddings\n",
    "            return reconstructed_embeds.to(model.dtype)\n",
    "        \n",
    "        target_layer = model.model.embed_tokens\n",
    "        hook_handle = target_layer.register_forward_hook(embedding_injection_hook)\n",
    "\n",
    "        for step in tqdm(range(optimization_steps), desc=f\"Optimizing Recon {recon_idx+1}\", leave=False):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass with injected embeddings\n",
    "            output_logits = model(**inputs_on_device).logits\n",
    "            \n",
    "            # Loss calculation - try to match original logits (input reconstruction objective)\n",
    "            loss = F.mse_loss(output_logits.float(), original_logits.float())\n",
    "            \n",
    "            # Regularization to keep embeddings reasonable\n",
    "            reg_loss = reg_loss_factor * torch.mean(reconstructed_embeds ** 2)\n",
    "            total_loss = loss + reg_loss\n",
    "            \n",
    "            total_loss.backward()\n",
    "            \n",
    "            # Optional: only update specific token positions\n",
    "            if reconstructed_embeds.grad is not None:\n",
    "                reconstructed_embeds.grad[:, :-1, :] = 0.0 # Only update the last token\n",
    "\n",
    "            optimizer.step()\n",
    "            \n",
    "            if step % 100 == 0:\n",
    "                print(f\"Step {step}: Loss={total_loss.item():.6f}\")\n",
    "            \n",
    "            if total_loss.item() < 1e-4:\n",
    "                print(f\"Converged at step {step}\")\n",
    "                break\n",
    "\n",
    "        hook_handle.remove()\n",
    "        print(f\"Reconstruction complete. Final Loss: {total_loss.item():.6f}\")\n",
    "\n",
    "        # --- Step 3: Get Reconstructed State ---\n",
    "        print(\"Capturing reconstructed activations...\")\n",
    "        hook_handle = target_layer.register_forward_hook(embedding_injection_hook)\n",
    "        \n",
    "        final_embeds = reconstructed_embeds.detach().to(model.dtype)\n",
    "        \n",
    "        reconstructed_activations = run_model_and_capture_activations(\n",
    "            model, inputs=inputs_on_device\n",
    "        )\n",
    "        hook_handle.remove()\n",
    "\n",
    "        # --- Step 4: Run Deep Analysis ---\n",
    "        print(\"Running deep calculation analysis...\")\n",
    "        analysis_results = analyze_calculation_vs_real_outputs(\n",
    "                original_activations,\n",
    "                reconstructed_activations,\n",
    "                mode='min',\n",
    "                n_rounds=-1\n",
    "            )\n",
    "        \n",
    "        analysis_results.extend(analyze_calculation_vs_real_outputs(\n",
    "            original_activations,\n",
    "            reconstructed_activations,\n",
    "            mode='random',\n",
    "            n_rounds=n_test_rounds\n",
    "        ))\n",
    "        \n",
    "        # --- Step 5: Save Results ---\n",
    "        save_analysis_results(analysis_results, string_input[0], recon_idx)\n",
    "        \n",
    "        # Clean up memory\n",
    "        del reconstructed_activations, reconstructed_embeds, analysis_results\n",
    "        if 'clear_activations' in globals():\n",
    "            globals()['clear_activations']() # Call clear_activations if it exists\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Workflow complete.\")\n",
    "    print(f\"Results are saved in 'attack_calc_error_analysis.csv'\")\n",
    "    print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fa7cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_texts = [\n",
    "    [1,\"The capital of France is\"],\n",
    "    [2,\"The largest mammal on Earth is\"],\n",
    "    [3,\"The process of photosynthesis occurs in\"],\n",
    "    [4,\"The speed of light in a vacuum is\"],\n",
    "    [5,\"The chemical symbol for gold is\"],\n",
    "    [6,\"The human body has how many bones\"],\n",
    "    [7,\"The Great Wall of China was built to\"],\n",
    "    [8,\"Water boils at what temperature\"],\n",
    "    [9,\"The smallest unit of matter is\"],\n",
    "    [10,\"Shakespeare wrote the play\"],\n",
    "    [11,\"The currency of Japan is\"],\n",
    "    [12,\"Mount Everest is located in\"],\n",
    "    [13,\"The inventor of the telephone was\"],\n",
    "    [14,\"DNA stands for\"],\n",
    "    [15,\"The largest ocean on Earth is\"],\n",
    "    [16,\"The planet closest to the Sun is\"],\n",
    "    [17,\"Gravity was discovered by\"],\n",
    "    [18,\"The Amazon rainforest is primarily located in\"],\n",
    "    [19,\"The freezing point of water is\"],\n",
    "    [20,\"The most abundant gas in Earth's atmosphere is\"],\n",
    "    [21,\"The Mona Lisa was painted by\"],\n",
    "    [22,\"The longest river in the world is\"],\n",
    "    [23,\"Photosynthesis converts carbon dioxide and water into\"],\n",
    "    [24,\"The study of earthquakes is called\"],\n",
    "    [25,\"The first person to walk on the moon was\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670b639c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>>> Starting Analysis for Prompt 1 (Mode: min) <<<<\n",
      "\n",
      "============================================================\n",
      "Input: '<s> The capital of France is'\n",
      "============================================================\n",
      "Registered 290 hooks.\n",
      "--- Logit Swap Attack ---\n",
      "Original top prediction: 'Paris' (ID: 3681)\n",
      "Target swap token:     'textt' (ID: 16196)\n",
      "New top prediction after swap: 'a'\n",
      "\n",
      "\n",
      "--- [Recon 1/2] Starting reconstruction ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing Recon 1:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# Loop through each prompt and run the full workflow\n",
    "for i, prompt in enumerate(sample_texts):\n",
    "    \n",
    "    # --- Run input reconstruction attack analysis ---\n",
    "    print(f\"\\n>>>> Starting Analysis for Prompt {i+1}<<<<\")\n",
    "    run_attack_and_analysis_workflow(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        string_input=prompt,\n",
    "        n_reconstructions=30,\n",
    "        n_test_rounds=5000,\n",
    "        optimization_steps=5000,\n",
    "        learning_rate=0.01,\n",
    "        reg_loss_factor=0.001\n",
    "    )\n",
    "\n",
    "print(\"\\n\\n<<<< ALL TESTS COMPLETE >>>>\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
