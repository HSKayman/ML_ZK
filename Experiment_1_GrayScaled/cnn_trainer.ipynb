{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#!pip install torch \n",
    "#!pip install torchsummary\n",
    "#!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datetime import datetime\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(CNN, self).__init__()\n",
    "        # Input: 1x224x224\n",
    "        self.features = nn.Sequential(\n",
    "            # Conv1: (224+4-5)/1 + 1 = 224\n",
    "            nn.Conv2d(1, 6, kernel_size=5, stride=1, padding=2),  # Output: 6x224x224\n",
    "            nn.ReLU(inplace=True),\n",
    "            # MaxPool1: (224-2)/2 + 1 = 112\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # Output: 6x112x112\n",
    "            \n",
    "            # Conv2: (112+4-5)/1 + 1 = 112 \n",
    "            nn.Conv2d(6, 16, kernel_size=5, stride=1, padding=2),  # Output: 16x112x112\n",
    "            nn.ReLU(inplace=True),\n",
    "            # MaxPool2: (112-2)/2 + 1 = 56  \n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # Output: 16x56x56\n",
    "\n",
    "            # Conv3: (56+2-3)/2 + 1 = 28\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1), # Output: 32x28x28\n",
    "            nn.ReLU(inplace=True),\n",
    "            # MaxPool3: (28-2)/2 + 1 = 14\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), # Output: 32x14x14\n",
    "            \n",
    "            # Conv4: (14+2-3)/2 + 1 = 7\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1), # Output: 64x7x7\n",
    "            nn.ReLU(inplace=True),\n",
    "            # MaxPool4: (7-2)/2 + 1 = 3\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), # Output: 64x3x3\n",
    "        )\n",
    "        \n",
    "        # Calculate flattened size: 64 * 3 * 3 = 576\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(64 * 3 * 3, 256),  # 576 -> 256\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256, 64), # 256 -> 64\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(64, num_classes), # 64 -> 2\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convolutional layers\n",
    "        x = self.features(x)\n",
    "\n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU Device: NVIDIA GeForce RTX 3050 Laptop GPU\n",
      "Memory Usage:\n",
      "Allocated: 0.00MB\n",
      "Cached: 0.00MB\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    print(f'GPU Device: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'Memory Usage:')\n",
    "    print(f'Allocated: {torch.cuda.memory_allocated(0)/1024**2:.2f}MB')\n",
    "    print(f'Cached: {torch.cuda.memory_reserved(0)/1024**2:.2f}MB')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 6, 224, 224]             156\n",
      "              ReLU-2          [-1, 6, 224, 224]               0\n",
      "         MaxPool2d-3          [-1, 6, 112, 112]               0\n",
      "            Conv2d-4         [-1, 16, 112, 112]           2,416\n",
      "              ReLU-5         [-1, 16, 112, 112]               0\n",
      "         MaxPool2d-6           [-1, 16, 56, 56]               0\n",
      "            Conv2d-7           [-1, 32, 28, 28]           4,640\n",
      "              ReLU-8           [-1, 32, 28, 28]               0\n",
      "         MaxPool2d-9           [-1, 32, 14, 14]               0\n",
      "           Conv2d-10             [-1, 64, 7, 7]          18,496\n",
      "             ReLU-11             [-1, 64, 7, 7]               0\n",
      "        MaxPool2d-12             [-1, 64, 3, 3]               0\n",
      "          Dropout-13                  [-1, 576]               0\n",
      "           Linear-14                  [-1, 256]         147,712\n",
      "             ReLU-15                  [-1, 256]               0\n",
      "          Dropout-16                  [-1, 256]               0\n",
      "           Linear-17                   [-1, 64]          16,448\n",
      "             ReLU-18                   [-1, 64]               0\n",
      "           Linear-19                    [-1, 2]             130\n",
      "================================================================\n",
      "Total params: 189,998\n",
      "Trainable params: 189,998\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.19\n",
      "Forward/backward pass size (MB): 9.11\n",
      "Params size (MB): 0.72\n",
      "Estimated Total Size (MB): 10.02\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model, (1, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# Convert input picture to tensor\n",
    "matrix_converter = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),  \n",
    "    transforms.Resize((224, 224)),                 \n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_number = 1\n",
    "# Set data set directory\n",
    "data_dir = f'data_for_model_{model_number}/train/'\n",
    "test_dir = f'data_for_model_{model_number}/test/'\n",
    "\n",
    "# Load data set\n",
    "dataset = datasets.ImageFolder(data_dir,transform=matrix_converter)\n",
    "test_dataset = datasets.ImageFolder(test_dir,transform=matrix_converter)\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, shuffle=False, num_workers=4, pin_memory=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noof classes: 2\n",
      "Classes: ['cat', 'dog']\n",
      "Total samples: 3000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Noof classes: {len(dataset.classes)}\")\n",
    "print(f\"Classes: {dataset.classes}\")\n",
    "print(f\"Total samples: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/40], Batch [10/94], Loss: 0.6896\n",
      "Epoch [1/40], Batch [20/94], Loss: 0.6849\n",
      "Epoch [1/40], Batch [30/94], Loss: 0.6980\n",
      "Epoch [1/40], Batch [40/94], Loss: 0.6820\n",
      "Epoch [1/40], Batch [50/94], Loss: 0.6948\n",
      "Epoch [1/40], Batch [60/94], Loss: 0.6932\n",
      "Epoch [1/40], Batch [70/94], Loss: 0.6964\n",
      "Epoch [1/40], Batch [80/94], Loss: 0.6934\n",
      "Epoch [1/40], Batch [90/94], Loss: 0.6883\n",
      "------------------------------\n",
      "Epoch [1/40]\n",
      "Loss: 0.0217\n",
      "Accuracy: 50.60%\n",
      "------------------------------\n",
      "Epoch [2/40], Batch [10/94], Loss: 0.6858\n",
      "Epoch [2/40], Batch [20/94], Loss: 0.6912\n",
      "Epoch [2/40], Batch [30/94], Loss: 0.6977\n",
      "Epoch [2/40], Batch [40/94], Loss: 0.6942\n",
      "Epoch [2/40], Batch [50/94], Loss: 0.6897\n",
      "Epoch [2/40], Batch [60/94], Loss: 0.6842\n",
      "Epoch [2/40], Batch [70/94], Loss: 0.6867\n",
      "Epoch [2/40], Batch [80/94], Loss: 0.6932\n",
      "Epoch [2/40], Batch [90/94], Loss: 0.6922\n",
      "------------------------------\n",
      "Epoch [2/40]\n",
      "Loss: 0.0217\n",
      "Accuracy: 49.20%\n",
      "------------------------------\n",
      "Epoch [3/40], Batch [10/94], Loss: 0.6931\n",
      "Epoch [3/40], Batch [20/94], Loss: 0.6923\n",
      "Epoch [3/40], Batch [30/94], Loss: 0.6923\n",
      "Epoch [3/40], Batch [40/94], Loss: 0.6874\n",
      "Epoch [3/40], Batch [50/94], Loss: 0.6764\n",
      "Epoch [3/40], Batch [60/94], Loss: 0.7076\n",
      "Epoch [3/40], Batch [70/94], Loss: 0.7014\n",
      "Epoch [3/40], Batch [80/94], Loss: 0.6912\n",
      "Epoch [3/40], Batch [90/94], Loss: 0.6722\n",
      "------------------------------\n",
      "Epoch [3/40]\n",
      "Loss: 0.0217\n",
      "Accuracy: 52.17%\n",
      "------------------------------\n",
      "Epoch [4/40], Batch [10/94], Loss: 0.6743\n",
      "Epoch [4/40], Batch [20/94], Loss: 0.6958\n",
      "Epoch [4/40], Batch [30/94], Loss: 0.7087\n",
      "Epoch [4/40], Batch [40/94], Loss: 0.6926\n",
      "Epoch [4/40], Batch [50/94], Loss: 0.6788\n",
      "Epoch [4/40], Batch [60/94], Loss: 0.6961\n",
      "Epoch [4/40], Batch [70/94], Loss: 0.6738\n",
      "Epoch [4/40], Batch [80/94], Loss: 0.6756\n",
      "Epoch [4/40], Batch [90/94], Loss: 0.6911\n",
      "------------------------------\n",
      "Epoch [4/40]\n",
      "Loss: 0.0215\n",
      "Accuracy: 55.53%\n",
      "------------------------------\n",
      "Epoch [5/40], Batch [10/94], Loss: 0.7440\n",
      "Epoch [5/40], Batch [20/94], Loss: 0.6720\n",
      "Epoch [5/40], Batch [30/94], Loss: 0.7219\n",
      "Epoch [5/40], Batch [40/94], Loss: 0.7035\n",
      "Epoch [5/40], Batch [50/94], Loss: 0.6591\n",
      "Epoch [5/40], Batch [60/94], Loss: 0.6956\n",
      "Epoch [5/40], Batch [70/94], Loss: 0.6388\n",
      "Epoch [5/40], Batch [80/94], Loss: 0.6782\n",
      "Epoch [5/40], Batch [90/94], Loss: 0.6668\n",
      "------------------------------\n",
      "Epoch [5/40]\n",
      "Loss: 0.0214\n",
      "Accuracy: 56.20%\n",
      "------------------------------\n",
      "Epoch [6/40], Batch [10/94], Loss: 0.6482\n",
      "Epoch [6/40], Batch [20/94], Loss: 0.6596\n",
      "Epoch [6/40], Batch [30/94], Loss: 0.7454\n",
      "Epoch [6/40], Batch [40/94], Loss: 0.6880\n",
      "Epoch [6/40], Batch [50/94], Loss: 0.6896\n",
      "Epoch [6/40], Batch [60/94], Loss: 0.6678\n",
      "Epoch [6/40], Batch [70/94], Loss: 0.6798\n",
      "Epoch [6/40], Batch [80/94], Loss: 0.6841\n",
      "Epoch [6/40], Batch [90/94], Loss: 0.6621\n",
      "------------------------------\n",
      "Epoch [6/40]\n",
      "Loss: 0.0211\n",
      "Accuracy: 58.87%\n",
      "------------------------------\n",
      "Epoch [7/40], Batch [10/94], Loss: 0.7530\n",
      "Epoch [7/40], Batch [20/94], Loss: 0.6725\n",
      "Epoch [7/40], Batch [30/94], Loss: 0.7213\n",
      "Epoch [7/40], Batch [40/94], Loss: 0.5923\n",
      "Epoch [7/40], Batch [50/94], Loss: 0.5966\n",
      "Epoch [7/40], Batch [60/94], Loss: 0.7060\n",
      "Epoch [7/40], Batch [70/94], Loss: 0.6170\n",
      "Epoch [7/40], Batch [80/94], Loss: 0.6913\n",
      "Epoch [7/40], Batch [90/94], Loss: 0.6292\n",
      "------------------------------\n",
      "Epoch [7/40]\n",
      "Loss: 0.0208\n",
      "Accuracy: 60.43%\n",
      "------------------------------\n",
      "Epoch [8/40], Batch [10/94], Loss: 0.6090\n",
      "Epoch [8/40], Batch [20/94], Loss: 0.6875\n",
      "Epoch [8/40], Batch [30/94], Loss: 0.6857\n",
      "Epoch [8/40], Batch [40/94], Loss: 0.6374\n",
      "Epoch [8/40], Batch [50/94], Loss: 0.6443\n",
      "Epoch [8/40], Batch [60/94], Loss: 0.6486\n",
      "Epoch [8/40], Batch [70/94], Loss: 0.6057\n",
      "Epoch [8/40], Batch [80/94], Loss: 0.6918\n",
      "Epoch [8/40], Batch [90/94], Loss: 0.7217\n",
      "------------------------------\n",
      "Epoch [8/40]\n",
      "Loss: 0.0207\n",
      "Accuracy: 61.57%\n",
      "------------------------------\n",
      "Epoch [9/40], Batch [10/94], Loss: 0.6849\n",
      "Epoch [9/40], Batch [20/94], Loss: 0.6267\n",
      "Epoch [9/40], Batch [30/94], Loss: 0.5861\n",
      "Epoch [9/40], Batch [40/94], Loss: 0.7163\n",
      "Epoch [9/40], Batch [50/94], Loss: 0.6378\n",
      "Epoch [9/40], Batch [60/94], Loss: 0.6632\n",
      "Epoch [9/40], Batch [70/94], Loss: 0.6345\n",
      "Epoch [9/40], Batch [80/94], Loss: 0.6635\n",
      "Epoch [9/40], Batch [90/94], Loss: 0.7189\n",
      "------------------------------\n",
      "Epoch [9/40]\n",
      "Loss: 0.0206\n",
      "Accuracy: 61.07%\n",
      "------------------------------\n",
      "Epoch [10/40], Batch [10/94], Loss: 0.5969\n",
      "Epoch [10/40], Batch [20/94], Loss: 0.6747\n",
      "Epoch [10/40], Batch [30/94], Loss: 0.6345\n",
      "Epoch [10/40], Batch [40/94], Loss: 0.5895\n",
      "Epoch [10/40], Batch [50/94], Loss: 0.6306\n",
      "Epoch [10/40], Batch [60/94], Loss: 0.7114\n",
      "Epoch [10/40], Batch [70/94], Loss: 0.6628\n",
      "Epoch [10/40], Batch [80/94], Loss: 0.6826\n",
      "Epoch [10/40], Batch [90/94], Loss: 0.5917\n",
      "------------------------------\n",
      "Epoch [10/40]\n",
      "Loss: 0.0201\n",
      "Accuracy: 64.10%\n",
      "------------------------------\n",
      "Epoch [11/40], Batch [10/94], Loss: 0.6441\n",
      "Epoch [11/40], Batch [20/94], Loss: 0.6132\n",
      "Epoch [11/40], Batch [30/94], Loss: 0.5970\n",
      "Epoch [11/40], Batch [40/94], Loss: 0.6579\n",
      "Epoch [11/40], Batch [50/94], Loss: 0.5785\n",
      "Epoch [11/40], Batch [60/94], Loss: 0.6813\n",
      "Epoch [11/40], Batch [70/94], Loss: 0.7521\n",
      "Epoch [11/40], Batch [80/94], Loss: 0.6285\n",
      "Epoch [11/40], Batch [90/94], Loss: 0.5709\n",
      "------------------------------\n",
      "Epoch [11/40]\n",
      "Loss: 0.0200\n",
      "Accuracy: 64.63%\n",
      "------------------------------\n",
      "Epoch [12/40], Batch [10/94], Loss: 0.5882\n",
      "Epoch [12/40], Batch [20/94], Loss: 0.5119\n",
      "Epoch [12/40], Batch [30/94], Loss: 0.6786\n",
      "Epoch [12/40], Batch [40/94], Loss: 0.6457\n",
      "Epoch [12/40], Batch [50/94], Loss: 0.5983\n",
      "Epoch [12/40], Batch [60/94], Loss: 0.6489\n",
      "Epoch [12/40], Batch [70/94], Loss: 0.7000\n",
      "Epoch [12/40], Batch [80/94], Loss: 0.5561\n",
      "Epoch [12/40], Batch [90/94], Loss: 0.6615\n",
      "------------------------------\n",
      "Epoch [12/40]\n",
      "Loss: 0.0195\n",
      "Accuracy: 64.77%\n",
      "------------------------------\n",
      "Epoch [13/40], Batch [10/94], Loss: 0.6431\n",
      "Epoch [13/40], Batch [20/94], Loss: 0.6228\n",
      "Epoch [13/40], Batch [30/94], Loss: 0.5985\n",
      "Epoch [13/40], Batch [40/94], Loss: 0.7278\n",
      "Epoch [13/40], Batch [50/94], Loss: 0.6146\n",
      "Epoch [13/40], Batch [60/94], Loss: 0.5910\n",
      "Epoch [13/40], Batch [70/94], Loss: 0.6929\n",
      "Epoch [13/40], Batch [80/94], Loss: 0.6013\n",
      "Epoch [13/40], Batch [90/94], Loss: 0.5698\n",
      "------------------------------\n",
      "Epoch [13/40]\n",
      "Loss: 0.0194\n",
      "Accuracy: 65.17%\n",
      "------------------------------\n",
      "Epoch [14/40], Batch [10/94], Loss: 0.5812\n",
      "Epoch [14/40], Batch [20/94], Loss: 0.6118\n",
      "Epoch [14/40], Batch [30/94], Loss: 0.6436\n",
      "Epoch [14/40], Batch [40/94], Loss: 0.5666\n",
      "Epoch [14/40], Batch [50/94], Loss: 0.5499\n",
      "Epoch [14/40], Batch [60/94], Loss: 0.6400\n",
      "Epoch [14/40], Batch [70/94], Loss: 0.5222\n",
      "Epoch [14/40], Batch [80/94], Loss: 0.5946\n",
      "Epoch [14/40], Batch [90/94], Loss: 0.5686\n",
      "------------------------------\n",
      "Epoch [14/40]\n",
      "Loss: 0.0195\n",
      "Accuracy: 65.10%\n",
      "------------------------------\n",
      "Epoch [15/40], Batch [10/94], Loss: 0.6999\n",
      "Epoch [15/40], Batch [20/94], Loss: 0.7047\n",
      "Epoch [15/40], Batch [30/94], Loss: 0.5370\n",
      "Epoch [15/40], Batch [40/94], Loss: 0.7179\n",
      "Epoch [15/40], Batch [50/94], Loss: 0.5527\n",
      "Epoch [15/40], Batch [60/94], Loss: 0.6437\n",
      "Epoch [15/40], Batch [70/94], Loss: 0.6098\n",
      "Epoch [15/40], Batch [80/94], Loss: 0.6119\n",
      "Epoch [15/40], Batch [90/94], Loss: 0.6950\n",
      "------------------------------\n",
      "Epoch [15/40]\n",
      "Loss: 0.0190\n",
      "Accuracy: 67.17%\n",
      "------------------------------\n",
      "Epoch [16/40], Batch [10/94], Loss: 0.5350\n",
      "Epoch [16/40], Batch [20/94], Loss: 0.6569\n",
      "Epoch [16/40], Batch [30/94], Loss: 0.6312\n",
      "Epoch [16/40], Batch [40/94], Loss: 0.6144\n",
      "Epoch [16/40], Batch [50/94], Loss: 0.5101\n",
      "Epoch [16/40], Batch [60/94], Loss: 0.5878\n",
      "Epoch [16/40], Batch [70/94], Loss: 0.8302\n",
      "Epoch [16/40], Batch [80/94], Loss: 0.6829\n",
      "Epoch [16/40], Batch [90/94], Loss: 0.6174\n",
      "------------------------------\n",
      "Epoch [16/40]\n",
      "Loss: 0.0191\n",
      "Accuracy: 66.80%\n",
      "------------------------------\n",
      "Epoch [17/40], Batch [10/94], Loss: 0.5895\n",
      "Epoch [17/40], Batch [20/94], Loss: 0.6330\n",
      "Epoch [17/40], Batch [30/94], Loss: 0.6125\n",
      "Epoch [17/40], Batch [40/94], Loss: 0.7376\n",
      "Epoch [17/40], Batch [50/94], Loss: 0.6539\n",
      "Epoch [17/40], Batch [60/94], Loss: 0.6217\n",
      "Epoch [17/40], Batch [70/94], Loss: 0.5826\n",
      "Epoch [17/40], Batch [80/94], Loss: 0.5955\n",
      "Epoch [17/40], Batch [90/94], Loss: 0.5415\n",
      "------------------------------\n",
      "Epoch [17/40]\n",
      "Loss: 0.0188\n",
      "Accuracy: 67.70%\n",
      "------------------------------\n",
      "Epoch [18/40], Batch [10/94], Loss: 0.5384\n",
      "Epoch [18/40], Batch [20/94], Loss: 0.5289\n",
      "Epoch [18/40], Batch [30/94], Loss: 0.6070\n",
      "Epoch [18/40], Batch [40/94], Loss: 0.6790\n",
      "Epoch [18/40], Batch [50/94], Loss: 0.5702\n",
      "Epoch [18/40], Batch [60/94], Loss: 0.6080\n",
      "Epoch [18/40], Batch [70/94], Loss: 0.6423\n",
      "Epoch [18/40], Batch [80/94], Loss: 0.6428\n",
      "Epoch [18/40], Batch [90/94], Loss: 0.7082\n",
      "------------------------------\n",
      "Epoch [18/40]\n",
      "Loss: 0.0187\n",
      "Accuracy: 67.03%\n",
      "------------------------------\n",
      "Epoch [19/40], Batch [10/94], Loss: 0.6377\n",
      "Epoch [19/40], Batch [20/94], Loss: 0.6094\n",
      "Epoch [19/40], Batch [30/94], Loss: 0.5114\n",
      "Epoch [19/40], Batch [40/94], Loss: 0.4944\n",
      "Epoch [19/40], Batch [50/94], Loss: 0.5158\n",
      "Epoch [19/40], Batch [60/94], Loss: 0.5537\n",
      "Epoch [19/40], Batch [70/94], Loss: 0.6137\n",
      "Epoch [19/40], Batch [80/94], Loss: 0.7338\n",
      "Epoch [19/40], Batch [90/94], Loss: 0.5023\n",
      "------------------------------\n",
      "Epoch [19/40]\n",
      "Loss: 0.0188\n",
      "Accuracy: 67.73%\n",
      "------------------------------\n",
      "Epoch [20/40], Batch [10/94], Loss: 0.5832\n",
      "Epoch [20/40], Batch [20/94], Loss: 0.5759\n",
      "Epoch [20/40], Batch [30/94], Loss: 0.5058\n",
      "Epoch [20/40], Batch [40/94], Loss: 0.5293\n",
      "Epoch [20/40], Batch [50/94], Loss: 0.6024\n",
      "Epoch [20/40], Batch [60/94], Loss: 0.5137\n",
      "Epoch [20/40], Batch [70/94], Loss: 0.5470\n",
      "Epoch [20/40], Batch [80/94], Loss: 0.6793\n",
      "Epoch [20/40], Batch [90/94], Loss: 0.5473\n",
      "------------------------------\n",
      "Epoch [20/40]\n",
      "Loss: 0.0186\n",
      "Accuracy: 68.77%\n",
      "------------------------------\n",
      "Epoch [21/40], Batch [10/94], Loss: 0.6617\n",
      "Epoch [21/40], Batch [20/94], Loss: 0.6048\n",
      "Epoch [21/40], Batch [30/94], Loss: 0.4842\n",
      "Epoch [21/40], Batch [40/94], Loss: 0.5897\n",
      "Epoch [21/40], Batch [50/94], Loss: 0.6166\n",
      "Epoch [21/40], Batch [60/94], Loss: 0.4599\n",
      "Epoch [21/40], Batch [70/94], Loss: 0.5809\n",
      "Epoch [21/40], Batch [80/94], Loss: 0.7252\n",
      "Epoch [21/40], Batch [90/94], Loss: 0.4797\n",
      "------------------------------\n",
      "Epoch [21/40]\n",
      "Loss: 0.0186\n",
      "Accuracy: 68.00%\n",
      "------------------------------\n",
      "Epoch [22/40], Batch [10/94], Loss: 0.7378\n",
      "Epoch [22/40], Batch [20/94], Loss: 0.6055\n",
      "Epoch [22/40], Batch [30/94], Loss: 0.9083\n",
      "Epoch [22/40], Batch [40/94], Loss: 0.6637\n",
      "Epoch [22/40], Batch [50/94], Loss: 0.6390\n",
      "Epoch [22/40], Batch [60/94], Loss: 0.6155\n",
      "Epoch [22/40], Batch [70/94], Loss: 0.5781\n",
      "Epoch [22/40], Batch [80/94], Loss: 0.5686\n",
      "Epoch [22/40], Batch [90/94], Loss: 0.5150\n",
      "------------------------------\n",
      "Epoch [22/40]\n",
      "Loss: 0.0182\n",
      "Accuracy: 70.17%\n",
      "------------------------------\n",
      "Epoch [23/40], Batch [10/94], Loss: 0.6423\n",
      "Epoch [23/40], Batch [20/94], Loss: 0.5110\n",
      "Epoch [23/40], Batch [30/94], Loss: 0.5273\n",
      "Epoch [23/40], Batch [40/94], Loss: 0.6598\n",
      "Epoch [23/40], Batch [50/94], Loss: 0.7203\n",
      "Epoch [23/40], Batch [60/94], Loss: 0.5237\n",
      "Epoch [23/40], Batch [70/94], Loss: 0.5150\n",
      "Epoch [23/40], Batch [80/94], Loss: 0.5126\n",
      "Epoch [23/40], Batch [90/94], Loss: 0.5576\n",
      "------------------------------\n",
      "Epoch [23/40]\n",
      "Loss: 0.0182\n",
      "Accuracy: 69.43%\n",
      "------------------------------\n",
      "Epoch [24/40], Batch [10/94], Loss: 0.5687\n",
      "Epoch [24/40], Batch [20/94], Loss: 0.4593\n",
      "Epoch [24/40], Batch [30/94], Loss: 0.5735\n",
      "Epoch [24/40], Batch [40/94], Loss: 0.6394\n",
      "Epoch [24/40], Batch [50/94], Loss: 0.6500\n",
      "Epoch [24/40], Batch [60/94], Loss: 0.4256\n",
      "Epoch [24/40], Batch [70/94], Loss: 0.5523\n",
      "Epoch [24/40], Batch [80/94], Loss: 0.4712\n",
      "Epoch [24/40], Batch [90/94], Loss: 0.5752\n",
      "------------------------------\n",
      "Epoch [24/40]\n",
      "Loss: 0.0181\n",
      "Accuracy: 69.83%\n",
      "------------------------------\n",
      "Epoch [25/40], Batch [10/94], Loss: 0.4732\n",
      "Epoch [25/40], Batch [20/94], Loss: 0.6022\n",
      "Epoch [25/40], Batch [30/94], Loss: 0.6504\n",
      "Epoch [25/40], Batch [40/94], Loss: 0.8818\n",
      "Epoch [25/40], Batch [50/94], Loss: 0.5949\n",
      "Epoch [25/40], Batch [60/94], Loss: 0.4968\n",
      "Epoch [25/40], Batch [70/94], Loss: 0.4663\n",
      "Epoch [25/40], Batch [80/94], Loss: 0.4052\n",
      "Epoch [25/40], Batch [90/94], Loss: 0.5615\n",
      "------------------------------\n",
      "Epoch [25/40]\n",
      "Loss: 0.0178\n",
      "Accuracy: 71.00%\n",
      "------------------------------\n",
      "Epoch [26/40], Batch [10/94], Loss: 0.8560\n",
      "Epoch [26/40], Batch [20/94], Loss: 0.7338\n",
      "Epoch [26/40], Batch [30/94], Loss: 0.6123\n",
      "Epoch [26/40], Batch [40/94], Loss: 0.5884\n",
      "Epoch [26/40], Batch [50/94], Loss: 0.5745\n",
      "Epoch [26/40], Batch [60/94], Loss: 0.6293\n",
      "Epoch [26/40], Batch [70/94], Loss: 0.7017\n",
      "Epoch [26/40], Batch [80/94], Loss: 0.5881\n",
      "Epoch [26/40], Batch [90/94], Loss: 0.4778\n",
      "------------------------------\n",
      "Epoch [26/40]\n",
      "Loss: 0.0183\n",
      "Accuracy: 69.47%\n",
      "------------------------------\n",
      "Epoch [27/40], Batch [10/94], Loss: 0.4302\n",
      "Epoch [27/40], Batch [20/94], Loss: 0.5412\n",
      "Epoch [27/40], Batch [30/94], Loss: 0.6041\n",
      "Epoch [27/40], Batch [40/94], Loss: 0.5660\n",
      "Epoch [27/40], Batch [50/94], Loss: 0.4955\n",
      "Epoch [27/40], Batch [60/94], Loss: 0.6210\n",
      "Epoch [27/40], Batch [70/94], Loss: 0.5291\n",
      "Epoch [27/40], Batch [80/94], Loss: 0.5640\n",
      "Epoch [27/40], Batch [90/94], Loss: 0.4457\n",
      "------------------------------\n",
      "Epoch [27/40]\n",
      "Loss: 0.0178\n",
      "Accuracy: 70.17%\n",
      "------------------------------\n",
      "Epoch [28/40], Batch [10/94], Loss: 0.5813\n",
      "Epoch [28/40], Batch [20/94], Loss: 0.5295\n",
      "Epoch [28/40], Batch [30/94], Loss: 0.5898\n",
      "Epoch [28/40], Batch [40/94], Loss: 0.5908\n",
      "Epoch [28/40], Batch [50/94], Loss: 0.7140\n",
      "Epoch [28/40], Batch [60/94], Loss: 0.7156\n",
      "Epoch [28/40], Batch [70/94], Loss: 0.4982\n",
      "Epoch [28/40], Batch [80/94], Loss: 0.6242\n",
      "Epoch [28/40], Batch [90/94], Loss: 0.4451\n",
      "------------------------------\n",
      "Epoch [28/40]\n",
      "Loss: 0.0176\n",
      "Accuracy: 70.97%\n",
      "------------------------------\n",
      "Epoch [29/40], Batch [10/94], Loss: 0.5491\n",
      "Epoch [29/40], Batch [20/94], Loss: 0.6122\n",
      "Epoch [29/40], Batch [30/94], Loss: 0.6509\n",
      "Epoch [29/40], Batch [40/94], Loss: 0.6446\n",
      "Epoch [29/40], Batch [50/94], Loss: 0.5697\n",
      "Epoch [29/40], Batch [60/94], Loss: 0.5673\n",
      "Epoch [29/40], Batch [70/94], Loss: 0.6271\n",
      "Epoch [29/40], Batch [80/94], Loss: 0.6603\n",
      "Epoch [29/40], Batch [90/94], Loss: 0.4339\n",
      "------------------------------\n",
      "Epoch [29/40]\n",
      "Loss: 0.0175\n",
      "Accuracy: 71.60%\n",
      "------------------------------\n",
      "Epoch [30/40], Batch [10/94], Loss: 0.5976\n",
      "Epoch [30/40], Batch [20/94], Loss: 0.5050\n",
      "Epoch [30/40], Batch [30/94], Loss: 0.5724\n",
      "Epoch [30/40], Batch [40/94], Loss: 0.5830\n",
      "Epoch [30/40], Batch [50/94], Loss: 0.4870\n",
      "Epoch [30/40], Batch [60/94], Loss: 0.3685\n",
      "Epoch [30/40], Batch [70/94], Loss: 0.5065\n",
      "Epoch [30/40], Batch [80/94], Loss: 0.4954\n",
      "Epoch [30/40], Batch [90/94], Loss: 0.4654\n",
      "------------------------------\n",
      "Epoch [30/40]\n",
      "Loss: 0.0173\n",
      "Accuracy: 71.53%\n",
      "------------------------------\n",
      "Epoch [31/40], Batch [10/94], Loss: 0.6428\n",
      "Epoch [31/40], Batch [20/94], Loss: 0.4183\n",
      "Epoch [31/40], Batch [30/94], Loss: 0.5213\n",
      "Epoch [31/40], Batch [40/94], Loss: 0.6783\n",
      "Epoch [31/40], Batch [50/94], Loss: 0.5462\n",
      "Epoch [31/40], Batch [60/94], Loss: 0.5164\n",
      "Epoch [31/40], Batch [70/94], Loss: 0.5459\n",
      "Epoch [31/40], Batch [80/94], Loss: 0.5962\n",
      "Epoch [31/40], Batch [90/94], Loss: 0.5265\n",
      "------------------------------\n",
      "Epoch [31/40]\n",
      "Loss: 0.0174\n",
      "Accuracy: 71.37%\n",
      "------------------------------\n",
      "Epoch [32/40], Batch [10/94], Loss: 0.4791\n",
      "Epoch [32/40], Batch [20/94], Loss: 0.4844\n",
      "Epoch [32/40], Batch [30/94], Loss: 0.6665\n",
      "Epoch [32/40], Batch [40/94], Loss: 0.5681\n",
      "Epoch [32/40], Batch [50/94], Loss: 0.7644\n",
      "Epoch [32/40], Batch [60/94], Loss: 0.6101\n",
      "Epoch [32/40], Batch [70/94], Loss: 0.5815\n",
      "Epoch [32/40], Batch [80/94], Loss: 0.4956\n",
      "Epoch [32/40], Batch [90/94], Loss: 0.5097\n",
      "------------------------------\n",
      "Epoch [32/40]\n",
      "Loss: 0.0173\n",
      "Accuracy: 71.30%\n",
      "------------------------------\n",
      "Epoch [33/40], Batch [10/94], Loss: 0.5507\n",
      "Epoch [33/40], Batch [20/94], Loss: 0.5538\n",
      "Epoch [33/40], Batch [30/94], Loss: 0.6127\n",
      "Epoch [33/40], Batch [40/94], Loss: 0.6955\n",
      "Epoch [33/40], Batch [50/94], Loss: 0.5846\n",
      "Epoch [33/40], Batch [60/94], Loss: 0.5943\n",
      "Epoch [33/40], Batch [70/94], Loss: 0.5775\n",
      "Epoch [33/40], Batch [80/94], Loss: 0.5349\n",
      "Epoch [33/40], Batch [90/94], Loss: 0.7687\n",
      "------------------------------\n",
      "Epoch [33/40]\n",
      "Loss: 0.0172\n",
      "Accuracy: 73.13%\n",
      "------------------------------\n",
      "Epoch [34/40], Batch [10/94], Loss: 0.5942\n",
      "Epoch [34/40], Batch [20/94], Loss: 0.5390\n",
      "Epoch [34/40], Batch [30/94], Loss: 0.6563\n",
      "Epoch [34/40], Batch [40/94], Loss: 0.4870\n",
      "Epoch [34/40], Batch [50/94], Loss: 0.4718\n",
      "Epoch [34/40], Batch [60/94], Loss: 0.6487\n",
      "Epoch [34/40], Batch [70/94], Loss: 0.5907\n",
      "Epoch [34/40], Batch [80/94], Loss: 0.5856\n",
      "Epoch [34/40], Batch [90/94], Loss: 0.5219\n",
      "------------------------------\n",
      "Epoch [34/40]\n",
      "Loss: 0.0173\n",
      "Accuracy: 71.23%\n",
      "------------------------------\n",
      "Epoch [35/40], Batch [10/94], Loss: 0.5107\n",
      "Epoch [35/40], Batch [20/94], Loss: 0.5427\n",
      "Epoch [35/40], Batch [30/94], Loss: 0.5378\n",
      "Epoch [35/40], Batch [40/94], Loss: 0.4714\n",
      "Epoch [35/40], Batch [50/94], Loss: 0.7168\n",
      "Epoch [35/40], Batch [60/94], Loss: 0.4840\n",
      "Epoch [35/40], Batch [70/94], Loss: 0.5358\n",
      "Epoch [35/40], Batch [80/94], Loss: 0.5158\n",
      "Epoch [35/40], Batch [90/94], Loss: 0.5620\n",
      "------------------------------\n",
      "Epoch [35/40]\n",
      "Loss: 0.0175\n",
      "Accuracy: 71.77%\n",
      "------------------------------\n",
      "Epoch [36/40], Batch [10/94], Loss: 0.5290\n",
      "Epoch [36/40], Batch [20/94], Loss: 0.6018\n",
      "Epoch [36/40], Batch [30/94], Loss: 0.5783\n",
      "Epoch [36/40], Batch [40/94], Loss: 0.6462\n",
      "Epoch [36/40], Batch [50/94], Loss: 0.4117\n",
      "Epoch [36/40], Batch [60/94], Loss: 0.5968\n",
      "Epoch [36/40], Batch [70/94], Loss: 0.4563\n",
      "Epoch [36/40], Batch [80/94], Loss: 0.3780\n",
      "Epoch [36/40], Batch [90/94], Loss: 0.5450\n",
      "------------------------------\n",
      "Epoch [36/40]\n",
      "Loss: 0.0167\n",
      "Accuracy: 74.13%\n",
      "------------------------------\n",
      "Epoch [37/40], Batch [10/94], Loss: 0.4859\n",
      "Epoch [37/40], Batch [20/94], Loss: 0.5167\n",
      "Epoch [37/40], Batch [30/94], Loss: 0.5997\n",
      "Epoch [37/40], Batch [40/94], Loss: 0.4505\n",
      "Epoch [37/40], Batch [50/94], Loss: 0.5282\n",
      "Epoch [37/40], Batch [60/94], Loss: 0.4649\n",
      "Epoch [37/40], Batch [70/94], Loss: 0.3904\n",
      "Epoch [37/40], Batch [80/94], Loss: 0.5257\n",
      "Epoch [37/40], Batch [90/94], Loss: 0.5887\n",
      "------------------------------\n",
      "Epoch [37/40]\n",
      "Loss: 0.0172\n",
      "Accuracy: 71.60%\n",
      "------------------------------\n",
      "Epoch [38/40], Batch [10/94], Loss: 0.6229\n",
      "Epoch [38/40], Batch [20/94], Loss: 0.4845\n",
      "Epoch [38/40], Batch [30/94], Loss: 0.5239\n",
      "Epoch [38/40], Batch [40/94], Loss: 0.4067\n",
      "Epoch [38/40], Batch [50/94], Loss: 0.6281\n",
      "Epoch [38/40], Batch [60/94], Loss: 0.5649\n",
      "Epoch [38/40], Batch [70/94], Loss: 0.6187\n",
      "Epoch [38/40], Batch [80/94], Loss: 0.4177\n",
      "Epoch [38/40], Batch [90/94], Loss: 0.6740\n",
      "------------------------------\n",
      "Epoch [38/40]\n",
      "Loss: 0.0169\n",
      "Accuracy: 72.23%\n",
      "------------------------------\n",
      "Epoch [39/40], Batch [10/94], Loss: 0.6437\n",
      "Epoch [39/40], Batch [20/94], Loss: 0.5372\n",
      "Epoch [39/40], Batch [30/94], Loss: 0.5611\n",
      "Epoch [39/40], Batch [40/94], Loss: 0.4832\n",
      "Epoch [39/40], Batch [50/94], Loss: 0.6233\n",
      "Epoch [39/40], Batch [60/94], Loss: 0.3595\n",
      "Epoch [39/40], Batch [70/94], Loss: 0.4429\n",
      "Epoch [39/40], Batch [80/94], Loss: 0.4581\n",
      "Epoch [39/40], Batch [90/94], Loss: 0.5452\n",
      "------------------------------\n",
      "Epoch [39/40]\n",
      "Loss: 0.0166\n",
      "Accuracy: 73.33%\n",
      "------------------------------\n",
      "Epoch [40/40], Batch [10/94], Loss: 0.5396\n",
      "Epoch [40/40], Batch [20/94], Loss: 0.4683\n",
      "Epoch [40/40], Batch [30/94], Loss: 0.6880\n",
      "Epoch [40/40], Batch [40/94], Loss: 0.5444\n",
      "Epoch [40/40], Batch [50/94], Loss: 0.4694\n",
      "Epoch [40/40], Batch [60/94], Loss: 0.3405\n",
      "Epoch [40/40], Batch [70/94], Loss: 0.4985\n",
      "Epoch [40/40], Batch [80/94], Loss: 0.4578\n",
      "Epoch [40/40], Batch [90/94], Loss: 0.4397\n",
      "------------------------------\n",
      "Epoch [40/40]\n",
      "Loss: 0.0168\n",
      "Accuracy: 72.83%\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    epoch_start_time = time.time()\n",
    "    \n",
    "    for batch_idx, (images, targets) in enumerate(train_loader):\n",
    "        # Move data to the same device as the model\n",
    "        images = images.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        predictions = model(images)  \n",
    "        loss = loss_func(predictions, targets)\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        _, predicted = torch.max(predictions.data, 1)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if (batch_idx + 1) % 10 == 0:\n",
    "            print(f\"Epoch [{epoch + 1}/{num_epochs}], \"\n",
    "                  f\"Batch [{batch_idx + 1}/{len(train_loader)}], \"\n",
    "                  f\"Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    avg_loss = total_loss / len(dataset)\n",
    "    accuracy = 100 * correct / len(dataset)\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}]\")\n",
    "    print(f\"Loss: {avg_loss:.4f}\")\n",
    "    print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # if device.type == 'cuda':\n",
    "    #     torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.4164\n",
      "Test Accuracy: 81.00%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_loss = 0.0\n",
    "    test_correct = 0\n",
    "    for test_images, test_targets in test_loader:\n",
    "        # Move test data to device\n",
    "        test_images = test_images.to(device)\n",
    "        test_targets = test_targets.to(device)\n",
    "        \n",
    "        test_predictions = model(test_images)  \n",
    "        loss = loss_func(test_predictions, test_targets)\n",
    "        test_loss += loss.item()\n",
    "        _, predicted = torch.max(test_predictions.data, 1)\n",
    "        test_correct += (predicted == test_targets).sum().item()\n",
    "    \n",
    "    avg_test_loss = test_loss / len(test_dataset)\n",
    "    test_accuracy = 100 * test_correct / len(test_dataset)\n",
    "    print(f\"Test Loss: {avg_test_loss:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy: 72.83%\n"
     ]
    }
   ],
   "source": [
    "times = datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "torch.save({\n",
    "'model_state_dict': model.state_dict(),\n",
    "'optimizer_state_dict': optimizer.state_dict(),\n",
    "'final_accuracy': accuracy,\n",
    "}, f'CNN_model_{model_number}_final_{times}.pth')\n",
    "\n",
    "print(f\"Best accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
