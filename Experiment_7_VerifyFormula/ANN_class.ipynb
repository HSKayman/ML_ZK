{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a5219e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888990cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "def set_seeds(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seeds(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335576a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare data\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec10cb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1206d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bea3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_t = torch.FloatTensor(X_train_scaled)\n",
    "X_test_t = torch.FloatTensor(X_test_scaled)\n",
    "y_train_t = torch.LongTensor(y_train)\n",
    "y_test_t = torch.LongTensor(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dafb62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN(nn.Module):\n",
    "    def __init__(self, input_size=4, hidden_sizes=[64, 32], num_classes=3):\n",
    "        super(ANN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_sizes[0])\n",
    "        self.fc2 = nn.Linear(hidden_sizes[0], hidden_sizes[1])\n",
    "        self.fc3 = nn.Linear(hidden_sizes[1], num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def copy_weights_from(self, other_model):\n",
    "        self.load_state_dict(other_model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b167b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_with_shuffled_data(model, X_train, y_train, epochs=50, batch_size=32, lr=0.001):\n",
    "    # Create shuffled dataset\n",
    "    shuffle_idx = torch.randperm(len(X_train))\n",
    "    X_shuffled = X_train[shuffle_idx]\n",
    "    y_shuffled = y_train[shuffle_idx]\n",
    "    \n",
    "    # Create DataLoader\n",
    "    dataset = TensorDataset(X_shuffled, y_shuffled)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Training\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for inputs, labels in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2a73d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize two models with IDENTICAL weights\n",
    "print(\"Initializing models with identical weights...\")\n",
    "model1 = ANN()\n",
    "model2 = ANN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c7ef68",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_weights = model1.state_dict()\n",
    "torch.save(initial_weights, 'initial_weights.pth')\n",
    "\n",
    "# Ensure model2 has exact same weights\n",
    "model2.load_state_dict(initial_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33db0008",
   "metadata": {},
   "outputs": [],
   "source": [
    "for (name1, param1), (name2, param2) in zip(model1.named_parameters(), model2.named_parameters()):\n",
    "    assert torch.equal(param1, param2), f\"Initial weights not identical for {name1}\"\n",
    "print(\"âœ“ Initial weights verified as identical\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e80fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTraining Model 1...\")\n",
    "model1 = train_model_with_shuffled_data(model1, X_train_t, y_train_t, epochs=50)\n",
    "\n",
    "print(\"Training Model 2 (with different shuffling)...\")\n",
    "model2 = train_model_with_shuffled_data(model2, X_train_t, y_train_t, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4518a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_activations(model, X_data):\n",
    "    activations = {}\n",
    "    hooks = []\n",
    "    \n",
    "    def get_activation(name):\n",
    "        def hook(module, input, output):\n",
    "            activations[name] = output.detach()\n",
    "        return hook\n",
    "    \n",
    "    # Register hooks\n",
    "    hooks.append(model.fc1.register_forward_hook(get_activation('fc1')))\n",
    "    hooks.append(model.fc2.register_forward_hook(get_activation('fc2')))\n",
    "    hooks.append(model.fc3.register_forward_hook(get_activation('fc3')))\n",
    "    \n",
    "    # Forward pass\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        _ = model(X_data)\n",
    "    \n",
    "    # Remove hooks\n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "    \n",
    "    return activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d718ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect activations for both models on the SAME test data\n",
    "print(\"\\nCollecting activations from both models...\")\n",
    "activations1 = get_all_activations(model1, X_test_t)\n",
    "activations2 = get_all_activations(model2, X_test_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb5ef6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_activation_differences(activations1, activations2):\n",
    "    results = []\n",
    "    \n",
    "    layers = ['fc1', 'fc2', 'fc3']\n",
    "    \n",
    "    for layer in layers:\n",
    "        act1 = activations1[layer]\n",
    "        act2 = activations2[layer]\n",
    "        \n",
    "        # Calculate differences\n",
    "        abs_diff = torch.abs(act1 - act2)\n",
    "        \n",
    "        # For each sample\n",
    "        for sample_idx in range(act1.shape[0]):\n",
    "            sample_diff = abs_diff[sample_idx]\n",
    "            \n",
    "            # For each neuron\n",
    "            for neuron_idx in range(sample_diff.shape[0]):\n",
    "                neuron_diff = sample_diff[neuron_idx].item()\n",
    "                \n",
    "                results.append({\n",
    "                    'layer': layer,\n",
    "                    'sample_idx': sample_idx,\n",
    "                    'neuron_idx': neuron_idx,\n",
    "                    'activation_model1': act1[sample_idx, neuron_idx].item(),\n",
    "                    'activation_model2': act2[sample_idx, neuron_idx].item(),\n",
    "                    'abs_difference': neuron_diff,\n",
    "                    'relative_difference': neuron_diff / (abs(act1[sample_idx, neuron_idx].item()) + 1e-10)\n",
    "                })\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be567259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze differences\n",
    "diff_df = analyze_activation_differences(activations1, activations2)\n",
    "print(f\"Total activation comparisons: {len(diff_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a696f0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find minimum differences per layer\n",
    "print(\"\\nMinimum absolute differences per layer:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "min_diffs_by_layer = diff_df.groupby('layer')['abs_difference'].agg(['min', 'mean', 'max', 'std'])\n",
    "print(min_diffs_by_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1feb6286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the neuron with the smallest difference for each layer\n",
    "print(\"\\nNeurons with smallest differences:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for layer in ['fc1', 'fc2', 'fc3']:\n",
    "    layer_data = diff_df[diff_df['layer'] == layer]\n",
    "    min_idx = layer_data['abs_difference'].idxmin()\n",
    "    min_row = layer_data.loc[min_idx]\n",
    "    \n",
    "    print(f\"\\n{layer}:\")\n",
    "    print(f\"  Sample {min_row['sample_idx']}, Neuron {min_row['neuron_idx']}\")\n",
    "    print(f\"  Model1 activation: {min_row['activation_model1']:.6f}\")\n",
    "    print(f\"  Model2 activation: {min_row['activation_model2']:.6f}\")\n",
    "    print(f\"  Absolute difference: {min_row['abs_difference']:.2e}\")\n",
    "    print(f\"  Relative difference: {min_row['relative_difference']:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f156934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize distribution of differences\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('Distribution of Activation Differences Between Identically Initialized Models', fontsize=16)\n",
    "\n",
    "# 1. Histogram of absolute differences by layer\n",
    "ax = axes[0, 0]\n",
    "for layer in ['fc1', 'fc2', 'fc3']:\n",
    "    layer_data = diff_df[diff_df['layer'] == layer]['abs_difference']\n",
    "    ax.hist(layer_data, bins=50, alpha=0.5, label=layer, density=True)\n",
    "\n",
    "ax.set_xlabel('Absolute Difference')\n",
    "ax.set_ylabel('Density')\n",
    "ax.set_title('Distribution of Absolute Differences')\n",
    "ax.set_yscale('log')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Box plot of differences by layer\n",
    "ax = axes[0, 1]\n",
    "diff_df.boxplot(column='abs_difference', by='layer', ax=ax)\n",
    "ax.set_yscale('log')\n",
    "ax.set_title('Absolute Differences by Layer')\n",
    "ax.set_xlabel('Layer')\n",
    "ax.set_ylabel('Absolute Difference (log scale)')\n",
    "\n",
    "# 3. Cumulative distribution\n",
    "ax = axes[1, 0]\n",
    "thresholds = np.logspace(-10, 0, 1000)\n",
    "\n",
    "for layer in ['fc1', 'fc2', 'fc3']:\n",
    "    layer_data = diff_df[diff_df['layer'] == layer]['abs_difference'].values\n",
    "    percentages = []\n",
    "    \n",
    "    for thresh in thresholds:\n",
    "        pct = np.sum(layer_data <= thresh) / len(layer_data) * 100\n",
    "        percentages.append(pct)\n",
    "    \n",
    "    ax.semilogx(thresholds, percentages, label=layer, linewidth=2)\n",
    "\n",
    "ax.set_xlabel('Threshold')\n",
    "ax.set_ylabel('Percentage of Activations Below Threshold')\n",
    "ax.set_title('Cumulative Distribution of Differences')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_ylim(0, 105)\n",
    "\n",
    "# 4. Scatter plot: Model1 vs Model2 activations\n",
    "ax = axes[1, 1]\n",
    "sample_data = diff_df[diff_df['sample_idx'] == 0]  # First sample\n",
    "\n",
    "for layer in ['fc1', 'fc2', 'fc3']:\n",
    "    layer_data = sample_data[sample_data['layer'] == layer]\n",
    "    ax.scatter(layer_data['activation_model1'], \n",
    "              layer_data['activation_model2'], \n",
    "              alpha=0.5, label=layer, s=20)\n",
    "\n",
    "ax.plot([-10, 10], [-10, 10], 'k--', alpha=0.5, label='y=x')\n",
    "ax.set_xlabel('Model 1 Activation')\n",
    "ax.set_ylabel('Model 2 Activation')\n",
    "ax.set_title('Activation Correlation (Sample 0)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('identical_init_differences.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b843422",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nThreshold Analysis for Identically Initialized Models:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "test_thresholds = [1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1]\n",
    "\n",
    "threshold_analysis = []\n",
    "\n",
    "for thresh in test_thresholds:\n",
    "    row = {'threshold': thresh}\n",
    "    \n",
    "    for layer in ['fc1', 'fc2', 'fc3']:\n",
    "        layer_data = diff_df[diff_df['layer'] == layer]['abs_difference'].values\n",
    "        pass_rate = np.sum(layer_data <= thresh) / len(layer_data) * 100\n",
    "        row[f'{layer}_pass_rate'] = pass_rate\n",
    "    \n",
    "    # All layers simultaneously\n",
    "    all_pass = 0\n",
    "    total_samples = len(diff_df['sample_idx'].unique())\n",
    "    \n",
    "    for sample_idx in diff_df['sample_idx'].unique():\n",
    "        sample_data = diff_df[diff_df['sample_idx'] == sample_idx]\n",
    "        \n",
    "        # Check if ALL neurons in ALL layers pass for this sample\n",
    "        max_diff_per_layer = sample_data.groupby('layer')['abs_difference'].max()\n",
    "        if all(max_diff_per_layer <= thresh):\n",
    "            all_pass += 1\n",
    "    \n",
    "    row['all_layers_pass_rate'] = (all_pass / total_samples) * 100\n",
    "    threshold_analysis.append(row)\n",
    "\n",
    "threshold_df = pd.DataFrame(threshold_analysis)\n",
    "print(threshold_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2726d30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find practical threshold\n",
    "print(\"\\nPractical Threshold Determination:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Find minimum difference across all comparisons\n",
    "global_min_diff = diff_df['abs_difference'].min()\n",
    "global_max_diff = diff_df['abs_difference'].max()\n",
    "\n",
    "print(f\"Global minimum difference: {global_min_diff:.2e}\")\n",
    "print(f\"Global maximum difference: {global_max_diff:.2e}\")\n",
    "\n",
    "# Find threshold for different confidence levels\n",
    "confidence_levels = [90, 95, 99, 99.9]\n",
    "\n",
    "print(\"\\nThresholds for different confidence levels:\")\n",
    "for conf in confidence_levels:\n",
    "    threshold = np.percentile(diff_df['abs_difference'], conf)\n",
    "    print(f\"  {conf}% of differences are below: {threshold:.2e}\")\n",
    "\n",
    "# %%\n",
    "# Analyze per-neuron statistics\n",
    "neuron_stats = diff_df.groupby(['layer', 'neuron_idx'])['abs_difference'].agg(['min', 'mean', 'max', 'std'])\n",
    "\n",
    "print(\"\\nNeurons with most consistent activations (lowest max difference):\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for layer in ['fc1', 'fc2', 'fc3']:\n",
    "    layer_stats = neuron_stats.loc[layer].sort_values('max')\n",
    "    print(f\"\\n{layer} - Top 5 most consistent neurons:\")\n",
    "    print(layer_stats.head())\n",
    "\n",
    "# %%\n",
    "# Create final recommendation plot\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Left: Show practical threshold ranges\n",
    "all_diffs = diff_df['abs_difference'].values\n",
    "percentiles = [50, 90, 95, 99, 99.9, 99.99]\n",
    "threshold_values = [np.percentile(all_diffs, p) for p in percentiles]\n",
    "\n",
    "bars = ax1.bar(range(len(percentiles)), threshold_values, color='skyblue')\n",
    "ax1.set_xticks(range(len(percentiles)))\n",
    "ax1.set_xticklabels([f'{p}%' for p in percentiles])\n",
    "ax1.set_yscale('log')\n",
    "ax1.set_ylabel('Threshold Value (log scale)')\n",
    "ax1.set_title('Threshold Values at Different Confidence Levels')\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, val in zip(bars, threshold_values):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height*1.5,\n",
    "             f'{val:.2e}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# Right: Compare with gradient-based reconstruction threshold\n",
    "ax2.axhline(y=0.007, color='red', linestyle='--', linewidth=2, label='Gradient reconstruction threshold (0.007)')\n",
    "ax2.axhline(y=global_min_diff, color='green', linestyle='--', linewidth=2, label=f'Min diff identical init ({global_min_diff:.2e})')\n",
    "ax2.axhline(y=np.percentile(all_diffs, 99.9), color='blue', linestyle='--', linewidth=2, \n",
    "            label=f'99.9% threshold ({np.percentile(all_diffs, 99.9):.2e})')\n",
    "\n",
    "ax2.set_ylim(1e-10, 1e0)\n",
    "ax2.set_yscale('log')\n",
    "ax2.set_ylabel('Threshold Value (log scale)')\n",
    "ax2.set_title('Threshold Comparison')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('threshold_recommendation.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# %%\n",
    "# Final summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"THRESHOLD RECOMMENDATION SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nBased on identical initialization analysis:\")\n",
    "print(f\"  - Minimum observed difference: {global_min_diff:.2e}\")\n",
    "print(f\"  - 99.9th percentile difference: {np.percentile(all_diffs, 99.9):.2e}\")\n",
    "print(f\"  - 99.99th percentile difference: {np.percentile(all_diffs, 99.99):.2e}\")\n",
    "\n",
    "print(\"\\nComparison with gradient reconstruction:\")\n",
    "print(f\"  - Gradient reconstruction threshold: 0.007\")\n",
    "print(f\"  - Ratio: {0.007 / np.percentile(all_diffs, 99.9):.0f}x larger than 99.9th percentile\")\n",
    "\n",
    "print(\"\\nRECOMMENDATION:\")\n",
    "practical_threshold = np.percentile(all_diffs, 99.99)\n",
    "print(f\"  Use threshold of {practical_threshold:.2e} for distinguishing genuinely different activations\")\n",
    "print(f\"  This ensures 99.99% confidence that differences above this threshold are meaningful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894cd9f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
