{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e10562b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hskay\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LlamaTokenizer, LlamaForCausalLM\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1229\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[1;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\utils\\import_utils.py:1851\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1849\u001b[0m     value \u001b[38;5;241m=\u001b[39m Placeholder\n\u001b[0;32m   1850\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m-> 1851\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1852\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[0;32m   1853\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\utils\\import_utils.py:1863\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1861\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_module\u001b[39m(\u001b[38;5;28mself\u001b[39m, module_name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m   1862\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1863\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1864\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1865\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1866\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1867\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1868\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\importlib\\__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    124\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\models\\__init__.py:15\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2020 The HuggingFace Team. All rights reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     16\u001b[0m     albert,\n\u001b[0;32m     17\u001b[0m     align,\n\u001b[0;32m     18\u001b[0m     altclip,\n\u001b[0;32m     19\u001b[0m     aria,\n\u001b[0;32m     20\u001b[0m     audio_spectrogram_transformer,\n\u001b[0;32m     21\u001b[0m     auto,\n\u001b[0;32m     22\u001b[0m     autoformer,\n\u001b[0;32m     23\u001b[0m     bamba,\n\u001b[0;32m     24\u001b[0m     bark,\n\u001b[0;32m     25\u001b[0m     bart,\n\u001b[0;32m     26\u001b[0m     barthez,\n\u001b[0;32m     27\u001b[0m     bartpho,\n\u001b[0;32m     28\u001b[0m     beit,\n\u001b[0;32m     29\u001b[0m     bert,\n\u001b[0;32m     30\u001b[0m     bert_generation,\n\u001b[0;32m     31\u001b[0m     bert_japanese,\n\u001b[0;32m     32\u001b[0m     bertweet,\n\u001b[0;32m     33\u001b[0m     big_bird,\n\u001b[0;32m     34\u001b[0m     bigbird_pegasus,\n\u001b[0;32m     35\u001b[0m     biogpt,\n\u001b[0;32m     36\u001b[0m     bit,\n\u001b[0;32m     37\u001b[0m     blenderbot,\n\u001b[0;32m     38\u001b[0m     blenderbot_small,\n\u001b[0;32m     39\u001b[0m     blip,\n\u001b[0;32m     40\u001b[0m     blip_2,\n\u001b[0;32m     41\u001b[0m     bloom,\n\u001b[0;32m     42\u001b[0m     bridgetower,\n\u001b[0;32m     43\u001b[0m     bros,\n\u001b[0;32m     44\u001b[0m     byt5,\n\u001b[0;32m     45\u001b[0m     camembert,\n\u001b[0;32m     46\u001b[0m     canine,\n\u001b[0;32m     47\u001b[0m     chameleon,\n\u001b[0;32m     48\u001b[0m     chinese_clip,\n\u001b[0;32m     49\u001b[0m     clap,\n\u001b[0;32m     50\u001b[0m     clip,\n\u001b[0;32m     51\u001b[0m     clipseg,\n\u001b[0;32m     52\u001b[0m     clvp,\n\u001b[0;32m     53\u001b[0m     code_llama,\n\u001b[0;32m     54\u001b[0m     codegen,\n\u001b[0;32m     55\u001b[0m     cohere,\n\u001b[0;32m     56\u001b[0m     cohere2,\n\u001b[0;32m     57\u001b[0m     colpali,\n\u001b[0;32m     58\u001b[0m     conditional_detr,\n\u001b[0;32m     59\u001b[0m     convbert,\n\u001b[0;32m     60\u001b[0m     convnext,\n\u001b[0;32m     61\u001b[0m     convnextv2,\n\u001b[0;32m     62\u001b[0m     cpm,\n\u001b[0;32m     63\u001b[0m     cpmant,\n\u001b[0;32m     64\u001b[0m     ctrl,\n\u001b[0;32m     65\u001b[0m     cvt,\n\u001b[0;32m     66\u001b[0m     dab_detr,\n\u001b[0;32m     67\u001b[0m     dac,\n\u001b[0;32m     68\u001b[0m     data2vec,\n\u001b[0;32m     69\u001b[0m     dbrx,\n\u001b[0;32m     70\u001b[0m     deberta,\n\u001b[0;32m     71\u001b[0m     deberta_v2,\n\u001b[0;32m     72\u001b[0m     decision_transformer,\n\u001b[0;32m     73\u001b[0m     deformable_detr,\n\u001b[0;32m     74\u001b[0m     deit,\n\u001b[0;32m     75\u001b[0m     deprecated,\n\u001b[0;32m     76\u001b[0m     depth_anything,\n\u001b[0;32m     77\u001b[0m     depth_pro,\n\u001b[0;32m     78\u001b[0m     detr,\n\u001b[0;32m     79\u001b[0m     dialogpt,\n\u001b[0;32m     80\u001b[0m     diffllama,\n\u001b[0;32m     81\u001b[0m     dinat,\n\u001b[0;32m     82\u001b[0m     dinov2,\n\u001b[0;32m     83\u001b[0m     dinov2_with_registers,\n\u001b[0;32m     84\u001b[0m     distilbert,\n\u001b[0;32m     85\u001b[0m     dit,\n\u001b[0;32m     86\u001b[0m     donut,\n\u001b[0;32m     87\u001b[0m     dpr,\n\u001b[0;32m     88\u001b[0m     dpt,\n\u001b[0;32m     89\u001b[0m     efficientnet,\n\u001b[0;32m     90\u001b[0m     electra,\n\u001b[0;32m     91\u001b[0m     emu3,\n\u001b[0;32m     92\u001b[0m     encodec,\n\u001b[0;32m     93\u001b[0m     encoder_decoder,\n\u001b[0;32m     94\u001b[0m     ernie,\n\u001b[0;32m     95\u001b[0m     esm,\n\u001b[0;32m     96\u001b[0m     falcon,\n\u001b[0;32m     97\u001b[0m     falcon_mamba,\n\u001b[0;32m     98\u001b[0m     fastspeech2_conformer,\n\u001b[0;32m     99\u001b[0m     flaubert,\n\u001b[0;32m    100\u001b[0m     flava,\n\u001b[0;32m    101\u001b[0m     fnet,\n\u001b[0;32m    102\u001b[0m     focalnet,\n\u001b[0;32m    103\u001b[0m     fsmt,\n\u001b[0;32m    104\u001b[0m     funnel,\n\u001b[0;32m    105\u001b[0m     fuyu,\n\u001b[0;32m    106\u001b[0m     gemma,\n\u001b[0;32m    107\u001b[0m     gemma2,\n\u001b[0;32m    108\u001b[0m     git,\n\u001b[0;32m    109\u001b[0m     glm,\n\u001b[0;32m    110\u001b[0m     glpn,\n\u001b[0;32m    111\u001b[0m     got_ocr2,\n\u001b[0;32m    112\u001b[0m     gpt2,\n\u001b[0;32m    113\u001b[0m     gpt_bigcode,\n\u001b[0;32m    114\u001b[0m     gpt_neo,\n\u001b[0;32m    115\u001b[0m     gpt_neox,\n\u001b[0;32m    116\u001b[0m     gpt_neox_japanese,\n\u001b[0;32m    117\u001b[0m     gpt_sw3,\n\u001b[0;32m    118\u001b[0m     gptj,\n\u001b[0;32m    119\u001b[0m     granite,\n\u001b[0;32m    120\u001b[0m     granitemoe,\n\u001b[0;32m    121\u001b[0m     granitemoeshared,\n\u001b[0;32m    122\u001b[0m     grounding_dino,\n\u001b[0;32m    123\u001b[0m     groupvit,\n\u001b[0;32m    124\u001b[0m     helium,\n\u001b[0;32m    125\u001b[0m     herbert,\n\u001b[0;32m    126\u001b[0m     hiera,\n\u001b[0;32m    127\u001b[0m     hubert,\n\u001b[0;32m    128\u001b[0m     ibert,\n\u001b[0;32m    129\u001b[0m     idefics,\n\u001b[0;32m    130\u001b[0m     idefics2,\n\u001b[0;32m    131\u001b[0m     idefics3,\n\u001b[0;32m    132\u001b[0m     ijepa,\n\u001b[0;32m    133\u001b[0m     imagegpt,\n\u001b[0;32m    134\u001b[0m     informer,\n\u001b[0;32m    135\u001b[0m     instructblip,\n\u001b[0;32m    136\u001b[0m     instructblipvideo,\n\u001b[0;32m    137\u001b[0m     jamba,\n\u001b[0;32m    138\u001b[0m     jetmoe,\n\u001b[0;32m    139\u001b[0m     kosmos2,\n\u001b[0;32m    140\u001b[0m     layoutlm,\n\u001b[0;32m    141\u001b[0m     layoutlmv2,\n\u001b[0;32m    142\u001b[0m     layoutlmv3,\n\u001b[0;32m    143\u001b[0m     layoutxlm,\n\u001b[0;32m    144\u001b[0m     led,\n\u001b[0;32m    145\u001b[0m     levit,\n\u001b[0;32m    146\u001b[0m     lilt,\n\u001b[0;32m    147\u001b[0m     llama,\n\u001b[0;32m    148\u001b[0m     llava,\n\u001b[0;32m    149\u001b[0m     llava_next,\n\u001b[0;32m    150\u001b[0m     llava_next_video,\n\u001b[0;32m    151\u001b[0m     llava_onevision,\n\u001b[0;32m    152\u001b[0m     longformer,\n\u001b[0;32m    153\u001b[0m     longt5,\n\u001b[0;32m    154\u001b[0m     luke,\n\u001b[0;32m    155\u001b[0m     lxmert,\n\u001b[0;32m    156\u001b[0m     m2m_100,\n\u001b[0;32m    157\u001b[0m     mamba,\n\u001b[0;32m    158\u001b[0m     mamba2,\n\u001b[0;32m    159\u001b[0m     marian,\n\u001b[0;32m    160\u001b[0m     markuplm,\n\u001b[0;32m    161\u001b[0m     mask2former,\n\u001b[0;32m    162\u001b[0m     maskformer,\n\u001b[0;32m    163\u001b[0m     mbart,\n\u001b[0;32m    164\u001b[0m     mbart50,\n\u001b[0;32m    165\u001b[0m     megatron_bert,\n\u001b[0;32m    166\u001b[0m     megatron_gpt2,\n\u001b[0;32m    167\u001b[0m     mgp_str,\n\u001b[0;32m    168\u001b[0m     mimi,\n\u001b[0;32m    169\u001b[0m     mistral,\n\u001b[0;32m    170\u001b[0m     mixtral,\n\u001b[0;32m    171\u001b[0m     mllama,\n\u001b[0;32m    172\u001b[0m     mluke,\n\u001b[0;32m    173\u001b[0m     mobilebert,\n\u001b[0;32m    174\u001b[0m     mobilenet_v1,\n\u001b[0;32m    175\u001b[0m     mobilenet_v2,\n\u001b[0;32m    176\u001b[0m     mobilevit,\n\u001b[0;32m    177\u001b[0m     mobilevitv2,\n\u001b[0;32m    178\u001b[0m     modernbert,\n\u001b[0;32m    179\u001b[0m     moonshine,\n\u001b[0;32m    180\u001b[0m     moshi,\n\u001b[0;32m    181\u001b[0m     mpnet,\n\u001b[0;32m    182\u001b[0m     mpt,\n\u001b[0;32m    183\u001b[0m     mra,\n\u001b[0;32m    184\u001b[0m     mt5,\n\u001b[0;32m    185\u001b[0m     musicgen,\n\u001b[0;32m    186\u001b[0m     musicgen_melody,\n\u001b[0;32m    187\u001b[0m     mvp,\n\u001b[0;32m    188\u001b[0m     myt5,\n\u001b[0;32m    189\u001b[0m     nemotron,\n\u001b[0;32m    190\u001b[0m     nllb,\n\u001b[0;32m    191\u001b[0m     nllb_moe,\n\u001b[0;32m    192\u001b[0m     nougat,\n\u001b[0;32m    193\u001b[0m     nystromformer,\n\u001b[0;32m    194\u001b[0m     olmo,\n\u001b[0;32m    195\u001b[0m     olmo2,\n\u001b[0;32m    196\u001b[0m     olmoe,\n\u001b[0;32m    197\u001b[0m     omdet_turbo,\n\u001b[0;32m    198\u001b[0m     oneformer,\n\u001b[0;32m    199\u001b[0m     openai,\n\u001b[0;32m    200\u001b[0m     opt,\n\u001b[0;32m    201\u001b[0m     owlv2,\n\u001b[0;32m    202\u001b[0m     owlvit,\n\u001b[0;32m    203\u001b[0m     paligemma,\n\u001b[0;32m    204\u001b[0m     patchtsmixer,\n\u001b[0;32m    205\u001b[0m     patchtst,\n\u001b[0;32m    206\u001b[0m     pegasus,\n\u001b[0;32m    207\u001b[0m     pegasus_x,\n\u001b[0;32m    208\u001b[0m     perceiver,\n\u001b[0;32m    209\u001b[0m     persimmon,\n\u001b[0;32m    210\u001b[0m     phi,\n\u001b[0;32m    211\u001b[0m     phi3,\n\u001b[0;32m    212\u001b[0m     phimoe,\n\u001b[0;32m    213\u001b[0m     phobert,\n\u001b[0;32m    214\u001b[0m     pix2struct,\n\u001b[0;32m    215\u001b[0m     pixtral,\n\u001b[0;32m    216\u001b[0m     plbart,\n\u001b[0;32m    217\u001b[0m     poolformer,\n\u001b[0;32m    218\u001b[0m     pop2piano,\n\u001b[0;32m    219\u001b[0m     prophetnet,\n\u001b[0;32m    220\u001b[0m     pvt,\n\u001b[0;32m    221\u001b[0m     pvt_v2,\n\u001b[0;32m    222\u001b[0m     qwen2,\n\u001b[0;32m    223\u001b[0m     qwen2_5_vl,\n\u001b[0;32m    224\u001b[0m     qwen2_audio,\n\u001b[0;32m    225\u001b[0m     qwen2_moe,\n\u001b[0;32m    226\u001b[0m     qwen2_vl,\n\u001b[0;32m    227\u001b[0m     rag,\n\u001b[0;32m    228\u001b[0m     recurrent_gemma,\n\u001b[0;32m    229\u001b[0m     reformer,\n\u001b[0;32m    230\u001b[0m     regnet,\n\u001b[0;32m    231\u001b[0m     rembert,\n\u001b[0;32m    232\u001b[0m     resnet,\n\u001b[0;32m    233\u001b[0m     roberta,\n\u001b[0;32m    234\u001b[0m     roberta_prelayernorm,\n\u001b[0;32m    235\u001b[0m     roc_bert,\n\u001b[0;32m    236\u001b[0m     roformer,\n\u001b[0;32m    237\u001b[0m     rt_detr,\n\u001b[0;32m    238\u001b[0m     rt_detr_v2,\n\u001b[0;32m    239\u001b[0m     rwkv,\n\u001b[0;32m    240\u001b[0m     sam,\n\u001b[0;32m    241\u001b[0m     seamless_m4t,\n\u001b[0;32m    242\u001b[0m     seamless_m4t_v2,\n\u001b[0;32m    243\u001b[0m     segformer,\n\u001b[0;32m    244\u001b[0m     seggpt,\n\u001b[0;32m    245\u001b[0m     sew,\n\u001b[0;32m    246\u001b[0m     sew_d,\n\u001b[0;32m    247\u001b[0m     siglip,\n\u001b[0;32m    248\u001b[0m     speech_encoder_decoder,\n\u001b[0;32m    249\u001b[0m     speech_to_text,\n\u001b[0;32m    250\u001b[0m     speecht5,\n\u001b[0;32m    251\u001b[0m     splinter,\n\u001b[0;32m    252\u001b[0m     squeezebert,\n\u001b[0;32m    253\u001b[0m     stablelm,\n\u001b[0;32m    254\u001b[0m     starcoder2,\n\u001b[0;32m    255\u001b[0m     superglue,\n\u001b[0;32m    256\u001b[0m     superpoint,\n\u001b[0;32m    257\u001b[0m     swiftformer,\n\u001b[0;32m    258\u001b[0m     swin,\n\u001b[0;32m    259\u001b[0m     swin2sr,\n\u001b[0;32m    260\u001b[0m     swinv2,\n\u001b[0;32m    261\u001b[0m     switch_transformers,\n\u001b[0;32m    262\u001b[0m     t5,\n\u001b[0;32m    263\u001b[0m     table_transformer,\n\u001b[0;32m    264\u001b[0m     tapas,\n\u001b[0;32m    265\u001b[0m     textnet,\n\u001b[0;32m    266\u001b[0m     time_series_transformer,\n\u001b[0;32m    267\u001b[0m     timesformer,\n\u001b[0;32m    268\u001b[0m     timm_backbone,\n\u001b[0;32m    269\u001b[0m     timm_wrapper,\n\u001b[0;32m    270\u001b[0m     trocr,\n\u001b[0;32m    271\u001b[0m     tvp,\n\u001b[0;32m    272\u001b[0m     udop,\n\u001b[0;32m    273\u001b[0m     umt5,\n\u001b[0;32m    274\u001b[0m     unispeech,\n\u001b[0;32m    275\u001b[0m     unispeech_sat,\n\u001b[0;32m    276\u001b[0m     univnet,\n\u001b[0;32m    277\u001b[0m     upernet,\n\u001b[0;32m    278\u001b[0m     video_llava,\n\u001b[0;32m    279\u001b[0m     videomae,\n\u001b[0;32m    280\u001b[0m     vilt,\n\u001b[0;32m    281\u001b[0m     vipllava,\n\u001b[0;32m    282\u001b[0m     vision_encoder_decoder,\n\u001b[0;32m    283\u001b[0m     vision_text_dual_encoder,\n\u001b[0;32m    284\u001b[0m     visual_bert,\n\u001b[0;32m    285\u001b[0m     vit,\n\u001b[0;32m    286\u001b[0m     vit_mae,\n\u001b[0;32m    287\u001b[0m     vit_msn,\n\u001b[0;32m    288\u001b[0m     vitdet,\n\u001b[0;32m    289\u001b[0m     vitmatte,\n\u001b[0;32m    290\u001b[0m     vitpose,\n\u001b[0;32m    291\u001b[0m     vitpose_backbone,\n\u001b[0;32m    292\u001b[0m     vits,\n\u001b[0;32m    293\u001b[0m     vivit,\n\u001b[0;32m    294\u001b[0m     wav2vec2,\n\u001b[0;32m    295\u001b[0m     wav2vec2_bert,\n\u001b[0;32m    296\u001b[0m     wav2vec2_conformer,\n\u001b[0;32m    297\u001b[0m     wav2vec2_phoneme,\n\u001b[0;32m    298\u001b[0m     wav2vec2_with_lm,\n\u001b[0;32m    299\u001b[0m     wavlm,\n\u001b[0;32m    300\u001b[0m     whisper,\n\u001b[0;32m    301\u001b[0m     x_clip,\n\u001b[0;32m    302\u001b[0m     xglm,\n\u001b[0;32m    303\u001b[0m     xlm,\n\u001b[0;32m    304\u001b[0m     xlm_roberta,\n\u001b[0;32m    305\u001b[0m     xlm_roberta_xl,\n\u001b[0;32m    306\u001b[0m     xlnet,\n\u001b[0;32m    307\u001b[0m     xmod,\n\u001b[0;32m    308\u001b[0m     yolos,\n\u001b[0;32m    309\u001b[0m     yoso,\n\u001b[0;32m    310\u001b[0m     zamba,\n\u001b[0;32m    311\u001b[0m     zamba2,\n\u001b[0;32m    312\u001b[0m     zoedepth,\n\u001b[0;32m    313\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\models\\mobilenet_v1\\__init__.py:29\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[0;32m     28\u001b[0m _file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mglobals\u001b[39m()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__file__\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m---> 29\u001b[0m sys\u001b[38;5;241m.\u001b[39mmodules[\u001b[38;5;18m__name__\u001b[39m] \u001b[38;5;241m=\u001b[39m _LazyModule(\u001b[38;5;18m__name__\u001b[39m, _file, \u001b[43mdefine_import_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_file\u001b[49m\u001b[43m)\u001b[49m, module_spec\u001b[38;5;241m=\u001b[39m__spec__)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\utils\\import_utils.py:2287\u001b[0m, in \u001b[0;36mdefine_import_structure\u001b[1;34m(module_path)\u001b[0m\n\u001b[0;32m   2266\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefine_import_structure\u001b[39m(module_path: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m IMPORT_STRUCTURE_T:\n\u001b[0;32m   2267\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2268\u001b[0m \u001b[38;5;124;03m    This method takes a module_path as input and creates an import structure digestible by a _LazyModule.\u001b[39;00m\n\u001b[0;32m   2269\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2285\u001b[0m \u001b[38;5;124;03m    The import structure is a dict defined with frozensets as keys, and dicts of strings to sets of objects.\u001b[39;00m\n\u001b[0;32m   2286\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2287\u001b[0m     import_structure \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_import_structure_from_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2288\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m spread_import_structure(import_structure)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\utils\\import_utils.py:2057\u001b[0m, in \u001b[0;36mcreate_import_structure_from_path\u001b[1;34m(module_path)\u001b[0m\n\u001b[0;32m   2054\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m module_name\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.py\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   2055\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m-> 2057\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(directory, module_name), encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m   2058\u001b[0m     file_content \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m   2060\u001b[0m \u001b[38;5;66;03m# Remove the .py suffix\u001b[39;00m\n",
      "File \u001b[1;32m<frozen codecs>:309\u001b[0m, in \u001b[0;36m__init__\u001b[1;34m(self, errors)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "import os\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8863042",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "MODEL_1_PATH = \"meta-llama/Llama-2-7b-chat-hf\" \n",
    "MODEL_2_PATH = \"meta-llama/Llama-2-7b-hf\"       \n",
    "\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3911279d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:13<00:00,  6.58s/it]\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:49<00:00, 24.82s/it]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = LlamaTokenizer.from_pretrained(MODEL_1_PATH)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model_1 = LlamaForCausalLM.from_pretrained(\n",
    "    MODEL_1_PATH,\n",
    "    torch_dtype=torch.float32, \n",
    "    device_map=DEVICE\n",
    ")\n",
    "\n",
    "model_2 = LlamaForCausalLM.from_pretrained(\n",
    "    MODEL_2_PATH,\n",
    "    torch_dtype=torch.float32, \n",
    "    device_map=DEVICE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ea8373",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = model_1.to(DEVICE)\n",
    "model_2 = model_2.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdd67eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_weights(model, move_to_cpu=False,layer_range=None):\n",
    "    weights = {}\n",
    "    \n",
    "    # Embedding weights\n",
    "    embed_weight = model.model.embed_tokens.weight\n",
    "    weights['embed_tokens'] = embed_weight.detach().cpu() if move_to_cpu else embed_weight.detach()\n",
    "    \n",
    "    # Layer-specific weights\n",
    "    if layer_range is None:\n",
    "        layer_range = range(len(model.model.layers))\n",
    "    \n",
    "    for i in layer_range:\n",
    "        if i >= len(model.model.layers):\n",
    "            continue\n",
    "            \n",
    "        layer = model.model.layers[i]\n",
    "        layer_prefix = f\"layer_{i}\"\n",
    "        \n",
    "        # Self-attention weights\n",
    "        weights[f\"{layer_prefix}_q_proj\"] = layer.self_attn.q_proj.weight.detach().cpu() if move_to_cpu else layer.self_attn.q_proj.weight.detach()\n",
    "        weights[f\"{layer_prefix}_k_proj\"] = layer.self_attn.k_proj.weight.detach().cpu() if move_to_cpu else layer.self_attn.k_proj.weight.detach()\n",
    "        weights[f\"{layer_prefix}_v_proj\"] = layer.self_attn.v_proj.weight.detach().cpu() if move_to_cpu else layer.self_attn.v_proj.weight.detach()\n",
    "        weights[f\"{layer_prefix}_o_proj\"] = layer.self_attn.o_proj.weight.detach().cpu() if move_to_cpu else layer.self_attn.o_proj.weight.detach()\n",
    "        \n",
    "        # MLP weights\n",
    "        weights[f\"{layer_prefix}_gate_proj\"] = layer.mlp.gate_proj.weight.detach().cpu() if move_to_cpu else layer.mlp.gate_proj.weight.detach()\n",
    "        weights[f\"{layer_prefix}_up_proj\"] = layer.mlp.up_proj.weight.detach().cpu() if move_to_cpu else layer.mlp.up_proj.weight.detach()\n",
    "        weights[f\"{layer_prefix}_down_proj\"] = layer.mlp.down_proj.weight.detach().cpu() if move_to_cpu else layer.mlp.down_proj.weight.detach()\n",
    "        \n",
    "        # Layer norm weights\n",
    "        weights[f\"{layer_prefix}_input_layernorm\"] = layer.input_layernorm.weight.detach().cpu() if move_to_cpu else layer.input_layernorm.weight.detach()\n",
    "        weights[f\"{layer_prefix}_post_attention_layernorm\"] = layer.post_attention_layernorm.weight.detach().cpu() if move_to_cpu else layer.post_attention_layernorm.weight.detach()\n",
    "    \n",
    "    # Final layer norm and LM head\n",
    "    weights['final_norm'] = model.model.norm.weight.detach().cpu() if move_to_cpu else model.model.norm.weight.detach()\n",
    "    weights['lm_head'] = model.lm_head.weight.detach().cpu() if move_to_cpu else model.lm_head.weight.detach()\n",
    "    \n",
    "    return weights\n",
    "\n",
    "def calculate_weight_differences(weights_1, weights_2):\n",
    "    differences = {}\n",
    "    \n",
    "    common_keys = set(weights_1.keys()) & set(weights_2.keys())\n",
    "    print(f\"Comparing {len(common_keys)} weight matrices...\")\n",
    "    \n",
    "    for i, key in enumerate(common_keys):\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Processing {i+1}/{len(common_keys)}: {key}\")\n",
    "            \n",
    "        w1 = weights_1[key]\n",
    "        w2 = weights_2[key]\n",
    "        \n",
    "        if w1.shape != w2.shape:\n",
    "            print(f\"Warning: Shape mismatch for {key}: {w1.shape} vs {w2.shape}\")\n",
    "            continue\n",
    "        \n",
    "        # Calculate difference matrix\n",
    "        diff_matrix = w1 - w2\n",
    "        \n",
    "        # Calculate various norms and statistics\n",
    "        frobenius_norm = torch.norm(diff_matrix, p='fro').item()\n",
    "        frobenius_norm_relative = frobenius_norm / (torch.norm(w1, p='fro').item() + 1e-10)\n",
    "        \n",
    "        spectral_norm = torch.norm(diff_matrix, p=2).item()\n",
    "        spectral_norm_relative = spectral_norm / (torch.norm(w1, p=2).item() + 1e-10)\n",
    "        \n",
    "        # Element-wise statistics\n",
    "        abs_diff = torch.abs(diff_matrix)\n",
    "        mean_abs_diff = torch.mean(abs_diff).item()\n",
    "        max_abs_diff = torch.max(abs_diff).item()\n",
    "        std_diff = torch.std(diff_matrix).item()\n",
    "        \n",
    "        # Percentage of significantly different weights (threshold = 1e-6)\n",
    "        significant_diff_ratio = (abs_diff > 1e-6).float().mean().item()\n",
    "        \n",
    "        # Cosine similarity\n",
    "        w1_flat = w1.flatten()\n",
    "        w2_flat = w2.flatten()\n",
    "        cosine_sim = F.cosine_similarity(w1_flat.unsqueeze(0), w2_flat.unsqueeze(0)).item()\n",
    "        \n",
    "        differences[key] = {\n",
    "            'frobenius_norm': frobenius_norm,\n",
    "            'frobenius_norm_relative': frobenius_norm_relative,\n",
    "            'spectral_norm': spectral_norm,\n",
    "            'spectral_norm_relative': spectral_norm_relative,\n",
    "            'mean_abs_difference': mean_abs_diff,\n",
    "            'max_abs_difference': max_abs_diff,\n",
    "            'std_difference': std_diff,\n",
    "            'significant_diff_ratio': significant_diff_ratio,\n",
    "            'cosine_similarity': cosine_sim,\n",
    "            'weight_shape': w1.shape,\n",
    "            'total_parameters': w1.numel()\n",
    "        }\n",
    "    \n",
    "    return differences\n",
    "\n",
    "def analyze_weight_patterns(weight_differences):\n",
    "    analysis = {\n",
    "        'by_component_type': defaultdict(list),\n",
    "        'by_layer_depth': defaultdict(list),\n",
    "        'summary_stats': {}\n",
    "    }\n",
    "    \n",
    "    # Group by component type\n",
    "    for layer_name, diff_data in weight_differences.items():\n",
    "        if any(x in layer_name for x in ['q_proj', 'k_proj', 'v_proj', 'o_proj']):\n",
    "            component_type = 'attention'\n",
    "        elif any(x in layer_name for x in ['gate_proj', 'up_proj', 'down_proj']):\n",
    "            component_type = 'mlp'\n",
    "        elif 'layernorm' in layer_name or 'norm' in layer_name:\n",
    "            component_type = 'normalization'\n",
    "        elif 'embed' in layer_name:\n",
    "            component_type = 'embedding'\n",
    "        elif 'lm_head' in layer_name:\n",
    "            component_type = 'output'\n",
    "        else:\n",
    "            component_type = 'other'\n",
    "        \n",
    "        analysis['by_component_type'][component_type].append({\n",
    "            'layer_name': layer_name,\n",
    "            'frobenius_norm': diff_data['frobenius_norm'],\n",
    "            'frobenius_norm_relative': diff_data['frobenius_norm_relative'],\n",
    "            'significant_diff_ratio': diff_data['significant_diff_ratio'],\n",
    "            'cosine_similarity': diff_data['cosine_similarity']\n",
    "        })\n",
    "    \n",
    "    # Group by layer depth\n",
    "    for layer_name, diff_data in weight_differences.items():\n",
    "        if 'layer_' in layer_name:\n",
    "            try:\n",
    "                layer_num = int(layer_name.split('_')[1])\n",
    "                analysis['by_layer_depth'][layer_num].append({\n",
    "                    'layer_name': layer_name,\n",
    "                    'frobenius_norm': diff_data['frobenius_norm'],\n",
    "                    'frobenius_norm_relative': diff_data['frobenius_norm_relative'],\n",
    "                    'cosine_similarity': diff_data['cosine_similarity']\n",
    "                })\n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "    # Calculate summary statistics\n",
    "    all_frobenius = [data['frobenius_norm'] for data in weight_differences.values()]\n",
    "    all_frobenius_rel = [data['frobenius_norm_relative'] for data in weight_differences.values()]\n",
    "    all_significant_ratios = [data['significant_diff_ratio'] for data in weight_differences.values()]\n",
    "    all_cosine_sims = [data['cosine_similarity'] for data in weight_differences.values()]\n",
    "    \n",
    "    analysis['summary_stats'] = {\n",
    "        'total_layers_compared': len(weight_differences),\n",
    "        'mean_frobenius_norm': np.mean(all_frobenius),\n",
    "        'std_frobenius_norm': np.std(all_frobenius),\n",
    "        'max_frobenius_norm': np.max(all_frobenius),\n",
    "        'min_frobenius_norm': np.min(all_frobenius),\n",
    "        'mean_frobenius_norm_relative': np.mean(all_frobenius_rel),\n",
    "        'mean_significant_diff_ratio': np.mean(all_significant_ratios),\n",
    "        'mean_cosine_similarity': np.mean(all_cosine_sims),\n",
    "        'min_cosine_similarity': np.min(all_cosine_sims),\n",
    "        'total_parameters_compared': sum(data['total_parameters'] for data in weight_differences.values())\n",
    "    }\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "def print_weight_analysis_summary(analysis):\n",
    "    print(\"=\"*70)\n",
    "    print(\"LLAMA MODEL WEIGHT DIFFERENCE ANALYSIS SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Overall statistics\n",
    "    stats = analysis['summary_stats']\n",
    "    print(f\"\\nðŸ“Š OVERALL STATISTICS:\")\n",
    "    print(f\"  â€¢ Total layers compared: {stats['total_layers_compared']}\")\n",
    "    print(f\"  â€¢ Total parameters compared: {stats['total_parameters_compared']:,}\")\n",
    "    print(f\"  â€¢ Mean Frobenius norm: {stats['mean_frobenius_norm']:.2e}\")\n",
    "    print(f\"  â€¢ Mean relative Frobenius norm: {stats['mean_frobenius_norm_relative']:.8f}\")\n",
    "    print(f\"  â€¢ Max Frobenius norm: {stats['max_frobenius_norm']:.2e}\")\n",
    "    print(f\"  â€¢ Min Frobenius norm: {stats['min_frobenius_norm']:.2e}\")\n",
    "    print(f\"  â€¢ Mean cosine similarity: {stats['mean_cosine_similarity']:.8f}\")\n",
    "    print(f\"  â€¢ Min cosine similarity: {stats['min_cosine_similarity']:.8f}\")\n",
    "    print(f\"  â€¢ Mean significant difference ratio: {stats['mean_significant_diff_ratio']:.4f}\")\n",
    "    \n",
    "    # Component type analysis\n",
    "    print(f\"\\nðŸ”§ BY COMPONENT TYPE:\")\n",
    "    for comp_type, comp_data in analysis['by_component_type'].items():\n",
    "        frob_norms = [item['frobenius_norm_relative'] for item in comp_data]\n",
    "        cosine_sims = [item['cosine_similarity'] for item in comp_data]\n",
    "        sig_ratios = [item['significant_diff_ratio'] for item in comp_data]\n",
    "        \n",
    "        print(f\"  {comp_type.upper()}:\")\n",
    "        print(f\"    - Count: {len(comp_data)} layers\")\n",
    "        print(f\"    - Mean relative Frobenius: {np.mean(frob_norms):.8f} Â± {np.std(frob_norms):.8f}\")\n",
    "        print(f\"    - Mean cosine similarity: {np.mean(cosine_sims):.8f} Â± {np.std(cosine_sims):.8f}\")\n",
    "        print(f\"    - Mean sig. diff ratio: {np.mean(sig_ratios):.4f}\")\n",
    "    \n",
    "    # Layer depth analysis (if available)\n",
    "    if analysis['by_layer_depth']:\n",
    "        print(f\"\\nðŸ“ˆ BY LAYER DEPTH:\")\n",
    "        for depth in sorted(analysis['by_layer_depth'].keys())[:10]:  # Show first 10 layers\n",
    "            depth_data = analysis['by_layer_depth'][depth]\n",
    "            frob_norms = [item['frobenius_norm_relative'] for item in depth_data]\n",
    "            cosine_sims = [item['cosine_similarity'] for item in depth_data]\n",
    "            \n",
    "            print(f\"  Layer {depth}: Frob={np.mean(frob_norms):.6f}, Cosine={np.mean(cosine_sims):.6f}\")\n",
    "    \n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec280794",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_1 = get_model_weights(model_1)\n",
    "weights_2 = get_model_weights(model_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566bf7c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing 291 weight matrices...\n",
      "Processing 1/291: layer_22_down_proj\n",
      "Processing 11/291: layer_15_post_attention_layernorm\n",
      "Processing 21/291: layer_27_gate_proj\n",
      "Processing 31/291: layer_31_post_attention_layernorm\n",
      "Processing 41/291: layer_3_q_proj\n",
      "Processing 51/291: layer_17_up_proj\n",
      "Processing 61/291: layer_14_up_proj\n",
      "Processing 71/291: layer_15_gate_proj\n",
      "Processing 81/291: layer_17_post_attention_layernorm\n",
      "Processing 91/291: layer_8_down_proj\n",
      "Processing 101/291: layer_24_input_layernorm\n",
      "Processing 111/291: layer_21_input_layernorm\n",
      "Processing 121/291: layer_28_v_proj\n",
      "Processing 131/291: layer_6_v_proj\n",
      "Processing 141/291: layer_29_v_proj\n",
      "Processing 151/291: layer_20_q_proj\n",
      "Processing 161/291: layer_12_o_proj\n",
      "Processing 171/291: layer_12_gate_proj\n",
      "Processing 181/291: layer_20_up_proj\n",
      "Processing 191/291: layer_19_q_proj\n",
      "Processing 201/291: layer_22_post_attention_layernorm\n",
      "Processing 211/291: final_norm\n",
      "Processing 221/291: layer_14_input_layernorm\n",
      "Processing 231/291: layer_26_k_proj\n",
      "Processing 241/291: layer_8_o_proj\n",
      "Processing 251/291: layer_11_gate_proj\n",
      "Processing 261/291: layer_18_up_proj\n",
      "Processing 271/291: layer_31_input_layernorm\n",
      "Processing 281/291: layer_30_input_layernorm\n",
      "Processing 291/291: layer_25_gate_proj\n"
     ]
    }
   ],
   "source": [
    "weight_differences = calculate_weight_differences(weights_1, weights_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff68a2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis = analyze_weight_patterns(weight_differences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f8f2a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "LLAMA MODEL WEIGHT DIFFERENCE ANALYSIS SUMMARY\n",
      "======================================================================\n",
      "\n",
      "ðŸ“Š OVERALL STATISTICS:\n",
      "  â€¢ Total layers compared: 291\n",
      "  â€¢ Total parameters compared: 6,738,415,616\n",
      "  â€¢ Mean Frobenius norm: 5.00e+00\n",
      "  â€¢ Mean relative Frobenius norm: 0.05240598\n",
      "  â€¢ Max Frobenius norm: 2.33e+01\n",
      "  â€¢ Min Frobenius norm: 6.37e-02\n",
      "  â€¢ Mean cosine similarity: 1.00115807\n",
      "  â€¢ Min cosine similarity: 0.99230218\n",
      "  â€¢ Mean significant difference ratio: 0.9642\n",
      "\n",
      "ðŸ”§ BY COMPONENT TYPE:\n",
      "  MLP:\n",
      "    - Count: 96 layers\n",
      "    - Mean relative Frobenius: 0.06610288 Â± 0.00257659\n",
      "    - Mean cosine similarity: 1.00398319 Â± 0.00041742\n",
      "    - Mean sig. diff ratio: 0.9729\n",
      "  ATTENTION:\n",
      "    - Count: 128 layers\n",
      "    - Mean relative Frobenius: 0.06287519 Â± 0.01413015\n",
      "    - Mean cosine similarity: 0.99922307 Â± 0.00117901\n",
      "    - Mean sig. diff ratio: 0.9713\n",
      "  NORMALIZATION:\n",
      "    - Count: 65 layers\n",
      "    - Mean relative Frobenius: 0.01038052 Â± 0.00265356\n",
      "    - Mean cosine similarity: 0.99998514 Â± 0.00004352\n",
      "    - Mean sig. diff ratio: 0.9371\n",
      "  EMBEDDING:\n",
      "    - Count: 1 layers\n",
      "    - Mean relative Frobenius: 0.05791559 Â± 0.00000000\n",
      "    - Mean cosine similarity: 1.02927232 Â± 0.00000000\n",
      "    - Mean sig. diff ratio: 0.9617\n",
      "  OUTPUT:\n",
      "    - Count: 1 layers\n",
      "    - Mean relative Frobenius: 0.12358926 Â± 0.00000000\n",
      "    - Mean cosine similarity: 1.02575266 Â± 0.00000000\n",
      "    - Mean sig. diff ratio: 0.9832\n",
      "\n",
      "ðŸ“ˆ BY LAYER DEPTH:\n",
      "  Layer 0: Frob=0.061750, Cosine=1.000780\n",
      "  Layer 1: Frob=0.057145, Cosine=1.000603\n",
      "  Layer 2: Frob=0.048795, Cosine=1.001271\n",
      "  Layer 3: Frob=0.051349, Cosine=1.001073\n",
      "  Layer 4: Frob=0.050610, Cosine=1.001082\n",
      "  Layer 5: Frob=0.049889, Cosine=1.001132\n",
      "  Layer 6: Frob=0.052620, Cosine=1.001073\n",
      "  Layer 7: Frob=0.052589, Cosine=1.001031\n",
      "  Layer 8: Frob=0.052245, Cosine=1.001056\n",
      "  Layer 9: Frob=0.051850, Cosine=1.001013\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print_weight_analysis_summary(analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2a3bf55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "activations_model_1 = {}\n",
    "activations_model_2 = {}\n",
    "current_hooks = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "854f9fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_activations():\n",
    "    global activations_model_1, activations_model_2\n",
    "    activations_model_1.clear()\n",
    "    activations_model_2.clear()\n",
    "\n",
    "def remove_all_hooks():\n",
    "    global current_hooks\n",
    "    for hook in current_hooks:\n",
    "        hook.remove()\n",
    "    current_hooks.clear()\n",
    "\n",
    "def get_activation_hook(name, model_name):\n",
    "    def hook(module, input, output):\n",
    "        try:\n",
    "            # Handle output\n",
    "            if isinstance(output, tuple):\n",
    "                activation = output[0] if output[0] is not None else None\n",
    "            else:\n",
    "                activation = output\n",
    "            \n",
    "            # Handle input - check for None values\n",
    "            input_tensor = None\n",
    "            if input is not None:\n",
    "                if isinstance(input, tuple) and len(input) > 0:\n",
    "                    input_tensor = input[0] if input[0] is not None else None\n",
    "                else:\n",
    "                    input_tensor = input if input is not None else None\n",
    "            \n",
    "            # Create activation data with None checks\n",
    "            activation_data = {\n",
    "                'output': activation.detach().cpu() if activation is not None else None,\n",
    "                'input': input_tensor.detach().cpu() if input_tensor is not None else None,\n",
    "                'weight': module.weight.detach().cpu() if hasattr(module, 'weight') and module.weight is not None else None,\n",
    "                'bias': module.bias.detach().cpu() if hasattr(module, 'bias') and module.bias is not None else None\n",
    "            }\n",
    "            \n",
    "            if model_name == \"Model_1\":\n",
    "                activations_model_1[name] = activation_data\n",
    "            else:\n",
    "                activations_model_2[name] = activation_data\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error in hook {name}: {e}\")\n",
    "            # Store None data to prevent missing keys\n",
    "            activation_data = {\n",
    "                'output': None,\n",
    "                'input': None, \n",
    "                'weight': None,\n",
    "                'bias': None\n",
    "            }\n",
    "            \n",
    "            if model_name == \"Model_1\":\n",
    "                activations_model_1[name] = activation_data\n",
    "            else:\n",
    "                activations_model_2[name] = activation_data\n",
    "            \n",
    "    return hook\n",
    "\n",
    "def register_llama_hooks(model, model_name, layer_range=None):\n",
    "    global current_hooks\n",
    "    hooks = []\n",
    "    layer_info = {}\n",
    "    \n",
    "    # Determine layer range\n",
    "    if layer_range is None:\n",
    "        layer_range = range(len(model.model.layers))\n",
    "    \n",
    "    for i in layer_range:\n",
    "        if i >= len(model.model.layers):\n",
    "            continue\n",
    "            \n",
    "        layer = model.model.layers[i]\n",
    "        layer_prefix = f\"layer_{i}\"\n",
    "        \n",
    "        # 1. Self-Attention Components\n",
    "        # Query, Key, Value projections\n",
    "        hooks.append(layer.self_attn.q_proj.register_forward_hook(\n",
    "            get_activation_hook(f\"{layer_prefix}_attention_q\", model_name)\n",
    "        ))\n",
    "        hooks.append(layer.self_attn.k_proj.register_forward_hook(\n",
    "            get_activation_hook(f\"{layer_prefix}_attention_k\", model_name)\n",
    "        ))\n",
    "        hooks.append(layer.self_attn.v_proj.register_forward_hook(\n",
    "            get_activation_hook(f\"{layer_prefix}_attention_v\", model_name)\n",
    "        ))\n",
    "        \n",
    "        # Output projection\n",
    "        hooks.append(layer.self_attn.o_proj.register_forward_hook(\n",
    "            get_activation_hook(f\"{layer_prefix}_attention_output\", model_name)\n",
    "        ))\n",
    "        \n",
    "        # 2. MLP Components  \n",
    "        hooks.append(layer.mlp.gate_proj.register_forward_hook(\n",
    "            get_activation_hook(f\"{layer_prefix}_mlp_gate\", model_name)\n",
    "        ))\n",
    "        hooks.append(layer.mlp.up_proj.register_forward_hook(\n",
    "            get_activation_hook(f\"{layer_prefix}_mlp_up\", model_name)\n",
    "        ))\n",
    "        hooks.append(layer.mlp.down_proj.register_forward_hook(\n",
    "            get_activation_hook(f\"{layer_prefix}_mlp_down\", model_name)\n",
    "        ))\n",
    "        \n",
    "        # 3. Layer Norms\n",
    "        hooks.append(layer.input_layernorm.register_forward_hook(\n",
    "            get_activation_hook(f\"{layer_prefix}_input_norm\", model_name)\n",
    "        ))\n",
    "        hooks.append(layer.post_attention_layernorm.register_forward_hook(\n",
    "            get_activation_hook(f\"{layer_prefix}_post_attn_norm\", model_name)\n",
    "        ))\n",
    "        \n",
    "        # Store layer info\n",
    "        layer_info[layer_prefix] = {\n",
    "            'layer_idx': i,\n",
    "            'components': ['attention_q', 'attention_k', 'attention_v', \n",
    "                         'attention_output', 'mlp_gate', 'mlp_up', 'mlp_down',\n",
    "                         'input_norm', 'post_attn_norm']\n",
    "        }\n",
    "    \n",
    "    # Final layer norm and LM head (optional)\n",
    "    hooks.append(model.model.norm.register_forward_hook(\n",
    "        get_activation_hook(\"final_norm\", model_name)\n",
    "    ))\n",
    "    hooks.append(model.lm_head.register_forward_hook(\n",
    "        get_activation_hook(\"lm_head\", model_name)\n",
    "    ))\n",
    "    \n",
    "    current_hooks.extend(hooks)\n",
    "    return hooks, layer_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf4d8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_random_neurons_per_layer(activations, seed=42):\n",
    "    np.random.seed(seed)\n",
    "    selected_neurons = {}\n",
    "    \n",
    "    for layer_name, layer_data in activations.items():\n",
    "        if not isinstance(layer_data, dict):\n",
    "            continue\n",
    "            \n",
    "        activation = layer_data.get('output')\n",
    "        \n",
    "        if activation is None:\n",
    "            print(f\"Skipping {layer_name}: No activation data\")\n",
    "            continue\n",
    "        \n",
    "        # Handle different activation shapes\n",
    "        if len(activation.shape) == 3:  # [batch, seq_len, hidden_size]\n",
    "            batch_size, seq_len, hidden_size = activation.shape\n",
    "            \n",
    "            if hidden_size == 0:\n",
    "                continue\n",
    "            \n",
    "            neuron_idx = np.random.randint(0, hidden_size)\n",
    "            \n",
    "            selected_neurons[layer_name] = {\n",
    "                'neuron_index': neuron_idx,\n",
    "                'sequence_length': seq_len,\n",
    "                'hidden_size': hidden_size,\n",
    "                'activation_shape': activation.shape\n",
    "            }\n",
    "            \n",
    "    return selected_neurons\n",
    "    \n",
    "def get_component_type(layer_name):\n",
    "    if 'attention' in layer_name:\n",
    "        return 'attention'\n",
    "    elif 'mlp' in layer_name:\n",
    "        return 'mlp'\n",
    "    elif 'norm' in layer_name:\n",
    "        return 'normalization'\n",
    "    elif 'lm_head' in layer_name:\n",
    "        return 'output'\n",
    "    else:\n",
    "        return 'other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9d201c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_neuron_outputs(layer_name, neuron_idx, input_tensor,\n",
    "                            weights_1, weights_2, \n",
    "                            actual_output_1, actual_output_2):\n",
    "    results = {\n",
    "        'neuron_index': neuron_idx,\n",
    "        'calculations': [],\n",
    "        'layer_type': get_component_type(layer_name)\n",
    "    }\n",
    "    \n",
    "    # Skip if essential data is missing\n",
    "    if input_tensor is None or weights_1 is None or weights_2 is None:\n",
    "        return results\n",
    "    \n",
    "    # Get weights and biases\n",
    "    w1 = weights_1.get('weight')\n",
    "    w2 = weights_2.get('weight')\n",
    "    b1 = weights_1.get('bias')\n",
    "    b2 = weights_2.get('bias')\n",
    "    \n",
    "    if w1 is None or w2 is None:\n",
    "        return results\n",
    "    \n",
    "    # Handle layer normalization differently (1D weights)\n",
    "    if 'norm' in layer_name:\n",
    "        # Layer norm: output = weight * normalized_input + bias\n",
    "        # For layer norm, we can't select individual neurons the same way\n",
    "        # Instead, we'll look at the scaling factor for the selected dimension\n",
    "        for token_idx in range(input_tensor.shape[1]):\n",
    "            try:\n",
    "                token_input = input_tensor[0, token_idx, :]  # [hidden_size]\n",
    "                \n",
    "                # For layer norm, weight is 1D, so we use it as element-wise multiplication\n",
    "                if neuron_idx < w1.shape[0] and neuron_idx < w2.shape[0]:\n",
    "                    # Get the scaling factor for this dimension\n",
    "                    scale_1 = w1[neuron_idx].item()\n",
    "                    scale_2 = w2[neuron_idx].item()\n",
    "                    \n",
    "                    # Get the normalized input value for this dimension\n",
    "                    input_val = token_input[neuron_idx].item()\n",
    "                    \n",
    "                    # Calculate scaled outputs\n",
    "                    calc_1 = scale_1 * input_val\n",
    "                    calc_2 = scale_2 * input_val\n",
    "                    \n",
    "                    if b1 is not None and neuron_idx < b1.shape[0]:\n",
    "                        calc_1 += b1[neuron_idx].item()\n",
    "                    if b2 is not None and neuron_idx < b2.shape[0]:\n",
    "                        calc_2 += b2[neuron_idx].item()\n",
    "                    \n",
    "                    # Get actual outputs\n",
    "                    actual_1 = actual_output_1[0, token_idx, neuron_idx] if actual_output_1 is not None else None\n",
    "                    actual_2 = actual_output_2[0, token_idx, neuron_idx] if actual_output_2 is not None else None\n",
    "                    \n",
    "                    results['calculations'].append({\n",
    "                        'token_position': token_idx,\n",
    "                        'model_1_calculated': calc_1,\n",
    "                        'model_2_calculated': calc_2,\n",
    "                        'difference': calc_1 - calc_2,\n",
    "                        'model_1_actual': actual_1.item() if actual_1 is not None else None,\n",
    "                        'model_2_actual': actual_2.item() if actual_2 is not None else None,\n",
    "                        'weight_diff': scale_1 - scale_2\n",
    "                    })\n",
    "                    \n",
    "            except Exception as e:\n",
    "                continue\n",
    "    \n",
    "    else:\n",
    "        # Handle regular linear layers (2D weights)\n",
    "        for token_idx in range(input_tensor.shape[1]):\n",
    "            try:\n",
    "                token_input = input_tensor[0, token_idx, :]  # [hidden_size]\n",
    "                \n",
    "                # Check bounds\n",
    "                if neuron_idx >= w1.shape[0] or neuron_idx >= w2.shape[0]:\n",
    "                    continue\n",
    "                \n",
    "                # Model 1 calculation: input @ w1.T + b1\n",
    "                calc_1 = torch.matmul(token_input, w1[neuron_idx, :])\n",
    "                if b1 is not None and neuron_idx < b1.shape[0]:\n",
    "                    calc_1 += b1[neuron_idx]\n",
    "                \n",
    "                # Model 2 calculation: input @ w2.T + b2  \n",
    "                calc_2 = torch.matmul(token_input, w2[neuron_idx, :])\n",
    "                if b2 is not None and neuron_idx < b2.shape[0]:\n",
    "                    calc_2 += b2[neuron_idx]\n",
    "                \n",
    "                # Apply activation function for MLP gate/up projections\n",
    "                if 'mlp_gate' in layer_name or 'mlp_up' in layer_name:\n",
    "                    calc_1 = F.silu(calc_1)\n",
    "                    calc_2 = F.silu(calc_2)\n",
    "                \n",
    "                # Get actual outputs\n",
    "                actual_1 = actual_output_1[0, token_idx, neuron_idx] if actual_output_1 is not None else None\n",
    "                actual_2 = actual_output_2[0, token_idx, neuron_idx] if actual_output_2 is not None else None\n",
    "                \n",
    "                results['calculations'].append({\n",
    "                    'token_position': token_idx,\n",
    "                    'model_1_calculated': calc_1.item(),\n",
    "                    'model_2_calculated': calc_2.item(),\n",
    "                    'difference': (calc_1 - calc_2).item(),\n",
    "                    'model_1_actual': actual_1.item() if actual_1 is not None else None,\n",
    "                    'model_2_actual': actual_2.item() if actual_2 is not None else None\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                continue\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e23253",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_neuron_calculations(model_1_activations, model_2_activations, \n",
    "                                selected_neurons):\n",
    "    comparison_results = {}\n",
    "    \n",
    "    for layer_name, neuron_info in selected_neurons.items():\n",
    "        neuron_idx = neuron_info['neuron_index']\n",
    "        \n",
    "        # Get current layer data\n",
    "        layer_1_data = model_1_activations.get(layer_name, {})\n",
    "        layer_2_data = model_2_activations.get(layer_name, {})\n",
    "        \n",
    "        # Skip if missing data\n",
    "        if not isinstance(layer_1_data, dict) or not isinstance(layer_2_data, dict):\n",
    "            continue\n",
    "            \n",
    "        # Get the input to this layer (from Model 1)\n",
    "        model_1_input = layer_1_data.get('input')\n",
    "        \n",
    "        if model_1_input is None:\n",
    "            continue\n",
    "        \n",
    "        # Get weights from both models\n",
    "        weights_1 = layer_1_data  # Contains weight and bias\n",
    "        weights_2 = layer_2_data  # Contains weight and bias\n",
    "        \n",
    "        # Calculate outputs for the selected neuron\n",
    "        results = calculate_neuron_outputs(\n",
    "            layer_name, neuron_idx, model_1_input,\n",
    "            weights_1, weights_2,\n",
    "            layer_1_data.get('output'), layer_2_data.get('output')\n",
    "        )\n",
    "        \n",
    "        if results and results['calculations']:\n",
    "            comparison_results[layer_name] = results\n",
    "            \n",
    "    return comparison_results\n",
    "\n",
    "def run_comparison(text_input, seed=42):\n",
    "    print(f\"Processing: {text_input[:50]}...\")\n",
    "    \n",
    "    # Clear previous data\n",
    "    clear_activations()\n",
    "    remove_all_hooks()\n",
    "    \n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(text_input, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n",
    "    \n",
    "    print(f\"Input tokens: {inputs['input_ids'].shape[1]}\")\n",
    "    \n",
    "    try:\n",
    "        # Register hooks for all layers\n",
    "        print(\"Registering hooks...\")\n",
    "        hooks_1, layers_1 = register_llama_hooks(model_1, \"Model_1\")\n",
    "        hooks_2, layers_2 = register_llama_hooks(model_2, \"Model_2\")\n",
    "        \n",
    "        # Run both models\n",
    "        print(\"Running models...\")\n",
    "        with torch.no_grad():\n",
    "            outputs_1 = model_1(**inputs)\n",
    "            outputs_2 = model_2(**inputs)\n",
    "        \n",
    "        print(f\"Captured {len(activations_model_1)} activations from Model 1\")\n",
    "        print(f\"Captured {len(activations_model_2)} activations from Model 2\")\n",
    "        \n",
    "        # Select random neurons (one per layer)\n",
    "        print(\"Selecting random neurons...\")\n",
    "        selected_neurons = select_random_neurons_per_layer(activations_model_1, seed=seed)\n",
    "        \n",
    "        print(f\"Selected neurons from {len(selected_neurons)} layers\")\n",
    "        \n",
    "        # Compare activations\n",
    "        print(\"Comparing activations...\")\n",
    "        comparison_results = compare_neuron_calculations(\n",
    "            activations_model_1,\n",
    "            activations_model_2,\n",
    "            selected_neurons\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_text': text_input,\n",
    "            'tokenized_input': inputs,\n",
    "            'model_1_output': outputs_1.logits,\n",
    "            'model_2_output': outputs_2.logits,\n",
    "            'layer_comparisons': comparison_results,\n",
    "            'selected_neurons': selected_neurons\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in run_comparison: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return {\n",
    "            'input_text': text_input,\n",
    "            'error': str(e),\n",
    "            'layer_comparisons': {},\n",
    "            'selected_neurons': {}\n",
    "        }\n",
    "    \n",
    "    finally:\n",
    "        # Always cleanup hooks\n",
    "        remove_all_hooks()\n",
    "\n",
    "def save_detailed_results(comparison_results, filename=\"detailed_activation_comparison.csv\"):\n",
    "    rows = []\n",
    "    \n",
    "    for layer_name, layer_data in comparison_results['layer_comparisons'].items():\n",
    "        if 'calculations' in layer_data:\n",
    "            for calc in layer_data['calculations']:\n",
    "                row = {\n",
    "                    'input_text': comparison_results['input_text'][:100],\n",
    "                    'layer_name': layer_name,\n",
    "                    'layer_type': layer_data['layer_type'],\n",
    "                    'neuron_index': layer_data['neuron_index'],\n",
    "                    'token_position': calc['token_position'],\n",
    "                    'model_1_calculated': calc['model_1_calculated'],\n",
    "                    'model_2_calculated': calc['model_2_calculated'],\n",
    "                    'difference': calc['difference'],\n",
    "                    'abs_difference': abs(calc['difference']),\n",
    "                    'model_1_actual': calc.get('model_1_actual'),\n",
    "                    'model_2_actual': calc.get('model_2_actual')\n",
    "                }\n",
    "                rows.append(row)\n",
    "    \n",
    "    df = pd.DataFrame(rows)\n",
    "    \n",
    "    # Save to CSV\n",
    "    if os.path.exists(filename):\n",
    "        df.to_csv(filename, mode='a', header=False, index=False)\n",
    "    else:\n",
    "        df.to_csv(filename, index=False)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbfda25",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_TEXTS = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Artificial intelligence is transforming the world of technology.\",\n",
    "    \"In a hole in the ground there lived a hobbit.\",\n",
    "    \"To be or not to be, that is the question Shakespeare posed.\",\n",
    "    \"Machine learning models require large datasets for training.\",\n",
    "    \"The mitochondria is the powerhouse of the cell in biology.\",\n",
    "    \"Climate change is causing unprecedented shifts in global weather patterns.\",\n",
    "    \"Mozart composed his first symphony at the age of eight years old.\",\n",
    "    \"The stock market experienced significant volatility during the pandemic crisis.\",\n",
    "    \"Quantum physics reveals the strange behavior of particles at subatomic levels.\",\n",
    "    \"Professional chefs recommend using fresh herbs to enhance flavor profiles.\",\n",
    "    \"Ancient Egyptian pyramids were built using sophisticated engineering techniques.\",\n",
    "    \"Regular exercise and proper nutrition are essential for maintaining good health.\",\n",
    "    \"The International Space Station orbits Earth approximately every ninety minutes.\",\n",
    "    \"Cryptocurrency markets operate twenty-four hours a day across global exchanges.\",\n",
    "    \"Vincent van Gogh painted Starry Night while staying at an asylum.\",\n",
    "    \"Professional athletes must maintain strict training regimens throughout their careers.\",\n",
    "    \"The Amazon rainforest produces twenty percent of the world's oxygen supply.\",\n",
    "    \"Modern architecture emphasizes clean lines and functional design principles.\",\n",
    "    \"Forensic scientists use DNA analysis to solve complex criminal investigations.\",\n",
    "    \"Traditional Japanese tea ceremonies follow centuries-old ritualistic practices.\",\n",
    "    \"Marine biologists study coral reef ecosystems threatened by ocean acidification.\",\n",
    "    \"The Renaissance period marked a cultural rebirth in European art and science.\",\n",
    "    \"Cybersecurity experts work tirelessly to protect digital infrastructure from threats.\",\n",
    "    \"Sustainable agriculture practices help preserve soil quality for future generations.\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb31afe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing text 1/5 ===\n",
      "Processing: The quick brown fox jumps over the lazy dog....\n",
      "Input tokens: 13\n",
      "Registering hooks...\n",
      "Running models...\n",
      "Captured 290 activations from Model 1\n",
      "Captured 290 activations from Model 2\n",
      "Selecting random neurons...\n",
      "Selected neurons from 290 layers\n",
      "Comparing activations...\n",
      "Visualization saved to neuron_comparison_text_0.png\n",
      "Completed text 1\n",
      "\n",
      "=== Processing text 2/5 ===\n",
      "Processing: Artificial intelligence is transforming the world ...\n",
      "Input tokens: 13\n",
      "Registering hooks...\n",
      "Running models...\n",
      "Captured 290 activations from Model 1\n",
      "Captured 290 activations from Model 2\n",
      "Selecting random neurons...\n",
      "Selected neurons from 290 layers\n",
      "Comparing activations...\n",
      "Visualization saved to neuron_comparison_text_1.png\n",
      "Completed text 2\n",
      "\n",
      "=== Processing text 3/5 ===\n",
      "Processing: In a hole in the ground there lived a hobbit....\n",
      "Input tokens: 14\n",
      "Registering hooks...\n",
      "Running models...\n",
      "Captured 290 activations from Model 1\n",
      "Captured 290 activations from Model 2\n",
      "Selecting random neurons...\n",
      "Selected neurons from 290 layers\n",
      "Comparing activations...\n",
      "Visualization saved to neuron_comparison_text_2.png\n",
      "Completed text 3\n",
      "\n",
      "=== Processing text 4/5 ===\n",
      "Processing: To be or not to be, that is the question Shakespea...\n",
      "Input tokens: 16\n",
      "Registering hooks...\n",
      "Running models...\n",
      "Captured 290 activations from Model 1\n",
      "Captured 290 activations from Model 2\n",
      "Selecting random neurons...\n",
      "Selected neurons from 290 layers\n",
      "Comparing activations...\n",
      "Visualization saved to neuron_comparison_text_3.png\n",
      "Completed text 4\n",
      "\n",
      "=== Processing text 5/5 ===\n",
      "Processing: Machine learning models require large datasets for...\n",
      "Input tokens: 10\n",
      "Registering hooks...\n",
      "Running models...\n",
      "Captured 290 activations from Model 1\n",
      "Captured 290 activations from Model 2\n",
      "Selecting random neurons...\n",
      "Selected neurons from 290 layers\n",
      "Comparing activations...\n",
      "Visualization saved to neuron_comparison_text_4.png\n",
      "Completed text 5\n"
     ]
    }
   ],
   "source": [
    "all_results = []\n",
    "\n",
    "for i, text in enumerate(TEST_TEXTS):\n",
    "    print(f\"\\n=== Processing text {i+1}/{len(TEST_TEXTS)} ===\")\n",
    "    \n",
    "    try:\n",
    "        # Use different seed for each text to get variety\n",
    "        result = run_comparison(text, seed=42+i)\n",
    "        \n",
    "        all_results.append(result)\n",
    "        \n",
    "        # Save detailed results\n",
    "        save_detailed_results(result, filename=\"all_layers_activation_comparison.csv\")\n",
    "        \n",
    "        print(f\"Completed text {i+1}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing text {i+1}: {e}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d90a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary visualization saved to activation_summary_all_layers.png\n",
      "\n",
      "=== SUMMARY STATISTICS (ALL LAYERS) ===\n",
      "\n",
      "Top 10 layers by average difference:\n",
      "  lm_head: 0.910089 Â± 0.295044\n",
      "  layer_31_mlp_down: 0.101140 Â± 0.032076\n",
      "  layer_30_attention_v: 0.073980 Â± 0.050429\n",
      "  layer_30_mlp_down: 0.072233 Â± 0.035467\n",
      "  layer_28_attention_k: 0.069618 Â± 0.032996\n",
      "  layer_31_attention_k: 0.068672 Â± 0.016603\n",
      "  layer_25_attention_q: 0.061632 Â± 0.036315\n",
      "  layer_31_attention_q: 0.059968 Â± 0.030755\n",
      "  layer_24_attention_q: 0.055806 Â± 0.027219\n",
      "  layer_26_attention_q: 0.052486 Â± 0.014143\n",
      "\n",
      "Average differences by component type:\n",
      "  normalization: 0.001776 Â± 0.002798\n",
      "  attention: 0.026334 Â± 0.021456\n",
      "  mlp: 0.011337 Â± 0.014316\n",
      "  output: 0.910089 Â± 0.295044\n",
      "\n",
      "Overall statistics:\n",
      "  Total comparisons: 19140\n",
      "  Mean absolute difference: 0.019002\n",
      "  Std deviation: 0.071256\n",
      "  Max difference: 3.069078\n",
      "  Min difference: 0.000000\n",
      "  Median difference: 0.005303\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Print summary statistics\n",
    "if all_results:\n",
    "    print(\"\\n=== SUMMARY STATISTICS (ALL LAYERS) ===\")\n",
    "    \n",
    "    all_layer_stats = defaultdict(list)\n",
    "    all_component_stats = defaultdict(list)\n",
    "    \n",
    "    for result in all_results:\n",
    "        for layer_name, layer_data in result['layer_comparisons'].items():\n",
    "            if 'calculations' in layer_data:\n",
    "                diffs = [abs(calc['difference']) for calc in layer_data['calculations']]\n",
    "                if diffs:\n",
    "                    mean_diff = np.mean(diffs)\n",
    "                    all_layer_stats[layer_name].append(mean_diff)\n",
    "                    all_component_stats[layer_data['layer_type']].append(mean_diff)\n",
    "    \n",
    "    print(\"\\nTop 10 layers by average difference:\")\n",
    "    layer_avg_diffs = [(layer, np.mean(diffs)) for layer, diffs in all_layer_stats.items()]\n",
    "    layer_avg_diffs.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    for layer, avg_diff in layer_avg_diffs[:10]:\n",
    "        std_diff = np.std(all_layer_stats[layer])\n",
    "        print(f\"  {layer}: {avg_diff:.6f} Â± {std_diff:.6f}\")\n",
    "    \n",
    "    print(\"\\nAverage differences by component type:\")\n",
    "    for component, diffs in all_component_stats.items():\n",
    "        print(f\"  {component}: {np.mean(diffs):.6f} Â± {np.std(diffs):.6f}\")\n",
    "    \n",
    "    # Calculate overall statistics\n",
    "    all_differences = []\n",
    "    for result in all_results:\n",
    "        for layer_name, layer_data in result['layer_comparisons'].items():\n",
    "            if 'calculations' in layer_data:\n",
    "                all_differences.extend([abs(calc['difference']) \n",
    "                                      for calc in layer_data['calculations']])\n",
    "    \n",
    "    if all_differences:\n",
    "        print(f\"\\nOverall statistics:\")\n",
    "        print(f\"  Total comparisons: {len(all_differences)}\")\n",
    "        print(f\"  Mean absolute difference: {np.mean(all_differences):.6f}\")\n",
    "        print(f\"  Std deviation: {np.std(all_differences):.6f}\")\n",
    "        print(f\"  Max difference: {np.max(all_differences):.6f}\")\n",
    "        print(f\"  Min difference: {np.min(all_differences):.6f}\")\n",
    "        print(f\"  Median difference: {np.median(all_differences):.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa40340e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "RUNNING ANALYSIS WITH DIFFERENCE TRACKING\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "Processing Query 1/5\n",
      "Text: The quick brown fox jumps over the lazy dog....\n",
      "======================================================================\n",
      "Processing: The quick brown fox jumps over the lazy dog....\n",
      "Input tokens: 13\n",
      "Registering hooks...\n",
      "Running models...\n",
      "Captured 290 activations from Model 1\n",
      "Captured 290 activations from Model 2\n",
      "Selecting random neurons...\n",
      "Selected neurons from 290 layers\n",
      "Comparing activations...\n",
      "\n",
      "Input text: The quick brown fox jumps over the lazy dog.\n",
      "\n",
      "============================================================\n",
      "Model 1 Predictions            | Model 2 Predictions           \n",
      "============================================================\n",
      "1. '\n",
      "' (0.456)                    | 1. '\n",
      "' (0.302)\n",
      "2. 'The' (0.043)                  | 2. 'The' (0.107)\n",
      "3. '' (0.039)                     | 3. '' (0.030)\n",
      "4. 'This' (0.023)                 | 4. 'This' (0.028)\n",
      "5. 'It' (0.016)                   | 5. 'I' (0.020)\n",
      "\n",
      "KL Divergence (Model1 || Model2): 0.3582\n",
      "\n",
      "==================================================\n",
      "QUERY 1 DIFFERENCE SUMMARY\n",
      "==================================================\n",
      "Total Absolute Difference: 69.58\n",
      "Number of Layers Analyzed: 290\n",
      "Number of Tokens: 13\n",
      "Average Difference per Layer: 0.24\n",
      "Average Difference per Token: 5.35\n",
      "\n",
      "Top 5 Layers with Highest Differences:\n",
      "  lm_head: 10.31\n",
      "  layer_31_mlp_down: 1.49\n",
      "  layer_30_attention_v: 1.25\n",
      "  layer_31_attention_k: 1.24\n",
      "  layer_24_attention_q: 1.16\n",
      "\n",
      "Completed Query 1\n",
      "\n",
      "======================================================================\n",
      "Processing Query 2/5\n",
      "Text: Artificial intelligence is transforming the world of technol...\n",
      "======================================================================\n",
      "Processing: Artificial intelligence is transforming the world ...\n",
      "Input tokens: 13\n",
      "Registering hooks...\n",
      "Running models...\n",
      "Captured 290 activations from Model 1\n",
      "Captured 290 activations from Model 2\n",
      "Selecting random neurons...\n",
      "Selected neurons from 290 layers\n",
      "Comparing activations...\n",
      "\n",
      "Input text: Artificial intelligence is transforming the world of technology.\n",
      "\n",
      "============================================================\n",
      "Model 1 Predictions            | Model 2 Predictions           \n",
      "============================================================\n",
      "1. 'Here' (0.643)                 | 1. 'It' (0.094)\n",
      "2. 'It' (0.056)                   | 2. 'The' (0.072)\n",
      "3. 'However' (0.034)              | 3. '\n",
      "' (0.065)\n",
      "4. 'In' (0.033)                   | 4. 'Art' (0.051)\n",
      "5. 'With' (0.031)                 | 5. 'In' (0.049)\n",
      "\n",
      "KL Divergence (Model1 || Model2): 2.3150\n",
      "\n",
      "==================================================\n",
      "QUERY 2 DIFFERENCE SUMMARY\n",
      "==================================================\n",
      "Total Absolute Difference: 73.78\n",
      "Number of Layers Analyzed: 290\n",
      "Number of Tokens: 13\n",
      "Average Difference per Layer: 0.25\n",
      "Average Difference per Token: 5.68\n",
      "\n",
      "Top 5 Layers with Highest Differences:\n",
      "  lm_head: 11.44\n",
      "  layer_18_attention_q: 1.46\n",
      "  layer_31_mlp_down: 1.40\n",
      "  layer_25_attention_q: 1.36\n",
      "  layer_30_mlp_down: 1.06\n",
      "\n",
      "Completed Query 2\n",
      "\n",
      "======================================================================\n",
      "Processing Query 3/5\n",
      "Text: In a hole in the ground there lived a hobbit....\n",
      "======================================================================\n",
      "Processing: In a hole in the ground there lived a hobbit....\n",
      "Input tokens: 14\n",
      "Registering hooks...\n",
      "Running models...\n",
      "Captured 290 activations from Model 1\n",
      "Captured 290 activations from Model 2\n",
      "Selecting random neurons...\n",
      "Selected neurons from 290 layers\n",
      "Comparing activations...\n",
      "\n",
      "Input text: In a hole in the ground there lived a hobbit.\n",
      "\n",
      "============================================================\n",
      "Model 1 Predictions            | Model 2 Predictions           \n",
      "============================================================\n",
      "1. '\n",
      "' (0.182)                    | 1. '\n",
      "' (0.153)\n",
      "2. 'Not' (0.182)                  | 2. 'Tol' (0.075)\n",
      "3. 'Tol' (0.046)                  | 3. 'Hob' (0.052)\n",
      "4. 'Not' (0.045)                  | 4. 'A' (0.036)\n",
      "5. 'There' (0.043)                | 5. 'He' (0.030)\n",
      "\n",
      "KL Divergence (Model1 || Model2): 0.8663\n",
      "\n",
      "==================================================\n",
      "QUERY 3 DIFFERENCE SUMMARY\n",
      "==================================================\n",
      "Total Absolute Difference: 70.06\n",
      "Number of Layers Analyzed: 290\n",
      "Number of Tokens: 14\n",
      "Average Difference per Layer: 0.24\n",
      "Average Difference per Token: 5.00\n",
      "\n",
      "Top 5 Layers with Highest Differences:\n",
      "  lm_head: 7.44\n",
      "  layer_30_attention_v: 2.27\n",
      "  layer_31_mlp_down: 1.94\n",
      "  layer_27_attention_q: 1.20\n",
      "  layer_24_attention_q: 1.18\n",
      "\n",
      "Completed Query 3\n",
      "\n",
      "======================================================================\n",
      "Processing Query 4/5\n",
      "Text: To be or not to be, that is the question Shakespeare posed....\n",
      "======================================================================\n",
      "Processing: To be or not to be, that is the question Shakespea...\n",
      "Input tokens: 16\n",
      "Registering hooks...\n",
      "Running models...\n",
      "Captured 290 activations from Model 1\n",
      "Captured 290 activations from Model 2\n",
      "Selecting random neurons...\n",
      "Selected neurons from 290 layers\n",
      "Comparing activations...\n",
      "\n",
      "Input text: To be or not to be, that is the question Shakespeare posed.\n",
      "\n",
      "============================================================\n",
      "Model 1 Predictions            | Model 2 Predictions           \n",
      "============================================================\n",
      "1. '\n",
      "' (0.192)                    | 1. '\n",
      "' (0.246)\n",
      "2. 'Here' (0.109)                 | 2. 'The' (0.061)\n",
      "3. 'In' (0.065)                   | 3. 'It' (0.045)\n",
      "4. 'But' (0.057)                  | 4. 'In' (0.038)\n",
      "5. 'Is' (0.052)                   | 5. 'And' (0.026)\n",
      "\n",
      "KL Divergence (Model1 || Model2): 0.5378\n",
      "\n",
      "==================================================\n",
      "QUERY 4 DIFFERENCE SUMMARY\n",
      "==================================================\n",
      "Total Absolute Difference: 96.96\n",
      "Number of Layers Analyzed: 290\n",
      "Number of Tokens: 16\n",
      "Average Difference per Layer: 0.33\n",
      "Average Difference per Token: 6.06\n",
      "\n",
      "Top 5 Layers with Highest Differences:\n",
      "  lm_head: 22.99\n",
      "  layer_31_attention_q: 1.84\n",
      "  layer_31_mlp_down: 1.65\n",
      "  layer_28_attention_q: 1.45\n",
      "  layer_22_attention_q: 1.31\n",
      "\n",
      "Completed Query 4\n",
      "\n",
      "======================================================================\n",
      "Processing Query 5/5\n",
      "Text: Machine learning models require large datasets for training....\n",
      "======================================================================\n",
      "Processing: Machine learning models require large datasets for...\n",
      "Input tokens: 10\n",
      "Registering hooks...\n",
      "Running models...\n",
      "Captured 290 activations from Model 1\n",
      "Captured 290 activations from Model 2\n",
      "Selecting random neurons...\n",
      "Selected neurons from 290 layers\n",
      "Comparing activations...\n",
      "\n",
      "Input text: Machine learning models require large datasets for training.\n",
      "\n",
      "============================================================\n",
      "Model 1 Predictions            | Model 2 Predictions           \n",
      "============================================================\n",
      "1. 'Here' (0.237)                 | 1. 'Machine' (0.170)\n",
      "2. 'In' (0.105)                   | 2. 'They' (0.069)\n",
      "3. 'However' (0.079)              | 3. 'The' (0.059)\n",
      "4. 'This' (0.074)                 | 4. 'This' (0.050)\n",
      "5. 'They' (0.071)                 | 5. '\n",
      "' (0.046)\n",
      "\n",
      "KL Divergence (Model1 || Model2): 1.1488\n",
      "\n",
      "==================================================\n",
      "QUERY 5 DIFFERENCE SUMMARY\n",
      "==================================================\n",
      "Total Absolute Difference: 53.32\n",
      "Number of Layers Analyzed: 290\n",
      "Number of Tokens: 10\n",
      "Average Difference per Layer: 0.18\n",
      "Average Difference per Token: 5.33\n",
      "\n",
      "Top 5 Layers with Highest Differences:\n",
      "  lm_head: 9.09\n",
      "  layer_30_mlp_down: 1.33\n",
      "  layer_28_attention_k: 1.30\n",
      "  layer_25_attention_q: 1.01\n",
      "  layer_18_attention_k: 0.85\n",
      "\n",
      "Completed Query 5\n",
      "Difference summary visualization saved to difference_summary.png\n",
      "\n",
      "======================================================================\n",
      "FINAL SUMMARY TABLE WITH DIFFERENCES\n",
      "======================================================================\n",
      " Query                                        Text Total_Diff Avg_Layer_Diff KL_Div Top1_Match\n",
      "     1 The quick brown fox jumps over the lazy ...      69.58           0.24 0.3582          âœ“\n",
      "     2 Artificial intelligence is transforming ...      73.78           0.25 2.3150          âœ—\n",
      "     3 In a hole in the ground there lived a ho...      70.06           0.24 0.8663          âœ“\n",
      "     4 To be or not to be, that is the question...      96.96           0.33 0.5378          âœ“\n",
      "     5 Machine learning models require large da...      53.32           0.18 1.1488          âœ—\n",
      "\n",
      "Correlation plot saved to activation_output_correlation.png\n",
      "Correlation coefficient: -0.214\n",
      "\n",
      "======================================================================\n",
      "ANALYSIS COMPLETE\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "def calculate_total_differences(result):\n",
    "    total_diff = 0\n",
    "    layer_diffs = {}\n",
    "    token_diffs = {}\n",
    "    \n",
    "    for layer_name, layer_data in result['layer_comparisons'].items():\n",
    "        if 'calculations' not in layer_data or not layer_data['calculations']:\n",
    "            continue\n",
    "        \n",
    "        layer_sum = 0\n",
    "        for calc in layer_data['calculations']:\n",
    "            diff = abs(calc['difference'])\n",
    "            layer_sum += diff\n",
    "            \n",
    "            # Track per-token differences\n",
    "            token_pos = calc['token_position']\n",
    "            if token_pos not in token_diffs:\n",
    "                token_diffs[token_pos] = 0\n",
    "            token_diffs[token_pos] += diff\n",
    "        \n",
    "        layer_diffs[layer_name] = layer_sum\n",
    "        total_diff += layer_sum\n",
    "    \n",
    "    return {\n",
    "        'total_difference': total_diff,\n",
    "        'layer_differences': layer_diffs,\n",
    "        'token_differences': token_diffs,\n",
    "        'num_layers': len(layer_diffs),\n",
    "        'num_tokens': len(token_diffs)\n",
    "    }\n",
    "\n",
    "def decode_and_compare_outputs(result, tokenizer, top_k=5):\n",
    "    input_ids = result['tokenized_input']['input_ids']\n",
    "    logits_1 = result['model_1_output']\n",
    "    logits_2 = result['model_2_output']\n",
    "    \n",
    "    # Get predictions for the last token (next token prediction)\n",
    "    last_token_logits_1 = logits_1[0, -1, :]  # [vocab_size]\n",
    "    last_token_logits_2 = logits_2[0, -1, :]  # [vocab_size]\n",
    "    \n",
    "    # Get top-k predictions\n",
    "    probs_1 = torch.softmax(last_token_logits_1, dim=-1)\n",
    "    probs_2 = torch.softmax(last_token_logits_2, dim=-1)\n",
    "    \n",
    "    top_probs_1, top_indices_1 = torch.topk(probs_1, top_k)\n",
    "    top_probs_2, top_indices_2 = torch.topk(probs_2, top_k)\n",
    "    \n",
    "    # Decode tokens\n",
    "    input_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(f\"\\nInput text: {input_text}\")\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"{'Model 1 Predictions':<30} | {'Model 2 Predictions':<30}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    for i in range(top_k):\n",
    "        token_1 = tokenizer.decode(top_indices_1[i])\n",
    "        prob_1 = top_probs_1[i].item()\n",
    "        \n",
    "        token_2 = tokenizer.decode(top_indices_2[i])\n",
    "        prob_2 = top_probs_2[i].item()\n",
    "        \n",
    "        print(f\"{i+1}. '{token_1}' ({prob_1:.3f}){' '*(20-len(token_1))} | \"\n",
    "              f\"{i+1}. '{token_2}' ({prob_2:.3f})\")\n",
    "    \n",
    "    # Calculate Jensen-Shannon divergence between distributions\n",
    "    def jensen_shannon_divergence(p, q):\n",
    "        \"\"\"Calculate Jensen-Shannon divergence between two probability distributions.\"\"\"\n",
    "        # Add small epsilon for numerical stability\n",
    "        p = p + 1e-10\n",
    "        q = q + 1e-10\n",
    "        \n",
    "        # Calculate the average distribution M = (P + Q) / 2\n",
    "        m = (p + q) / 2\n",
    "        \n",
    "        # Calculate KL divergences: KL(P||M) and KL(Q||M)\n",
    "        kl_pm = torch.nn.functional.kl_div(torch.log(m), p, reduction='sum')\n",
    "        kl_qm = torch.nn.functional.kl_div(torch.log(m), q, reduction='sum')\n",
    "        \n",
    "        # Jensen-Shannon divergence = (KL(P||M) + KL(Q||M)) / 2\n",
    "        js_div = (kl_pm + kl_qm) / 2\n",
    "        \n",
    "        return js_div.item()\n",
    "    \n",
    "    js_div = jensen_shannon_divergence(probs_1, probs_2)\n",
    "    \n",
    "    print(f\"\\nJensen-Shannon Divergence: {js_div:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'top_tokens_model_1': [tokenizer.decode(idx) for idx in top_indices_1],\n",
    "        'top_probs_model_1': top_probs_1.tolist(),\n",
    "        'top_tokens_model_2': [tokenizer.decode(idx) for idx in top_indices_2],\n",
    "        'top_probs_model_2': top_probs_2.tolist(),\n",
    "        'jensen_shannon_divergence': js_div\n",
    "    }\n",
    "\n",
    "def visualize_difference_summary(all_results, save_path='difference_summary.png'):\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    fig.suptitle('Sum of Activation Differences Analysis', fontsize=16)\n",
    "    \n",
    "    # 1. Total differences per query\n",
    "    ax = axes[0, 0]\n",
    "    total_diffs = []\n",
    "    query_labels = []\n",
    "    \n",
    "    for i, result in enumerate(all_results):\n",
    "        diff_analysis = calculate_total_differences(result)\n",
    "        total_diffs.append(diff_analysis['total_difference'])\n",
    "        query_labels.append(f\"Query {i+1}\")\n",
    "    \n",
    "    bars = ax.bar(query_labels, total_diffs, color='darkblue', alpha=0.7)\n",
    "    ax.set_ylabel('Total Absolute Difference')\n",
    "    ax.set_title('Total Activation Differences per Query')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, value in zip(bars, total_diffs):\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{value:.1f}', ha='center', va='bottom')\n",
    "    \n",
    "    # 2. Average difference per layer\n",
    "    ax = axes[0, 1]\n",
    "    avg_diffs_per_layer = []\n",
    "    \n",
    "    for result in all_results:\n",
    "        diff_analysis = calculate_total_differences(result)\n",
    "        if diff_analysis['num_layers'] > 0:\n",
    "            avg_diff = diff_analysis['total_difference'] / diff_analysis['num_layers']\n",
    "            avg_diffs_per_layer.append(avg_diff)\n",
    "    \n",
    "    ax.plot(range(len(avg_diffs_per_layer)), avg_diffs_per_layer, 'ro-', markersize=8)\n",
    "    ax.set_xlabel('Query Index')\n",
    "    ax.set_ylabel('Average Difference per Layer')\n",
    "    ax.set_title('Average Layer Difference by Query')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Difference distribution across layers (for first query)\n",
    "    ax = axes[0, 2]\n",
    "    if all_results:\n",
    "        first_result = all_results[0]\n",
    "        diff_analysis = calculate_total_differences(first_result)\n",
    "        \n",
    "        # Get layer types and their differences\n",
    "        layer_types = {'norm': 0, 'mlp': 0, 'attn': 0, 'other': 0}\n",
    "        for layer_name, diff in diff_analysis['layer_differences'].items():\n",
    "            if 'norm' in layer_name:\n",
    "                layer_types['norm'] += diff\n",
    "            elif 'mlp' in layer_name:\n",
    "                layer_types['mlp'] += diff\n",
    "            elif 'attn' in layer_name:\n",
    "                layer_types['attn'] += diff\n",
    "            else:\n",
    "                layer_types['other'] += diff\n",
    "        \n",
    "        # Create pie chart\n",
    "        non_zero_types = {k: v for k, v in layer_types.items() if v > 0}\n",
    "        if non_zero_types:\n",
    "            ax.pie(non_zero_types.values(), labels=non_zero_types.keys(), \n",
    "                   autopct='%1.1f%%', startangle=90)\n",
    "            ax.set_title('Difference Distribution by Layer Type\\n(Query 1)')\n",
    "    \n",
    "    # 4. Token-wise differences (averaged across queries)\n",
    "    ax = axes[1, 0]\n",
    "    max_tokens = max(len(calculate_total_differences(r)['token_differences']) \n",
    "                     for r in all_results)\n",
    "    \n",
    "    avg_token_diffs = []\n",
    "    for token_pos in range(max_tokens):\n",
    "        token_sum = 0\n",
    "        count = 0\n",
    "        for result in all_results:\n",
    "            diff_analysis = calculate_total_differences(result)\n",
    "            if token_pos in diff_analysis['token_differences']:\n",
    "                token_sum += diff_analysis['token_differences'][token_pos]\n",
    "                count += 1\n",
    "        if count > 0:\n",
    "            avg_token_diffs.append(token_sum / count)\n",
    "        else:\n",
    "            avg_token_diffs.append(0)\n",
    "    \n",
    "    ax.bar(range(len(avg_token_diffs)), avg_token_diffs, color='green', alpha=0.7)\n",
    "    ax.set_xlabel('Token Position')\n",
    "    ax.set_ylabel('Average Absolute Difference')\n",
    "    ax.set_title('Average Difference by Token Position')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 5. Cumulative differences\n",
    "    ax = axes[1, 1]\n",
    "    for i, result in enumerate(all_results):\n",
    "        diff_analysis = calculate_total_differences(result)\n",
    "        \n",
    "        # Sort layers by name for consistent ordering\n",
    "        sorted_layers = sorted(diff_analysis['layer_differences'].items())\n",
    "        layer_names = [l[0] for l in sorted_layers]\n",
    "        layer_diffs = [l[1] for l in sorted_layers]\n",
    "        \n",
    "        # Calculate cumulative sum\n",
    "        cumulative = np.cumsum(layer_diffs)\n",
    "        \n",
    "        # Plot every 10th layer to avoid overcrowding\n",
    "        x_points = list(range(0, len(cumulative), 10))\n",
    "        y_points = [cumulative[i] for i in x_points]\n",
    "        \n",
    "        ax.plot(x_points, y_points, '-o', label=f'Query {i+1}', \n",
    "                markersize=4, alpha=0.7)\n",
    "    \n",
    "    ax.set_xlabel('Layer Index (every 10th)')\n",
    "    ax.set_ylabel('Cumulative Difference')\n",
    "    ax.set_title('Cumulative Differences Across Layers')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 6. Summary statistics table\n",
    "    ax = axes[1, 2]\n",
    "    ax.axis('off')\n",
    "    \n",
    "    summary_text = \"Summary Statistics\\n\" + \"=\"*30 + \"\\n\\n\"\n",
    "    \n",
    "    for i, result in enumerate(all_results[:5]):  # Show first 5\n",
    "        diff_analysis = calculate_total_differences(result)\n",
    "        text_preview = result['input_text'][:30] + \"...\"\n",
    "        \n",
    "        summary_text += f\"Query {i+1}: {text_preview}\\n\"\n",
    "        summary_text += f\"  Total Diff: {diff_analysis['total_difference']:.2f}\\n\"\n",
    "        summary_text += f\"  Layers: {diff_analysis['num_layers']}\\n\"\n",
    "        summary_text += f\"  Avg/Layer: {diff_analysis['total_difference']/diff_analysis['num_layers']:.2f}\\n\\n\"\n",
    "    \n",
    "    ax.text(0.05, 0.95, summary_text, transform=ax.transAxes,\n",
    "            fontsize=9, verticalalignment='top', fontfamily='monospace')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Difference summary visualization saved to {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c571d7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the analysis with difference tracking\n",
    "all_results = []\n",
    "difference_summaries = []\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RUNNING ANALYSIS WITH DIFFERENCE TRACKING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for i, text in enumerate(TEST_TEXTS):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Processing Query {i+1}/{len(TEST_TEXTS)}\")\n",
    "    print(f\"Text: {text[:60]}...\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    try:\n",
    "        # Run comparison\n",
    "        result = run_comparison(text, seed=42+i)\n",
    "        \n",
    "        # Calculate differences\n",
    "        diff_analysis = calculate_total_differences(result)\n",
    "        \n",
    "        # Add output comparison\n",
    "        output_comp = decode_and_compare_outputs(result, tokenizer, top_k=5)\n",
    "        result['output_comparison'] = output_comp\n",
    "        result['difference_analysis'] = diff_analysis\n",
    "        \n",
    "        all_results.append(result)\n",
    "        difference_summaries.append(diff_analysis)\n",
    "        \n",
    "        # Print summary for this query\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"QUERY {i+1} DIFFERENCE SUMMARY\")\n",
    "        print(f\"{'='*50}\")\n",
    "        print(f\"Total Absolute Difference: {diff_analysis['total_difference']:.2f}\")\n",
    "        print(f\"Number of Layers Analyzed: {diff_analysis['num_layers']}\")\n",
    "        print(f\"Number of Tokens: {diff_analysis['num_tokens']}\")\n",
    "        print(f\"Average Difference per Layer: {diff_analysis['total_difference']/diff_analysis['num_layers']:.2f}\")\n",
    "        print(f\"Average Difference per Token: {diff_analysis['total_difference']/diff_analysis['num_tokens']:.2f}\")\n",
    "        \n",
    "        # Show top 5 layers with highest differences\n",
    "        sorted_layers = sorted(diff_analysis['layer_differences'].items(), \n",
    "                             key=lambda x: x[1], reverse=True)[:5]\n",
    "        print(f\"\\nTop 5 Layers with Highest Differences:\")\n",
    "        for layer_name, diff in sorted_layers:\n",
    "            print(f\"  {layer_name}: {diff:.2f}\")\n",
    "        \n",
    "        print(f\"\\nCompleted Query {i+1}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing query {i+1}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        continue\n",
    "\n",
    "\n",
    "# Create visualizations\n",
    "if all_results:\n",
    "    visualize_difference_summary(all_results, save_path='difference_summary.png')\n",
    "\n",
    "\n",
    "# Create a detailed comparison table with differences\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL SUMMARY TABLE WITH DIFFERENCES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "summary_data = []\n",
    "\n",
    "for i, result in enumerate(all_results):\n",
    "    if 'difference_analysis' in result and 'output_comparison' in result:\n",
    "        diff = result['difference_analysis']\n",
    "        out = result['output_comparison']\n",
    "        \n",
    "        summary_data.append({\n",
    "            'Query': i+1,\n",
    "            'Text': result['input_text'][:40] + '...',\n",
    "            'Total_Diff': f\"{diff['total_difference']:.2f}\",\n",
    "            'Avg_Layer_Diff': f\"{diff['total_difference']/diff['num_layers']:.2f}\",\n",
    "            'KL_Div': f\"{out['kl_divergence']:.4f}\",\n",
    "            'Top1_Match': 'âœ“' if out['top_tokens_model_1'][0] == out['top_tokens_model_2'][0] else 'âœ—'\n",
    "        })\n",
    "\n",
    "df_summary = pd.DataFrame(summary_data)\n",
    "print(df_summary.to_string(index=False))\n",
    "\n",
    "# Plot correlation between activation differences and output differences\n",
    "if len(all_results) > 1:\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "    \n",
    "    activation_diffs = [r['difference_analysis']['total_difference'] for r in all_results \n",
    "                       if 'difference_analysis' in r]\n",
    "    kl_divs = [r['output_comparison']['kl_divergence'] for r in all_results \n",
    "               if 'output_comparison' in r]\n",
    "    \n",
    "    if len(activation_diffs) == len(kl_divs):\n",
    "        ax.scatter(activation_diffs, kl_divs, s=100, alpha=0.7, c='purple')\n",
    "        \n",
    "        # Add labels for each point\n",
    "        for i, (x, y) in enumerate(zip(activation_diffs, kl_divs)):\n",
    "            ax.annotate(f'Q{i+1}', (x, y), xytext=(5, 5), \n",
    "                       textcoords='offset points', fontsize=9)\n",
    "        \n",
    "        # Add trend line\n",
    "        z = np.polyfit(activation_diffs, kl_divs, 1)\n",
    "        p = np.poly1d(z)\n",
    "        ax.plot(activation_diffs, p(activation_diffs), \"r--\", alpha=0.8, \n",
    "                label=f'Trend: y={z[0]:.4f}x+{z[1]:.4f}')\n",
    "        \n",
    "        ax.set_xlabel('Total Activation Difference')\n",
    "        ax.set_ylabel('KL Divergence')\n",
    "        ax.set_title('Correlation: Activation Differences vs Output Differences')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Calculate correlation\n",
    "        correlation = np.corrcoef(activation_diffs, kl_divs)[0, 1]\n",
    "        ax.text(0.05, 0.95, f'Correlation: {correlation:.3f}', \n",
    "                transform=ax.transAxes, fontsize=12, \n",
    "                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('activation_output_correlation.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"\\nCorrelation plot saved to activation_output_correlation.png\")\n",
    "        print(f\"Correlation coefficient: {correlation:.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ANALYSIS COMPLETE\")\n",
    "print(\"=\"*70)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
