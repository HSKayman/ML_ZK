{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "86ce8fb9",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026-02-12 10:26:16.107909: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2026-02-12 10:26:16.145390: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2026-02-12 10:26:17.089444: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
        "import os\n",
        "import gc\n",
        "from typing import List, Dict, Any, Optional, Tuple"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "1af8778c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "# %%\n",
        "DEVICE = torch.device('cpu')#'cuda' if torch.cuda.is_available() else 'cpu')\n",
        "MODEL_PATH = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "print(f\"Using device: {DEVICE}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "d4ec2e00",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4040925b2e834b369505775f1295f56f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "LlamaForCausalLM(\n",
              "  (model): LlamaModel(\n",
              "    (embed_tokens): Embedding(32000, 4096)\n",
              "    (layers): ModuleList(\n",
              "      (0-31): 32 x LlamaDecoderLayer(\n",
              "        (self_attn): LlamaAttention(\n",
              "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
              "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
              "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
              "          (act_fn): SiLUActivation()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
              "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
              "      )\n",
              "    )\n",
              "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
              "    (rotary_emb): LlamaRotaryEmbedding()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# %%\n",
        "tokenizer = LlamaTokenizer.from_pretrained(MODEL_PATH)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# %%\n",
        "model = LlamaForCausalLM.from_pretrained(\n",
        "    MODEL_PATH,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=DEVICE\n",
        ")\n",
        "model.eval()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "e9f13ce9",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# =============================================================================\n",
        "# Distance Metrics for Comparing Logit Distributions\n",
        "# =============================================================================\n",
        "\n",
        "def compute_l2_distance(original_logits: torch.Tensor, perturbed_logits: torch.Tensor) -> float:\n",
        "    # apply softmax to the logits\n",
        "    original_logits = F.softmax(original_logits, dim=-1)\n",
        "    perturbed_logits = F.softmax(perturbed_logits, dim=-1)\n",
        "    # Compute L2 (Euclidean) distance between two logit vectors\n",
        "    return torch.norm(original_logits - perturbed_logits, p=2).item()\n",
        "\n",
        "def compute_cosine_distance(original_logits: torch.Tensor, perturbed_logits: torch.Tensor) -> float:\n",
        "    # Compute cosine distance (1 - cosine_similarity) between two logit vectors\n",
        "    cos_sim = F.cosine_similarity(original_logits.unsqueeze(0), perturbed_logits.unsqueeze(0))\n",
        "    return (1 - cos_sim).item()\n",
        "\n",
        "def compute_kl_divergence(original_logits: torch.Tensor, perturbed_logits: torch.Tensor) -> float:\n",
        "    # Compute KL divergence: KL(original || perturbed) after softmax\n",
        "    original_probs = F.softmax(original_logits, dim=-1)\n",
        "    perturbed_log_probs = F.log_softmax(perturbed_logits, dim=-1)\n",
        "    # KL(P || Q) = sum(P * log(P/Q)) = sum(P * (log_P - log_Q))\n",
        "    kl_div = F.kl_div(perturbed_log_probs, original_probs, reduction='sum')\n",
        "    return kl_div.item()\n",
        "\n",
        "def compute_js_divergence(original_logits: torch.Tensor, perturbed_logits: torch.Tensor) -> float:\n",
        "   # Compute Jensen-Shannon divergence: 0.5*KL(P||M) + 0.5*KL(Q||M) where M = 0.5*(P+Q).\n",
        "    P = F.softmax(original_logits, dim=-1)\n",
        "    Q = F.softmax(perturbed_logits, dim=-1)\n",
        "    M = 0.5 * (P + Q)\n",
        "    \n",
        "    # KL(P || M)\n",
        "    kl_pm = F.kl_div(M.log(), P, reduction='sum')\n",
        "    # KL(Q || M)\n",
        "    kl_qm = F.kl_div(M.log(), Q, reduction='sum')\n",
        "    \n",
        "    js_div = 0.5 * (kl_pm + kl_qm)\n",
        "    return js_div.item()\n",
        "\n",
        "def compute_all_distances(original_logits: torch.Tensor, perturbed_logits: torch.Tensor) -> Dict[str, float]:\n",
        "    # Compute all distance metrics between original and perturbed logits.\n",
        "    return {\n",
        "        'l2_distance': compute_l2_distance(original_logits, perturbed_logits),\n",
        "        'cosine_distance': compute_cosine_distance(original_logits, perturbed_logits),\n",
        "        'kl_divergence': compute_kl_divergence(original_logits, perturbed_logits),\n",
        "        'js_divergence': compute_js_divergence(original_logits, perturbed_logits),\n",
        "    }\n",
        "\n",
        "# %%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "0fbd8738",
      "metadata": {},
      "outputs": [],
      "source": [
        "# %%\n",
        "# Global variables for detailed activation capture\n",
        "captured_activations = {}\n",
        "current_hooks = []\n",
        "hook_errors = []\n",
        "\n",
        "def clear_activations():\n",
        "    global captured_activations\n",
        "    captured_activations.clear()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "def remove_all_hooks():\n",
        "    global current_hooks\n",
        "    for hook in current_hooks:\n",
        "        try:\n",
        "            hook.remove()\n",
        "        except:\n",
        "            pass\n",
        "    current_hooks.clear()\n",
        "\n",
        "def get_activation_hook(name):\n",
        "    def hook(module, input, output):\n",
        "        global hook_errors\n",
        "        try:\n",
        "            # Handle different output types\n",
        "            if output is None:\n",
        "                activation = None\n",
        "            elif isinstance(output, tuple):\n",
        "                activation = output[0]\n",
        "            elif hasattr(output, 'last_hidden_state'):\n",
        "                # Handle model output objects\n",
        "                activation = output.last_hidden_state\n",
        "            else:\n",
        "                activation = output\n",
        "            \n",
        "            # Handle input\n",
        "            input_tensor = input[0] if isinstance(input, tuple) and len(input) > 0 else None\n",
        "\n",
        "            # Safely detach and move to CPU\n",
        "            def safe_detach_cpu(tensor):\n",
        "                if tensor is None:\n",
        "                    return None\n",
        "                try:\n",
        "                    # Check if tensor is on meta device\n",
        "                    if hasattr(tensor, 'device') and str(tensor.device) == 'meta':\n",
        "                        return None\n",
        "                    return tensor.detach().cpu()\n",
        "                except Exception as e:\n",
        "                    hook_errors.append(f\"Detach error in {name}: {str(e)}\")\n",
        "                    return None\n",
        "\n",
        "            captured_activations[name] = {\n",
        "                'output': safe_detach_cpu(activation),\n",
        "                'input': safe_detach_cpu(input_tensor),\n",
        "                'weight': safe_detach_cpu(module.weight) if hasattr(module, 'weight') and module.weight is not None else None,\n",
        "                'bias': safe_detach_cpu(module.bias) if hasattr(module, 'bias') and module.bias is not None else None\n",
        "            }\n",
        "        except Exception as e:\n",
        "            error_msg = f\"Hook error in {name}: {str(e)}\"\n",
        "            hook_errors.append(error_msg)\n",
        "            captured_activations[name] = {'output': None, 'input': None, 'weight': None, 'bias': None}\n",
        "    return hook\n",
        "\n",
        "def register_llama_hooks(model):\n",
        "    global current_hooks\n",
        "    remove_all_hooks() # clear any old hooks first\n",
        "    hook_errors.clear()\n",
        "\n",
        "    # total_layers = len(model.model.layers)\n",
        "\n",
        "    # for i in range(total_layers):\n",
        "    #     layer = model.model.layers[i]\n",
        "    #     layer_prefix = f\"layer_{i}\"\n",
        "    #     components = [\n",
        "    #         (layer.self_attn.q_proj, f\"{layer_prefix}_attention_q\"), (layer.self_attn.k_proj, f\"{layer_prefix}_attention_k\"),\n",
        "    #         (layer.self_attn.v_proj, f\"{layer_prefix}_attention_v\"), (layer.self_attn.o_proj, f\"{layer_prefix}_attention_output\"),\n",
        "    #         (layer.mlp.gate_proj, f\"{layer_prefix}_mlp_gate\"), (layer.mlp.up_proj, f\"{layer_prefix}_mlp_up\"),\n",
        "    #         (layer.mlp.down_proj, f\"{layer_prefix}_mlp_down\"), (layer.input_layernorm, f\"{layer_prefix}_input_norm\"),\n",
        "    #         (layer.post_attention_layernorm, f\"{layer_prefix}_post_attn_norm\"),\n",
        "    #     ]\n",
        "    #     for module, name in components:\n",
        "    #         current_hooks.append(module.register_forward_hook(get_activation_hook(name)))\n",
        "    \n",
        "    current_hooks.append(model.model.norm.register_forward_hook(get_activation_hook(\"final_norm\")))\n",
        "    current_hooks.append(model.lm_head.register_forward_hook(get_activation_hook(\"lm_head\")))\n",
        "    # print(f\"Registered {len(current_hooks)} hooks.\")\n",
        "\n",
        "def run_model_and_capture_activations(model, inputs=None, inputs_embeds=None):\n",
        "    global hook_errors\n",
        "    clear_activations()\n",
        "    register_llama_hooks(model)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        if inputs is not None:\n",
        "            _ = model(**inputs)\n",
        "        elif inputs_embeds is not None:\n",
        "            _ = model(inputs_embeds=inputs_embeds)\n",
        "        else:\n",
        "            raise ValueError(\"Either inputs or inputs_embeds must be provided.\")\n",
        "            \n",
        "    remove_all_hooks()\n",
        "    \n",
        "    # Print any hook errors that occurred\n",
        "    if hook_errors:\n",
        "        print(f\"WARNING: {len(hook_errors)} hook errors occurred:\")\n",
        "        for err in hook_errors[:5]:\n",
        "            print(f\"  - {err}\")\n",
        "        if len(hook_errors) > 5:\n",
        "            print(f\"  ... and {len(hook_errors) - 5} more\")\n",
        "    \n",
        "    # return a copy of the captured activations\n",
        "    return captured_activations.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "a7d10519",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# RMSNorm and Gradient-Based Perturbation Functions\n",
        "# =============================================================================\n",
        "\n",
        "def compute_swap_gradient(\n",
        "    z: torch.Tensor,\n",
        "    W: torch.Tensor,\n",
        "    top1_idx: int,\n",
        "    top2_idx: int,\n",
        "    norm_layer: nn.Module,\n",
        "    bias: Optional[torch.Tensor] = None\n",
        ") -> torch.Tensor:\n",
        "    # Compute gradient of swap loss w.r.t. pre-norm activations z.\n",
        "    # swap loss: L = p[top1] - p[top2], we minimize this to achieve swap.\n",
        "    z = z.clone().detach().requires_grad_(True)\n",
        "    \n",
        "    z_norm = norm_layer(z)\n",
        "    logits = F.linear(z_norm, W, bias)\n",
        "    probs = F.softmax(logits, dim=-1)\n",
        "    \n",
        "    swap_loss = probs[top1_idx] - probs[top2_idx]\n",
        "    swap_loss.backward()\n",
        "    \n",
        "    return -z.grad.detach()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "592caac7",
      "metadata": {},
      "outputs": [],
      "source": [
        "def rank_neurons_by_alignment(\n",
        "    gradient: torch.Tensor,\n",
        "    W: torch.Tensor,\n",
        "    exclude_indices: Optional[List[int]] = None\n",
        ") -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "    # Rank neurons by alignment: gradient sensitivity weighted by W column norms\n",
        "    # gradient is [hidden_size], W is [vocab_size, hidden_size]\n",
        "    # W[:,i] column norm tells us how much neuron i affects outputs\n",
        "    \n",
        "    w_col_norms = torch.norm(W, dim=0)  # [hidden_size]\n",
        "    projections = gradient * w_col_norms  # [hidden_size]\n",
        "    \n",
        "    # Scores are absolute values\n",
        "    scores = torch.abs(projections)\n",
        "    \n",
        "    # If excluding certain neurons, set their scores to -inf so they're ranked last\n",
        "    if exclude_indices is not None and len(exclude_indices) > 0:\n",
        "        for idx in exclude_indices:\n",
        "            scores[idx] = -float('inf')\n",
        "    \n",
        "    # Sort by score descending\n",
        "    sorted_scores, sorted_indices = torch.sort(scores, descending=True)\n",
        "    \n",
        "    # Get signs for perturbation direction\n",
        "    signs = torch.sign(projections)\n",
        "    \n",
        "    return sorted_indices, sorted_scores, signs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "10d6338a",
      "metadata": {},
      "outputs": [],
      "source": [
        "def identify_special_node(\n",
        "    gradient: torch.Tensor,\n",
        "    W: torch.Tensor\n",
        ") -> Tuple[int, float]:\n",
        "    #special_node_idx: Index of the special neuron\n",
        "    # special_node_score: Its impact score\n",
        "\n",
        "    w_col_norms = torch.norm(W, dim=0)  # [hidden_size]\n",
        "    projections = gradient * w_col_norms  # [hidden_size]\n",
        "    scores = torch.abs(projections)\n",
        "    \n",
        "    special_node_idx = torch.argmax(scores).item()\n",
        "    special_node_score = scores[special_node_idx].item()\n",
        "    \n",
        "    return special_node_idx, special_node_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "23dcc77e",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_top_k_predictions(logits: torch.Tensor, tokenizer, k: int = 3) -> Dict[str, Any]:\n",
        "    probs = F.softmax(logits, dim=-1)\n",
        "    top_logits, top_indices = torch.topk(logits, k)\n",
        "    top_probs = probs[top_indices]\n",
        "    result = {}\n",
        "    for i in range(k):\n",
        "        idx = top_indices[i].item()\n",
        "        result[f'top{i+1}_word'] = tokenizer.decode([idx])\n",
        "        result[f'top{i+1}_index'] = idx\n",
        "        result[f'top{i+1}_logit'] = top_logits[i].item()\n",
        "        result[f'top{i+1}_softmax'] = top_probs[i].item()\n",
        "    return result\n",
        "\n",
        "\n",
        "def run_optimized_swap_attack(\n",
        "    model, tokenizer, string_input, filename, epsilon_values, max_neurons_list\n",
        "):\n",
        "    input_id, input_text = string_input\n",
        "    sample_input = tokenizer(input_text, return_tensors=\"pt\")\n",
        "    inputs_on_device = {k: v.to(model.device) for k, v in sample_input.items()}\n",
        "\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Input ID: {input_id}, Input: '{input_text}'\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    # Capture activations ONCE\n",
        "    original_activations = run_model_and_capture_activations(model, inputs=inputs_on_device)\n",
        "    try:\n",
        "        pre_norm_activations = original_activations['final_norm']['input']\n",
        "        if pre_norm_activations is None:\n",
        "            print('I am making mistake check me!!!!')\n",
        "            pre_norm_activations = original_activations['final_norm']['output']\n",
        "        pre_norm_activations = pre_norm_activations.to(model.device).float()\n",
        "    except KeyError:\n",
        "        print(\"ERROR: Could not find 'final_norm' in activations.\")\n",
        "        return None\n",
        "\n",
        "    z = pre_norm_activations[0, -1, :].float()\n",
        "    norm_layer = model.model.norm\n",
        "    W = model.lm_head.weight.detach().float()\n",
        "    bias = model.lm_head.bias.detach().float() if hasattr(model.lm_head, 'bias') and model.lm_head.bias is not None else None\n",
        "    hidden_size = z.shape[-1]\n",
        "    max_budget = max(max_neurons_list)\n",
        "\n",
        "    # Original predictions ONCE\n",
        "    z_norm = norm_layer(z)\n",
        "    original_logits = F.linear(z_norm, W, bias)\n",
        "    top2_indices = torch.topk(original_logits, 2).indices\n",
        "    top1_idx = top2_indices[0].item()\n",
        "    top2_idx = top2_indices[1].item()\n",
        "    original_top3 = get_top_k_predictions(original_logits, tokenizer, k=3)\n",
        "\n",
        "    print(f\"  Original top-1: '{original_top3['top1_word']}' (p={original_top3['top1_softmax']:.4f})\")\n",
        "    print(f\"  Original top-2: '{original_top3['top2_word']}' (p={original_top3['top2_softmax']:.4f})\")\n",
        "\n",
        "    # Gradient, special node, rankings — all computed ONCE\n",
        "    gradient = compute_swap_gradient(z, W, top1_idx, top2_idx, norm_layer, bias)\n",
        "    special_node_idx, special_node_score = identify_special_node(gradient, W)\n",
        "    print(f\"  Special node: idx={special_node_idx}, score={special_node_score:.4f}\")\n",
        "\n",
        "    bl_indices, bl_scores, bl_signs = rank_neurons_by_alignment(gradient, W)\n",
        "    cn_indices, cn_scores, cn_signs = rank_neurons_by_alignment(gradient, W, [special_node_idx])\n",
        "\n",
        "    # --- Nested epsilon x neuron loop (the core optimization) ---\n",
        "    def find_swap_points(sorted_indices, sorted_scores, signs):\n",
        "        # For each epsilon, greedily add neurons and find swap point.\n",
        "        results = []\n",
        "        for epsilon in epsilon_values:\n",
        "            z_mod = z.clone()\n",
        "            perturbed = []\n",
        "            found = False\n",
        "            for k in range(min(max_budget, hidden_size)):\n",
        "                neuron_idx = sorted_indices[k].item()\n",
        "                if sorted_scores[k] == -float('inf'):\n",
        "                    continue\n",
        "                z_mod[neuron_idx] += signs[neuron_idx].item() * epsilon\n",
        "                perturbed.append(neuron_idx)\n",
        "                z_mod_norm = norm_layer(z_mod)\n",
        "                new_logits = F.linear(z_mod_norm, W, bias)\n",
        "                # kl divergence between original logit and new logit\n",
        "                kl_div_new = F.kl_div(new_logits.log(), original_logits, reduction='sum')\n",
        "                #check kl_div_new > 0.1 then break\n",
        "                if kl_div_new > 0.1:\n",
        "                    \n",
        "                    results.append({\n",
        "                        'epsilon': epsilon, 'success': True,\n",
        "                        'num_neurons': len(perturbed),\n",
        "                        'distances': compute_all_distances(original_logits, new_logits),\n",
        "                        'top3': get_top_k_predictions(new_logits, tokenizer, k=3),\n",
        "                        'special_node_used': special_node_idx in perturbed,\n",
        "                        'top_token_changed': torch.argmax(new_logits) != top1_idx,\n",
        "                    })\n",
        "                    found = True\n",
        "                    break\n",
        "    \n",
        "            if not found:\n",
        "                z_mod_norm = norm_layer(z_mod)\n",
        "                final_logits = F.linear(z_mod_norm, W, bias)\n",
        "                results.append({\n",
        "                    'epsilon': epsilon, 'success': False,\n",
        "                    'num_neurons': len(perturbed),\n",
        "                    'distances': compute_all_distances(original_logits, final_logits),\n",
        "                    'top3': get_top_k_predictions(final_logits, tokenizer, k=3),\n",
        "                    'special_node_used': special_node_idx in perturbed,\n",
        "                })\n",
        "        return results\n",
        "\n",
        "    print(\"  Computing baseline swap points...\")\n",
        "    bl_swap = find_swap_points(bl_indices, bl_scores, bl_signs)\n",
        "    print(\"  Computing constrained swap points...\")\n",
        "    cn_swap = find_swap_points(cn_indices, cn_scores, cn_signs)\n",
        "\n",
        "    # Build CSV records for each neuron budget\n",
        "    records = []\n",
        "    for mn in max_neurons_list:\n",
        "        bl = next((r for r in bl_swap if r['success'] and r['num_neurons'] <= mn), bl_swap[-1])\n",
        "        cn = next((r for r in cn_swap if r['success'] and r['num_neurons'] <= mn), cn_swap[-1])\n",
        "\n",
        "        bl_ok = bl['success'] and bl['num_neurons'] <= mn\n",
        "        cn_ok = cn['success'] and cn['num_neurons'] <= mn\n",
        "\n",
        "        record = {\n",
        "            'input_id': input_id,\n",
        "            'allowed_neurons': mn,\n",
        "            'special_node_idx': special_node_idx,\n",
        "            'special_node_score': special_node_score,\n",
        "\n",
        "            'baseline_success': bl_ok,\n",
        "            'baseline_num_neurons': bl['num_neurons'],\n",
        "            'baseline_epsilon': bl['epsilon'],\n",
        "            'baseline_special_used': bl['special_node_used'],\n",
        "            'baseline_total_magnitude': bl['num_neurons'] * abs(bl['epsilon']),\n",
        "            'baseline_max_perturbation': abs(bl['epsilon']),\n",
        "            **{f'baseline_{k}': v for k, v in bl['distances'].items()},\n",
        "            **{f'baseline_final_{k}': v for k, v in bl['top3'].items()},\n",
        "\n",
        "            'constrained_success': cn_ok,\n",
        "            'constrained_num_neurons': cn['num_neurons'],\n",
        "            'constrained_epsilon': cn['epsilon'],\n",
        "            'constrained_special_avoided': not cn['special_node_used'],\n",
        "            'constrained_total_magnitude': cn['num_neurons'] * abs(cn['epsilon']),\n",
        "            'constrained_max_perturbation': abs(cn['epsilon']),\n",
        "            **{f'constrained_{k}': v for k, v in cn['distances'].items()},\n",
        "            **{f'constrained_final_{k}': v for k, v in cn['top3'].items()},\n",
        "\n",
        "            **{f'orig_{k}': v for k, v in original_top3.items()},\n",
        "\n",
        "            'neurons_diff': cn['num_neurons'] - bl['num_neurons'],\n",
        "            'epsilon_diff': cn['epsilon'] - bl['epsilon'],\n",
        "            'magnitude_diff': (cn['num_neurons'] * abs(cn['epsilon'])) - (bl['num_neurons'] * abs(bl['epsilon'])),\n",
        "        }\n",
        "        records.append(record)\n",
        "\n",
        "    # Save all records at once\n",
        "    df = pd.DataFrame(records)\n",
        "    file_exists = os.path.exists(filename)\n",
        "    df.to_csv(filename, mode='a', header=not file_exists, index=False)\n",
        "\n",
        "    del original_activations, pre_norm_activations\n",
        "    clear_activations()\n",
        "\n",
        "    bl_successes = sum(1 for r in records if r['baseline_success'])\n",
        "    cn_successes = sum(1 for r in records if r['constrained_success'])\n",
        "    print(f\"  Saved {len(records)} records — baseline successes: {bl_successes}, constrained successes: {cn_successes}\")\n",
        "    return records"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "8625cd3f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experiment date: 2026-02-12_10-26-25\n",
            "Epsilon values: 999 from 0.1 to 99.9\n",
            "Neuron budgets: 4095 from 1 to 4095\n",
            "Output: ./gradient_swap_attack_optimized_2026-02-12_10-26-25.csv\n",
            "\n",
            ">>>> Prompt 1/25 <<<<\n",
            "\n",
            "================================================================================\n",
            "Input ID: 1, Input: 'The capital of France is'\n",
            "================================================================================\n",
            "  Original top-1: 'Paris' (p=0.8931)\n",
            "  Original top-2: 'a' (p=0.0345)\n",
            "  Special node: idx=3556, score=0.0408\n",
            "  Computing baseline swap points...\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[10], line 43\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, prompt \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(sample_texts):\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m>>>> Prompt \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(sample_texts)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m <<<<\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 43\u001b[0m     \u001b[43mrun_optimized_swap_attack\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstring_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mOUTPUT_FILE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPSILON_VALUES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_neurons_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_neurons_list\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAll done! Results saved to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mOUTPUT_FILE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "Cell \u001b[0;32mIn[9], line 109\u001b[0m, in \u001b[0;36mrun_optimized_swap_attack\u001b[0;34m(model, tokenizer, string_input, filename, epsilon_values, max_neurons_list)\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Computing baseline swap points...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 109\u001b[0m bl_swap \u001b[38;5;241m=\u001b[39m \u001b[43mfind_swap_points\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbl_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbl_scores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbl_signs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Computing constrained swap points...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    111\u001b[0m cn_swap \u001b[38;5;241m=\u001b[39m find_swap_points(cn_indices, cn_scores, cn_signs)\n",
            "Cell \u001b[0;32mIn[9], line 79\u001b[0m, in \u001b[0;36mrun_optimized_swap_attack.<locals>.find_swap_points\u001b[0;34m(sorted_indices, sorted_scores, signs)\u001b[0m\n\u001b[1;32m     77\u001b[0m perturbed\u001b[38;5;241m.\u001b[39mappend(neuron_idx)\n\u001b[1;32m     78\u001b[0m z_mod_norm \u001b[38;5;241m=\u001b[39m norm_layer(z_mod)\n\u001b[0;32m---> 79\u001b[0m new_logits \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz_mod_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# kl divergence between original logit and new logit\u001b[39;00m\n\u001b[1;32m     81\u001b[0m kl_div_new \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mkl_div(new_logits\u001b[38;5;241m.\u001b[39mlog(), original_logits, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from datetime import datetime\n",
        "date_of_run = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "\n",
        "sample_texts = [\n",
        "    [1,\"The capital of France is\"],\n",
        "    [2,\"The largest mammal on Earth is\"],\n",
        "    [3,\"The process of photosynthesis occurs in\"],\n",
        "    [4,\"The speed of light in a vacuum is\"],\n",
        "    [5,\"The chemical symbol for gold is\"],\n",
        "    [6,\"The human body has how many bones\"],\n",
        "    [7,\"The Great Wall of China was built to\"],\n",
        "    [8,\"Water boils at what temperature\"],\n",
        "    [9,\"The smallest unit of matter is\"],\n",
        "    [10,\"Shakespeare wrote the play\"],\n",
        "    [11,\"The currency of Japan is\"],\n",
        "    [12,\"Mount Everest is located in\"],\n",
        "    [13,\"The inventor of the telephone was\"],\n",
        "    [14,\"DNA stands for\"],\n",
        "    [15,\"The largest ocean on Earth is\"],\n",
        "    [16,\"The planet closest to the Sun is\"],\n",
        "    [17,\"Gravity was discovered by\"],\n",
        "    [18,\"The Amazon rainforest is primarily located in\"],\n",
        "    [19,\"The freezing point of water is\"],\n",
        "    [20,\"The most abundant gas in Earth's atmosphere is\"],\n",
        "    [21,\"The Mona Lisa was painted by\"],\n",
        "    [22,\"The longest river in the world is\"],\n",
        "    [23,\"Photosynthesis converts carbon dioxide and water into\"],\n",
        "    [24,\"The study of earthquakes is called\"],\n",
        "    [25,\"The first person to walk on the moon was\"]\n",
        "]\n",
        "\n",
        "EPSILON_VALUES = [i * 0.1 for i in range(1, 1000, 1)]\n",
        "max_neurons_list = list(range(1, 4096, 1))\n",
        "OUTPUT_FILE = f\"./gradient_swap_attack_optimized_{date_of_run}.csv\"\n",
        "\n",
        "print(f\"Experiment date: {date_of_run}\")\n",
        "print(f\"Epsilon values: {len(EPSILON_VALUES)} from {EPSILON_VALUES[0]} to {EPSILON_VALUES[-1]}\")\n",
        "print(f\"Neuron budgets: {len(max_neurons_list)} from {max_neurons_list[0]} to {max_neurons_list[-1]}\")\n",
        "print(f\"Output: {OUTPUT_FILE}\")\n",
        "\n",
        "for i, prompt in enumerate(sample_texts):\n",
        "    print(f\"\\n>>>> Prompt {i+1}/{len(sample_texts)} <<<<\")\n",
        "    run_optimized_swap_attack(\n",
        "        model=model, tokenizer=tokenizer, string_input=prompt,\n",
        "        filename=OUTPUT_FILE, epsilon_values=EPSILON_VALUES,\n",
        "        max_neurons_list=max_neurons_list\n",
        "    )\n",
        "\n",
        "print(f\"\\nAll done! Results saved to '{OUTPUT_FILE}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5697ecd0",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
