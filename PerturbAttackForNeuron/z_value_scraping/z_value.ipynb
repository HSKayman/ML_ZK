{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-04 17:03:53.948119: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2026-02-04 17:03:54.182948: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2026-02-04 17:03:55.410527: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "import os\n",
    "import gc\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "device-setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Output directory: ./scraped_data\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device('cpu')  # 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "MODEL_PATH = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "OUTPUT_DIR = \"./scraped_data\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "load-model",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e5e827bcf0944fb8dfe01347a688dee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = LlamaTokenizer.from_pretrained(MODEL_PATH)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=DEVICE\n",
    ")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6b56f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAYER EXECUTION ORDER:\n",
      "====================================================================================================\n",
      "  1. model.embed_tokens                                 | Embedding            | torch.Size([1, 3]) → torch.Size([1, 3, 4096])\n",
      "  2. model.rotary_emb                                   | LlamaRotaryEmbedding | torch.Size([1, 3, 4096]) → tuple\n",
      "  3. model.layers.0.input_layernorm                     | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "  4. model.layers.0.self_attn.q_proj                    | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "  5. model.layers.0.self_attn.k_proj                    | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "  6. model.layers.0.self_attn.v_proj                    | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "  7. model.layers.0.self_attn.o_proj                    | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "  8. model.layers.0.post_attention_layernorm            | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "  9. model.layers.0.mlp.gate_proj                       | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      " 10. model.layers.0.mlp.act_fn                          | SiLUActivation       | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 11008])\n",
      " 11. model.layers.0.mlp.up_proj                         | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      " 12. model.layers.0.mlp.down_proj                       | Linear               | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 4096])\n",
      " 13. model.layers.1.input_layernorm                     | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      " 14. model.layers.1.self_attn.q_proj                    | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      " 15. model.layers.1.self_attn.k_proj                    | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      " 16. model.layers.1.self_attn.v_proj                    | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      " 17. model.layers.1.self_attn.o_proj                    | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      " 18. model.layers.1.post_attention_layernorm            | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      " 19. model.layers.1.mlp.gate_proj                       | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      " 20. model.layers.1.mlp.act_fn                          | SiLUActivation       | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 11008])\n",
      " 21. model.layers.1.mlp.up_proj                         | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      " 22. model.layers.1.mlp.down_proj                       | Linear               | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 4096])\n",
      " 23. model.layers.2.input_layernorm                     | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      " 24. model.layers.2.self_attn.q_proj                    | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      " 25. model.layers.2.self_attn.k_proj                    | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      " 26. model.layers.2.self_attn.v_proj                    | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      " 27. model.layers.2.self_attn.o_proj                    | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      " 28. model.layers.2.post_attention_layernorm            | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      " 29. model.layers.2.mlp.gate_proj                       | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      " 30. model.layers.2.mlp.act_fn                          | SiLUActivation       | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 11008])\n",
      " 31. model.layers.2.mlp.up_proj                         | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      " 32. model.layers.2.mlp.down_proj                       | Linear               | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 4096])\n",
      " 33. model.layers.3.input_layernorm                     | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      " 34. model.layers.3.self_attn.q_proj                    | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      " 35. model.layers.3.self_attn.k_proj                    | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      " 36. model.layers.3.self_attn.v_proj                    | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      " 37. model.layers.3.self_attn.o_proj                    | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      " 38. model.layers.3.post_attention_layernorm            | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      " 39. model.layers.3.mlp.gate_proj                       | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      " 40. model.layers.3.mlp.act_fn                          | SiLUActivation       | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 11008])\n",
      " 41. model.layers.3.mlp.up_proj                         | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      " 42. model.layers.3.mlp.down_proj                       | Linear               | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 4096])\n",
      " 43. model.layers.4.input_layernorm                     | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      " 44. model.layers.4.self_attn.q_proj                    | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      " 45. model.layers.4.self_attn.k_proj                    | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      " 46. model.layers.4.self_attn.v_proj                    | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      " 47. model.layers.4.self_attn.o_proj                    | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      " 48. model.layers.4.post_attention_layernorm            | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      " 49. model.layers.4.mlp.gate_proj                       | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      " 50. model.layers.4.mlp.act_fn                          | SiLUActivation       | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 11008])\n",
      " 51. model.layers.4.mlp.up_proj                         | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      " 52. model.layers.4.mlp.down_proj                       | Linear               | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 4096])\n",
      " 53. model.layers.5.input_layernorm                     | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      " 54. model.layers.5.self_attn.q_proj                    | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      " 55. model.layers.5.self_attn.k_proj                    | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      " 56. model.layers.5.self_attn.v_proj                    | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      " 57. model.layers.5.self_attn.o_proj                    | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      " 58. model.layers.5.post_attention_layernorm            | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      " 59. model.layers.5.mlp.gate_proj                       | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      " 60. model.layers.5.mlp.act_fn                          | SiLUActivation       | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 11008])\n",
      " 61. model.layers.5.mlp.up_proj                         | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      " 62. model.layers.5.mlp.down_proj                       | Linear               | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 4096])\n",
      " 63. model.layers.6.input_layernorm                     | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      " 64. model.layers.6.self_attn.q_proj                    | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      " 65. model.layers.6.self_attn.k_proj                    | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      " 66. model.layers.6.self_attn.v_proj                    | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      " 67. model.layers.6.self_attn.o_proj                    | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      " 68. model.layers.6.post_attention_layernorm            | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      " 69. model.layers.6.mlp.gate_proj                       | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      " 70. model.layers.6.mlp.act_fn                          | SiLUActivation       | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 11008])\n",
      " 71. model.layers.6.mlp.up_proj                         | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      " 72. model.layers.6.mlp.down_proj                       | Linear               | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 4096])\n",
      " 73. model.layers.7.input_layernorm                     | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      " 74. model.layers.7.self_attn.q_proj                    | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      " 75. model.layers.7.self_attn.k_proj                    | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      " 76. model.layers.7.self_attn.v_proj                    | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      " 77. model.layers.7.self_attn.o_proj                    | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      " 78. model.layers.7.post_attention_layernorm            | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      " 79. model.layers.7.mlp.gate_proj                       | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      " 80. model.layers.7.mlp.act_fn                          | SiLUActivation       | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 11008])\n",
      " 81. model.layers.7.mlp.up_proj                         | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      " 82. model.layers.7.mlp.down_proj                       | Linear               | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 4096])\n",
      " 83. model.layers.8.input_layernorm                     | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      " 84. model.layers.8.self_attn.q_proj                    | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      " 85. model.layers.8.self_attn.k_proj                    | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      " 86. model.layers.8.self_attn.v_proj                    | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      " 87. model.layers.8.self_attn.o_proj                    | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      " 88. model.layers.8.post_attention_layernorm            | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      " 89. model.layers.8.mlp.gate_proj                       | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      " 90. model.layers.8.mlp.act_fn                          | SiLUActivation       | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 11008])\n",
      " 91. model.layers.8.mlp.up_proj                         | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      " 92. model.layers.8.mlp.down_proj                       | Linear               | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 4096])\n",
      " 93. model.layers.9.input_layernorm                     | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      " 94. model.layers.9.self_attn.q_proj                    | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      " 95. model.layers.9.self_attn.k_proj                    | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      " 96. model.layers.9.self_attn.v_proj                    | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      " 97. model.layers.9.self_attn.o_proj                    | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      " 98. model.layers.9.post_attention_layernorm            | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      " 99. model.layers.9.mlp.gate_proj                       | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      "100. model.layers.9.mlp.act_fn                          | SiLUActivation       | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 11008])\n",
      "101. model.layers.9.mlp.up_proj                         | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      "102. model.layers.9.mlp.down_proj                       | Linear               | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 4096])\n",
      "103. model.layers.10.input_layernorm                    | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "104. model.layers.10.self_attn.q_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "105. model.layers.10.self_attn.k_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "106. model.layers.10.self_attn.v_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "107. model.layers.10.self_attn.o_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "108. model.layers.10.post_attention_layernorm           | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "109. model.layers.10.mlp.gate_proj                      | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      "110. model.layers.10.mlp.act_fn                         | SiLUActivation       | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 11008])\n",
      "111. model.layers.10.mlp.up_proj                        | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      "112. model.layers.10.mlp.down_proj                      | Linear               | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 4096])\n",
      "113. model.layers.11.input_layernorm                    | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "114. model.layers.11.self_attn.q_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "115. model.layers.11.self_attn.k_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "116. model.layers.11.self_attn.v_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "117. model.layers.11.self_attn.o_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "118. model.layers.11.post_attention_layernorm           | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "119. model.layers.11.mlp.gate_proj                      | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      "120. model.layers.11.mlp.act_fn                         | SiLUActivation       | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 11008])\n",
      "121. model.layers.11.mlp.up_proj                        | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      "122. model.layers.11.mlp.down_proj                      | Linear               | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 4096])\n",
      "123. model.layers.12.input_layernorm                    | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "124. model.layers.12.self_attn.q_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "125. model.layers.12.self_attn.k_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "126. model.layers.12.self_attn.v_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "127. model.layers.12.self_attn.o_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "128. model.layers.12.post_attention_layernorm           | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "129. model.layers.12.mlp.gate_proj                      | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      "130. model.layers.12.mlp.act_fn                         | SiLUActivation       | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 11008])\n",
      "131. model.layers.12.mlp.up_proj                        | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      "132. model.layers.12.mlp.down_proj                      | Linear               | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 4096])\n",
      "133. model.layers.13.input_layernorm                    | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "134. model.layers.13.self_attn.q_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "135. model.layers.13.self_attn.k_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "136. model.layers.13.self_attn.v_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "137. model.layers.13.self_attn.o_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "138. model.layers.13.post_attention_layernorm           | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "139. model.layers.13.mlp.gate_proj                      | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      "140. model.layers.13.mlp.act_fn                         | SiLUActivation       | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 11008])\n",
      "141. model.layers.13.mlp.up_proj                        | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      "142. model.layers.13.mlp.down_proj                      | Linear               | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 4096])\n",
      "143. model.layers.14.input_layernorm                    | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "144. model.layers.14.self_attn.q_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "145. model.layers.14.self_attn.k_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "146. model.layers.14.self_attn.v_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "147. model.layers.14.self_attn.o_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "148. model.layers.14.post_attention_layernorm           | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "149. model.layers.14.mlp.gate_proj                      | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      "150. model.layers.14.mlp.act_fn                         | SiLUActivation       | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 11008])\n",
      "151. model.layers.14.mlp.up_proj                        | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      "152. model.layers.14.mlp.down_proj                      | Linear               | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 4096])\n",
      "153. model.layers.15.input_layernorm                    | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "154. model.layers.15.self_attn.q_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "155. model.layers.15.self_attn.k_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "156. model.layers.15.self_attn.v_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "157. model.layers.15.self_attn.o_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "158. model.layers.15.post_attention_layernorm           | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "159. model.layers.15.mlp.gate_proj                      | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      "160. model.layers.15.mlp.act_fn                         | SiLUActivation       | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 11008])\n",
      "161. model.layers.15.mlp.up_proj                        | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      "162. model.layers.15.mlp.down_proj                      | Linear               | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 4096])\n",
      "163. model.layers.16.input_layernorm                    | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "164. model.layers.16.self_attn.q_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "165. model.layers.16.self_attn.k_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "166. model.layers.16.self_attn.v_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "167. model.layers.16.self_attn.o_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "168. model.layers.16.post_attention_layernorm           | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "169. model.layers.16.mlp.gate_proj                      | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      "170. model.layers.16.mlp.act_fn                         | SiLUActivation       | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 11008])\n",
      "171. model.layers.16.mlp.up_proj                        | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      "172. model.layers.16.mlp.down_proj                      | Linear               | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 4096])\n",
      "173. model.layers.17.input_layernorm                    | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "174. model.layers.17.self_attn.q_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "175. model.layers.17.self_attn.k_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "176. model.layers.17.self_attn.v_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "177. model.layers.17.self_attn.o_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "178. model.layers.17.post_attention_layernorm           | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "179. model.layers.17.mlp.gate_proj                      | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      "180. model.layers.17.mlp.act_fn                         | SiLUActivation       | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 11008])\n",
      "181. model.layers.17.mlp.up_proj                        | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      "182. model.layers.17.mlp.down_proj                      | Linear               | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 4096])\n",
      "183. model.layers.18.input_layernorm                    | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "184. model.layers.18.self_attn.q_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "185. model.layers.18.self_attn.k_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "186. model.layers.18.self_attn.v_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "187. model.layers.18.self_attn.o_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "188. model.layers.18.post_attention_layernorm           | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "189. model.layers.18.mlp.gate_proj                      | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      "190. model.layers.18.mlp.act_fn                         | SiLUActivation       | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 11008])\n",
      "191. model.layers.18.mlp.up_proj                        | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      "192. model.layers.18.mlp.down_proj                      | Linear               | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 4096])\n",
      "193. model.layers.19.input_layernorm                    | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "194. model.layers.19.self_attn.q_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "195. model.layers.19.self_attn.k_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "196. model.layers.19.self_attn.v_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "197. model.layers.19.self_attn.o_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "198. model.layers.19.post_attention_layernorm           | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "199. model.layers.19.mlp.gate_proj                      | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      "200. model.layers.19.mlp.act_fn                         | SiLUActivation       | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 11008])\n",
      "201. model.layers.19.mlp.up_proj                        | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      "202. model.layers.19.mlp.down_proj                      | Linear               | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 4096])\n",
      "203. model.layers.20.input_layernorm                    | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "204. model.layers.20.self_attn.q_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "205. model.layers.20.self_attn.k_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "206. model.layers.20.self_attn.v_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "207. model.layers.20.self_attn.o_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "208. model.layers.20.post_attention_layernorm           | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "209. model.layers.20.mlp.gate_proj                      | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      "210. model.layers.20.mlp.act_fn                         | SiLUActivation       | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 11008])\n",
      "211. model.layers.20.mlp.up_proj                        | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      "212. model.layers.20.mlp.down_proj                      | Linear               | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 4096])\n",
      "213. model.layers.21.input_layernorm                    | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "214. model.layers.21.self_attn.q_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "215. model.layers.21.self_attn.k_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "216. model.layers.21.self_attn.v_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "217. model.layers.21.self_attn.o_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "218. model.layers.21.post_attention_layernorm           | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "219. model.layers.21.mlp.gate_proj                      | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      "220. model.layers.21.mlp.act_fn                         | SiLUActivation       | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 11008])\n",
      "221. model.layers.21.mlp.up_proj                        | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      "222. model.layers.21.mlp.down_proj                      | Linear               | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 4096])\n",
      "223. model.layers.22.input_layernorm                    | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "224. model.layers.22.self_attn.q_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "225. model.layers.22.self_attn.k_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "226. model.layers.22.self_attn.v_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "227. model.layers.22.self_attn.o_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "228. model.layers.22.post_attention_layernorm           | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "229. model.layers.22.mlp.gate_proj                      | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      "230. model.layers.22.mlp.act_fn                         | SiLUActivation       | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 11008])\n",
      "231. model.layers.22.mlp.up_proj                        | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      "232. model.layers.22.mlp.down_proj                      | Linear               | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 4096])\n",
      "233. model.layers.23.input_layernorm                    | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "234. model.layers.23.self_attn.q_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "235. model.layers.23.self_attn.k_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "236. model.layers.23.self_attn.v_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "237. model.layers.23.self_attn.o_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "238. model.layers.23.post_attention_layernorm           | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "239. model.layers.23.mlp.gate_proj                      | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      "240. model.layers.23.mlp.act_fn                         | SiLUActivation       | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 11008])\n",
      "241. model.layers.23.mlp.up_proj                        | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      "242. model.layers.23.mlp.down_proj                      | Linear               | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 4096])\n",
      "243. model.layers.24.input_layernorm                    | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "244. model.layers.24.self_attn.q_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "245. model.layers.24.self_attn.k_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "246. model.layers.24.self_attn.v_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "247. model.layers.24.self_attn.o_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "248. model.layers.24.post_attention_layernorm           | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "249. model.layers.24.mlp.gate_proj                      | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      "250. model.layers.24.mlp.act_fn                         | SiLUActivation       | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 11008])\n",
      "251. model.layers.24.mlp.up_proj                        | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      "252. model.layers.24.mlp.down_proj                      | Linear               | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 4096])\n",
      "253. model.layers.25.input_layernorm                    | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "254. model.layers.25.self_attn.q_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "255. model.layers.25.self_attn.k_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "256. model.layers.25.self_attn.v_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "257. model.layers.25.self_attn.o_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "258. model.layers.25.post_attention_layernorm           | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "259. model.layers.25.mlp.gate_proj                      | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      "260. model.layers.25.mlp.act_fn                         | SiLUActivation       | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 11008])\n",
      "261. model.layers.25.mlp.up_proj                        | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      "262. model.layers.25.mlp.down_proj                      | Linear               | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 4096])\n",
      "263. model.layers.26.input_layernorm                    | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "264. model.layers.26.self_attn.q_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "265. model.layers.26.self_attn.k_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "266. model.layers.26.self_attn.v_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "267. model.layers.26.self_attn.o_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "268. model.layers.26.post_attention_layernorm           | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "269. model.layers.26.mlp.gate_proj                      | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      "270. model.layers.26.mlp.act_fn                         | SiLUActivation       | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 11008])\n",
      "271. model.layers.26.mlp.up_proj                        | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      "272. model.layers.26.mlp.down_proj                      | Linear               | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 4096])\n",
      "273. model.layers.27.input_layernorm                    | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "274. model.layers.27.self_attn.q_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "275. model.layers.27.self_attn.k_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "276. model.layers.27.self_attn.v_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "277. model.layers.27.self_attn.o_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "278. model.layers.27.post_attention_layernorm           | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "279. model.layers.27.mlp.gate_proj                      | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      "280. model.layers.27.mlp.act_fn                         | SiLUActivation       | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 11008])\n",
      "281. model.layers.27.mlp.up_proj                        | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      "282. model.layers.27.mlp.down_proj                      | Linear               | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 4096])\n",
      "283. model.layers.28.input_layernorm                    | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "284. model.layers.28.self_attn.q_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "285. model.layers.28.self_attn.k_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "286. model.layers.28.self_attn.v_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "287. model.layers.28.self_attn.o_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "288. model.layers.28.post_attention_layernorm           | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "289. model.layers.28.mlp.gate_proj                      | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      "290. model.layers.28.mlp.act_fn                         | SiLUActivation       | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 11008])\n",
      "291. model.layers.28.mlp.up_proj                        | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      "292. model.layers.28.mlp.down_proj                      | Linear               | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 4096])\n",
      "293. model.layers.29.input_layernorm                    | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "294. model.layers.29.self_attn.q_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "295. model.layers.29.self_attn.k_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "296. model.layers.29.self_attn.v_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "297. model.layers.29.self_attn.o_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "298. model.layers.29.post_attention_layernorm           | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "299. model.layers.29.mlp.gate_proj                      | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      "300. model.layers.29.mlp.act_fn                         | SiLUActivation       | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 11008])\n",
      "301. model.layers.29.mlp.up_proj                        | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      "302. model.layers.29.mlp.down_proj                      | Linear               | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 4096])\n",
      "303. model.layers.30.input_layernorm                    | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "304. model.layers.30.self_attn.q_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "305. model.layers.30.self_attn.k_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "306. model.layers.30.self_attn.v_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "307. model.layers.30.self_attn.o_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "308. model.layers.30.post_attention_layernorm           | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "309. model.layers.30.mlp.gate_proj                      | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      "310. model.layers.30.mlp.act_fn                         | SiLUActivation       | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 11008])\n",
      "311. model.layers.30.mlp.up_proj                        | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      "312. model.layers.30.mlp.down_proj                      | Linear               | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 4096])\n",
      "313. model.layers.31.input_layernorm                    | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "314. model.layers.31.self_attn.q_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "315. model.layers.31.self_attn.k_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "316. model.layers.31.self_attn.v_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "317. model.layers.31.self_attn.o_proj                   | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "318. model.layers.31.post_attention_layernorm           | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "319. model.layers.31.mlp.gate_proj                      | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      "320. model.layers.31.mlp.act_fn                         | SiLUActivation       | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 11008])\n",
      "321. model.layers.31.mlp.up_proj                        | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 11008])\n",
      "322. model.layers.31.mlp.down_proj                      | Linear               | torch.Size([1, 3, 11008]) → torch.Size([1, 3, 4096])\n",
      "323. model.norm                                         | LlamaRMSNorm         | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 4096])\n",
      "324. lm_head                                            | Linear               | torch.Size([1, 3, 4096]) → torch.Size([1, 3, 32000])\n"
     ]
    }
   ],
   "source": [
    "# class ExecutionTracer:\n",
    "#     def __init__(self):\n",
    "#         self.execution_order = []\n",
    "#         self.counter = 0\n",
    "    \n",
    "#     def __call__(self, module, input, output):\n",
    "#         self.counter += 1\n",
    "#         module_name = None\n",
    "#         for name, mod in model.named_modules():\n",
    "#             if mod is module:\n",
    "#                 module_name = name\n",
    "#                 break\n",
    "        \n",
    "#         self.execution_order.append({\n",
    "#             'order': self.counter,\n",
    "#             'name': module_name,\n",
    "#             'type': module.__class__.__name__,\n",
    "#             'input_shape': input[0].shape if isinstance(input, tuple) and len(input) > 0 else 'special',\n",
    "#             'output_shape': output.shape if hasattr(output, 'shape') else type(output).__name__\n",
    "#         })\n",
    "\n",
    "# # Create tracer and register hooks\n",
    "# tracer = ExecutionTracer()\n",
    "# hooks = []\n",
    "# for name, module in model.named_modules():\n",
    "#     # Skip container modules\n",
    "#     if len(list(module.children())) == 0:\n",
    "#         hooks.append(module.register_forward_hook(tracer))\n",
    "\n",
    "# # Run a forward pass\n",
    "# input_ids = tokenizer(\"Hello world\", return_tensors=\"pt\").input_ids\n",
    "# with torch.no_grad():\n",
    "#     output = model(input_ids)\n",
    "\n",
    "# # Print execution order\n",
    "# print(\"LAYER EXECUTION ORDER:\")\n",
    "# print(\"=\"*100)\n",
    "# for item in tracer.execution_order:\n",
    "#     print(f\"{item['order']:3d}. {item['name']:<50} | {item['type']:<20} | {item['input_shape']} → {item['output_shape']}\")\n",
    "\n",
    "# # Clean up hooks\n",
    "# for hook in hooks:\n",
    "#     hook.remove()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "activation-capture",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables for activation capture\n",
    "captured_activations = {}\n",
    "current_hooks = []\n",
    "\n",
    "def clear_activations():\n",
    "    global captured_activations\n",
    "    captured_activations.clear()\n",
    "    gc.collect()\n",
    "\n",
    "def remove_all_hooks():\n",
    "    global current_hooks\n",
    "    for hook in current_hooks:\n",
    "        try:\n",
    "            hook.remove()\n",
    "        except:\n",
    "            pass\n",
    "    current_hooks.clear()\n",
    "\n",
    "def get_activation_hook(name):\n",
    "    def hook(module, input, output):\n",
    "        try:\n",
    "            if output is None:\n",
    "                activation = None\n",
    "            elif isinstance(output, tuple):\n",
    "                activation = output[0]\n",
    "            else:\n",
    "                activation = output\n",
    "            \n",
    "            input_tensor = input[0] if isinstance(input, tuple) and len(input) > 0 else None\n",
    "            \n",
    "            def safe_detach_cpu(tensor):\n",
    "                if tensor is None:\n",
    "                    return None\n",
    "                try:\n",
    "                    if hasattr(tensor, 'device') and str(tensor.device) == 'meta':\n",
    "                        return None\n",
    "                    return tensor.detach().cpu()\n",
    "                except:\n",
    "                    return None\n",
    "            \n",
    "            captured_activations[name] = {\n",
    "                'output': safe_detach_cpu(activation),\n",
    "                'input': safe_detach_cpu(input_tensor),\n",
    "            }\n",
    "        except Exception as e:\n",
    "            captured_activations[name] = {'output': None, 'input': None}\n",
    "    return hook\n",
    "\n",
    "def register_final_norm_hook(model):\n",
    "    global current_hooks\n",
    "    remove_all_hooks()\n",
    "    current_hooks.append(model.model.norm.register_forward_hook(get_activation_hook(\"final_norm\")))\n",
    "\n",
    "def run_model_and_capture(model, inputs):\n",
    "    clear_activations()\n",
    "    register_final_norm_hook(model)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    remove_all_hooks()\n",
    "    return captured_activations.copy(), outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "score-computation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_swap_gradient(\n",
    "    z: torch.Tensor,\n",
    "    W: torch.Tensor,\n",
    "    top1_idx: int,\n",
    "    top2_idx: int,\n",
    "    norm_layer: nn.Module,\n",
    "    bias: Optional[torch.Tensor] = None\n",
    ") -> torch.Tensor:\n",
    "    z = z.clone().detach().requires_grad_(True)\n",
    "    \n",
    "    # Forward pass through RMSNorm\n",
    "    z_norm = norm_layer(z)\n",
    "    \n",
    "    # Compute logits\n",
    "    logits = F.linear(z_norm, W, bias)\n",
    "    \n",
    "    # Compute probabilities\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    \n",
    "    # Swap loss: minimize p[top1] - p[top2]\n",
    "    swap_loss = probs[top1_idx] - probs[top2_idx]\n",
    "    \n",
    "    # Backward pass\n",
    "    swap_loss.backward()\n",
    "    \n",
    "    return -z.grad.detach()\n",
    "\n",
    "\n",
    "def compute_neuron_scores(\n",
    "    gradient: torch.Tensor,\n",
    "    W: torch.Tensor\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    w_col_norms = torch.norm(W, dim=0)  # [hidden_size]\n",
    "    projections = gradient * w_col_norms  # [hidden_size]\n",
    "    scores = torch.abs(projections)\n",
    "    return scores, projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "data-extraction",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_z_values_and_scores(model, tokenizer, input_text: str, input_id: int) -> Dict[str, Any]:\n",
    "    # Tokenize input\n",
    "    sample_input = tokenizer(input_text, return_tensors=\"pt\")\n",
    "    inputs_on_device = {k: v.to(model.device) for k, v in sample_input.items()}\n",
    "    \n",
    "    # Run model and capture activations\n",
    "    activations, outputs = run_model_and_capture(model, inputs_on_device)\n",
    "    \n",
    "    # Get pre-norm activations (z values) - input to final_norm\n",
    "    pre_norm_activations = activations['final_norm']['input']\n",
    "    if pre_norm_activations is None:\n",
    "        pre_norm_activations = activations['final_norm']['output']\n",
    "    pre_norm_activations = pre_norm_activations.float()\n",
    "    \n",
    "    # Get z for last token position\n",
    "    seq_len = pre_norm_activations.shape[1]\n",
    "    last_token_pos = seq_len - 1\n",
    "    z = pre_norm_activations[0, last_token_pos, :]  # [hidden_size=4096]\n",
    "    \n",
    "    # Get model components\n",
    "    norm_layer = model.model.norm\n",
    "    W = model.lm_head.weight.detach().float()  # [vocab_size=32000, hidden_size=4096]\n",
    "    bias = model.lm_head.bias.detach().float() if hasattr(model.lm_head, 'bias') and model.lm_head.bias is not None else None\n",
    "    \n",
    "    # Compute original logits and probabilities\n",
    "    z_norm = norm_layer(z)\n",
    "    original_logits = F.linear(z_norm, W, bias)\n",
    "    original_probs = F.softmax(original_logits, dim=-1)\n",
    "    \n",
    "    # Get top-k predictions\n",
    "    k = 10\n",
    "    top_logits, top_indices = torch.topk(original_logits, k)\n",
    "    top_probs = original_probs[top_indices]\n",
    "    \n",
    "    top1_idx = top_indices[0].item()\n",
    "    top2_idx = top_indices[1].item()\n",
    "    \n",
    "    # Compute gradient and scores\n",
    "    gradient = compute_swap_gradient(z, W, top1_idx, top2_idx, norm_layer, bias)\n",
    "    scores, signed_projections = compute_neuron_scores(gradient, W)\n",
    "    \n",
    "    # Find special node (highest impact neuron)\n",
    "    special_node_idx = torch.argmax(scores).item()\n",
    "    special_node_score = scores[special_node_idx].item()\n",
    "    \n",
    "    # Prepare result\n",
    "    result = {\n",
    "        'input_id': input_id,\n",
    "        'input_text': input_text,\n",
    "        'seq_len': seq_len,\n",
    "        'hidden_size': z.shape[0],\n",
    "        \n",
    "        # Z values (pre-norm activations)\n",
    "        'z_values': z.cpu().numpy(),  # [4096]\n",
    "        \n",
    "        # Scores and projections\n",
    "        'scores': scores.cpu().numpy(),  # [4096]\n",
    "        'signed_projections': signed_projections.cpu().numpy(),  # [4096]\n",
    "        'gradient': gradient.cpu().numpy(),  # [4096]\n",
    "        \n",
    "        # Special node info\n",
    "        'special_node_idx': special_node_idx,\n",
    "        'special_node_score': special_node_score,\n",
    "        \n",
    "        # Top-k predictions\n",
    "        'top_k_indices': top_indices.cpu().numpy(),  # [k]\n",
    "        'top_k_logits': top_logits.detach().cpu().numpy(),  # [k]\n",
    "        'top_k_probs': top_probs.detach().cpu().numpy(),  # [k]\n",
    "        'top_k_tokens': [tokenizer.decode([idx]) for idx in top_indices.tolist()],\n",
    "        \n",
    "        # Full logits for analysis (optional - can be large)\n",
    "        'original_logits': original_logits.detach().cpu().numpy(),  # [vocab_size=32000]\n",
    "    }\n",
    "    \n",
    "    # Clean up\n",
    "    del activations, pre_norm_activations\n",
    "    clear_activations()\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "save-weights",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_lm_head_weights(model, output_dir: str):\n",
    "    W = model.lm_head.weight.detach().cpu().float().numpy()\n",
    "    np.save(os.path.join(output_dir, \"lm_head_weights.npy\"), W)\n",
    "    print(f\"Saved lm_head weights: shape={W.shape}\")\n",
    "    \n",
    "    # Save norm layer weight (gamma for RMSNorm)\n",
    "    norm_weight = model.model.norm.weight.detach().cpu().float().numpy()\n",
    "    np.save(os.path.join(output_dir, \"final_norm_weight.npy\"), norm_weight)\n",
    "    print(f\"Saved final_norm weight (gamma): shape={norm_weight.shape}\")\n",
    "    \n",
    "    # Save bias if it exists\n",
    "    if hasattr(model.lm_head, 'bias') and model.lm_head.bias is not None:\n",
    "        bias = model.lm_head.bias.detach().cpu().float().numpy()\n",
    "        np.save(os.path.join(output_dir, \"lm_head_bias.npy\"), bias)\n",
    "        print(f\"Saved lm_head bias: shape={bias.shape}\")\n",
    "    \n",
    "    # Save model config info\n",
    "    config_info = {\n",
    "        'hidden_size': model.config.hidden_size,\n",
    "        'vocab_size': model.config.vocab_size,\n",
    "        'num_hidden_layers': model.config.num_hidden_layers,\n",
    "        'rms_norm_eps': model.config.rms_norm_eps,\n",
    "    }\n",
    "    with open(os.path.join(output_dir, \"model_config.json\"), 'w') as f:\n",
    "        json.dump(config_info, f, indent=2)\n",
    "    print(f\"Saved model config: {config_info}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "save-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_input_data(result: Dict[str, Any], output_dir: str):\n",
    "\n",
    "    input_id = result['input_id']\n",
    "    input_dir = os.path.join(output_dir, f\"input_{input_id}\")\n",
    "    os.makedirs(input_dir, exist_ok=True)\n",
    "    \n",
    "    # Save z values\n",
    "    np.save(os.path.join(input_dir, \"z_values.npy\"), result['z_values'])\n",
    "    \n",
    "    # Save scores and projections\n",
    "    np.save(os.path.join(input_dir, \"scores.npy\"), result['scores'])\n",
    "    np.save(os.path.join(input_dir, \"signed_projections.npy\"), result['signed_projections'])\n",
    "    np.save(os.path.join(input_dir, \"gradient.npy\"), result['gradient'])\n",
    "    \n",
    "    # Save top-k predictions\n",
    "    np.save(os.path.join(input_dir, \"top_k_indices.npy\"), result['top_k_indices'])\n",
    "    np.save(os.path.join(input_dir, \"top_k_logits.npy\"), result['top_k_logits'])\n",
    "    np.save(os.path.join(input_dir, \"top_k_probs.npy\"), result['top_k_probs'])\n",
    "    \n",
    "    # Save original logits (full vocab)\n",
    "    np.save(os.path.join(input_dir, \"original_logits.npy\"), result['original_logits'])\n",
    "    \n",
    "    # Save metadata as JSON\n",
    "    metadata = {\n",
    "        'input_id': result['input_id'],\n",
    "        'input_text': result['input_text'],\n",
    "        'seq_len': result['seq_len'],\n",
    "        'hidden_size': result['hidden_size'],\n",
    "        'special_node_idx': result['special_node_idx'],\n",
    "        'special_node_score': float(result['special_node_score']),\n",
    "        'top_k_tokens': result['top_k_tokens'],\n",
    "        'top_k_probs': [float(p) for p in result['top_k_probs']],\n",
    "    }\n",
    "    with open(os.path.join(input_dir, \"metadata.json\"), 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    return input_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "sample-inputs",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample texts for analysis\n",
    "sample_texts = [\n",
    "    [1, \"The capital of France is\"],\n",
    "    [2, \"The largest mammal on Earth is\"],\n",
    "    [3, \"The process of photosynthesis occurs in\"],\n",
    "    [4, \"The speed of light in a vacuum is\"],\n",
    "    [5, \"The chemical symbol for gold is\"],\n",
    "    [6, \"The human body has how many bones\"],\n",
    "    [7, \"The Great Wall of China was built to\"],\n",
    "    [8, \"Water boils at what temperature\"],\n",
    "    [9, \"The smallest unit of matter is\"],\n",
    "    [10, \"Shakespeare wrote the play\"],\n",
    "    [11, \"The currency of Japan is\"],\n",
    "    [12, \"Mount Everest is located in\"],\n",
    "    [13, \"The inventor of the telephone was\"],\n",
    "    [14, \"DNA stands for\"],\n",
    "    [15, \"The largest ocean on Earth is\"],\n",
    "    [16, \"The planet closest to the Sun is\"],\n",
    "    [17, \"Gravity was discovered by\"],\n",
    "    [18, \"The Amazon rainforest is primarily located in\"],\n",
    "    [19, \"The freezing point of water is\"],\n",
    "    [20, \"The most abundant gas in Earth's atmosphere is\"],\n",
    "    [21, \"The Mona Lisa was painted by\"],\n",
    "    [22, \"The longest river in the world is\"],\n",
    "    [23, \"Photosynthesis converts carbon dioxide and water into\"],\n",
    "    [24, \"The study of earthquakes is called\"],\n",
    "    [25, \"The first person to walk on the moon was\"]\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "save-model-weights",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model weights for offline simulation...\n",
      "Saved lm_head weights: shape=(32000, 4096)\n",
      "Saved final_norm weight (gamma): shape=(4096,)\n",
      "Saved model config: {'hidden_size': 4096, 'vocab_size': 32000, 'num_hidden_layers': 32, 'rms_norm_eps': 1e-05}\n",
      "\n",
      "Model weights saved!\n"
     ]
    }
   ],
   "source": [
    "# Save lm_head weights and norm weights (only need to do this once)\n",
    "print(\"Saving model weights for offline simulation...\")\n",
    "save_lm_head_weights(model, OUTPUT_DIR)\n",
    "print(\"\\nModel weights saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "run-extraction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Z-VALUE AND SCORE EXTRACTION\n",
      "================================================================================\n",
      "\n",
      "Processing 1/25: Input ID=1\n",
      "  Text: 'The capital of France is'\n",
      "  Special node: idx=3556, score=0.0408\n",
      "  Top-1: 'Paris' (p=0.8931)\n",
      "  Top-2: 'a' (p=0.0345)\n",
      "  Saved to: ./scraped_data/input_1\n",
      "\n",
      "Processing 2/25: Input ID=2\n",
      "  Text: 'The largest mammal on Earth is'\n",
      "  Special node: idx=2617, score=0.0132\n",
      "  Top-1: 'the' (p=0.9200)\n",
      "  Top-2: 'not' (p=0.0289)\n",
      "  Saved to: ./scraped_data/input_2\n",
      "\n",
      "Processing 3/25: Input ID=3\n",
      "  Text: 'The process of photosynthesis occurs in'\n",
      "  Special node: idx=1573, score=0.1018\n",
      "  Top-1: 'the' (p=0.7385)\n",
      "  Top-2: 'special' (p=0.0972)\n",
      "  Saved to: ./scraped_data/input_3\n",
      "\n",
      "Processing 4/25: Input ID=4\n",
      "  Text: 'The speed of light in a vacuum is'\n",
      "  Special node: idx=1839, score=0.7574\n",
      "  Top-1: 'approximately' (p=0.4901)\n",
      "  Top-2: '' (p=0.3251)\n",
      "  Saved to: ./scraped_data/input_4\n",
      "\n",
      "Processing 5/25: Input ID=5\n",
      "  Text: 'The chemical symbol for gold is'\n",
      "  Special node: idx=3556, score=0.0039\n",
      "  Top-1: 'Au' (p=0.9913)\n",
      "  Top-2: 'A' (p=0.0016)\n",
      "  Saved to: ./scraped_data/input_5\n",
      "\n",
      "Processing 6/25: Input ID=6\n",
      "  Text: 'The human body has how many bones'\n",
      "  Special node: idx=1360, score=0.0345\n",
      "  Top-1: '?' (p=0.9063)\n",
      "  Top-2: 'in' (p=0.0346)\n",
      "  Saved to: ./scraped_data/input_6\n",
      "\n",
      "Processing 7/25: Input ID=7\n",
      "  Text: 'The Great Wall of China was built to'\n",
      "  Special node: idx=1415, score=0.0904\n",
      "  Top-1: 'protect' (p=0.5323)\n",
      "  Top-2: 'keep' (p=0.3652)\n",
      "  Saved to: ./scraped_data/input_7\n",
      "\n",
      "Processing 8/25: Input ID=8\n",
      "  Text: 'Water boils at what temperature'\n",
      "  Special node: idx=1360, score=0.0168\n",
      "  Top-1: '?' (p=0.9294)\n",
      "  Top-2: 'and' (p=0.0234)\n",
      "  Saved to: ./scraped_data/input_8\n",
      "\n",
      "Processing 9/25: Input ID=9\n",
      "  Text: 'The smallest unit of matter is'\n",
      "  Special node: idx=3556, score=0.0606\n",
      "  Top-1: 'an' (p=0.4757)\n",
      "  Top-2: 'the' (p=0.2956)\n",
      "  Saved to: ./scraped_data/input_9\n",
      "\n",
      "Processing 10/25: Input ID=10\n",
      "  Text: 'Shakespeare wrote the play'\n",
      "  Special node: idx=1360, score=0.0747\n",
      "  Top-1: 'in' (p=0.5088)\n",
      "  Top-2: '\"' (p=0.0856)\n",
      "  Saved to: ./scraped_data/input_10\n",
      "\n",
      "Processing 11/25: Input ID=11\n",
      "  Text: 'The currency of Japan is'\n",
      "  Special node: idx=1360, score=0.0447\n",
      "  Top-1: 'the' (p=0.9112)\n",
      "  Top-2: 'called' (p=0.0449)\n",
      "  Saved to: ./scraped_data/input_11\n",
      "\n",
      "Processing 12/25: Input ID=12\n",
      "  Text: 'Mount Everest is located in'\n",
      "  Special node: idx=1839, score=0.0838\n",
      "  Top-1: 'the' (p=0.7317)\n",
      "  Top-2: 'which' (p=0.1518)\n",
      "  Saved to: ./scraped_data/input_12\n",
      "\n",
      "Processing 13/25: Input ID=13\n",
      "  Text: 'The inventor of the telephone was'\n",
      "  Special node: idx=3556, score=0.0637\n",
      "  Top-1: 'Alexander' (p=0.8651)\n",
      "  Top-2: 'not' (p=0.0343)\n",
      "  Saved to: ./scraped_data/input_13\n",
      "\n",
      "Processing 14/25: Input ID=14\n",
      "  Text: 'DNA stands for'\n",
      "  Special node: idx=2021, score=0.0842\n",
      "  Top-1: 'de' (p=0.7526)\n",
      "  Top-2: 'De' (p=0.2103)\n",
      "  Saved to: ./scraped_data/input_14\n",
      "\n",
      "Processing 15/25: Input ID=15\n",
      "  Text: 'The largest ocean on Earth is'\n",
      "  Special node: idx=1360, score=0.0024\n",
      "  Top-1: 'the' (p=0.9918)\n",
      "  Top-2: ':' (p=0.0009)\n",
      "  Saved to: ./scraped_data/input_15\n",
      "\n",
      "Processing 16/25: Input ID=16\n",
      "  Text: 'The planet closest to the Sun is'\n",
      "  Special node: idx=3556, score=0.0844\n",
      "  Top-1: 'Mercur' (p=0.9036)\n",
      "  Top-2: 'called' (p=0.0525)\n",
      "  Saved to: ./scraped_data/input_16\n",
      "\n",
      "Processing 17/25: Input ID=17\n",
      "  Text: 'Gravity was discovered by'\n",
      "  Special node: idx=1573, score=0.0552\n",
      "  Top-1: 'Isaac' (p=0.2509)\n",
      "  Top-2: 'Sir' (p=0.1694)\n",
      "  Saved to: ./scraped_data/input_17\n",
      "\n",
      "Processing 18/25: Input ID=18\n",
      "  Text: 'The Amazon rainforest is primarily located in'\n",
      "  Special node: idx=3556, score=0.1975\n",
      "  Top-1: 'which' (p=0.5527)\n",
      "  Top-2: 'Brazil' (p=0.2700)\n",
      "  Saved to: ./scraped_data/input_18\n",
      "\n",
      "Processing 19/25: Input ID=19\n",
      "  Text: 'The freezing point of water is'\n",
      "  Special node: idx=1839, score=0.1600\n",
      "  Top-1: '' (p=0.9062)\n",
      "  Top-2: '$' (p=0.0179)\n",
      "  Saved to: ./scraped_data/input_19\n",
      "\n",
      "Processing 20/25: Input ID=20\n",
      "  Text: 'The most abundant gas in Earth's atmosphere is'\n",
      "  Special node: idx=3556, score=0.0277\n",
      "  Top-1: 'nit' (p=0.9494)\n",
      "  Top-2: '_' (p=0.0176)\n",
      "  Saved to: ./scraped_data/input_20\n",
      "\n",
      "Processing 21/25: Input ID=21\n",
      "  Text: 'The Mona Lisa was painted by'\n",
      "  Special node: idx=3556, score=0.0099\n",
      "  Top-1: 'Leon' (p=0.9769)\n",
      "  Top-2: 'the' (p=0.0096)\n",
      "  Saved to: ./scraped_data/input_21\n",
      "\n",
      "Processing 22/25: Input ID=22\n",
      "  Text: 'The longest river in the world is'\n",
      "  Special node: idx=3241, score=0.0076\n",
      "  Top-1: 'the' (p=0.9685)\n",
      "  Top-2: 'not' (p=0.0052)\n",
      "  Saved to: ./scraped_data/input_22\n",
      "\n",
      "Processing 23/25: Input ID=23\n",
      "  Text: 'Photosynthesis converts carbon dioxide and water into'\n",
      "  Special node: idx=633, score=0.0522\n",
      "  Top-1: 'gl' (p=0.7724)\n",
      "  Top-2: 'organ' (p=0.0786)\n",
      "  Saved to: ./scraped_data/input_23\n",
      "\n",
      "Processing 24/25: Input ID=24\n",
      "  Text: 'The study of earthquakes is called'\n",
      "  Special node: idx=2084, score=0.0003\n",
      "  Top-1: 'se' (p=0.9991)\n",
      "  Top-2: 'Se' (p=0.0007)\n",
      "  Saved to: ./scraped_data/input_24\n",
      "\n",
      "Processing 25/25: Input ID=25\n",
      "  Text: 'The first person to walk on the moon was'\n",
      "  Special node: idx=1573, score=0.0296\n",
      "  Top-1: 'Neil' (p=0.9106)\n",
      "  Top-2: 'Ed' (p=0.0139)\n",
      "  Saved to: ./scraped_data/input_25\n",
      "\n",
      "================================================================================\n",
      "Extraction complete! Summary saved to ./scraped_data/summary.csv\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Run extraction for all inputs\n",
    "print(\"=\"*80)\n",
    "print(\"Z-VALUE AND SCORE EXTRACTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "all_results = []\n",
    "summary_records = []\n",
    "\n",
    "for idx, (input_id, input_text) in enumerate(sample_texts):\n",
    "    print(f\"\\nProcessing {idx+1}/{len(sample_texts)}: Input ID={input_id}\")\n",
    "    print(f\"  Text: '{input_text}'\")\n",
    "    \n",
    "    # Extract z-values and scores\n",
    "    result = extract_z_values_and_scores(model, tokenizer, input_text, input_id)\n",
    "    \n",
    "    # Save to files\n",
    "    save_path = save_input_data(result, OUTPUT_DIR)\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"  Special node: idx={result['special_node_idx']}, score={result['special_node_score']:.4f}\")\n",
    "    print(f\"  Top-1: '{result['top_k_tokens'][0]}' (p={result['top_k_probs'][0]:.4f})\")\n",
    "    print(f\"  Top-2: '{result['top_k_tokens'][1]}' (p={result['top_k_probs'][1]:.4f})\")\n",
    "    print(f\"  Saved to: {save_path}\")\n",
    "    \n",
    "    # Collect summary for CSV\n",
    "    summary_records.append({\n",
    "        'input_id': input_id,\n",
    "        'input_text': input_text,\n",
    "        'special_node_idx': result['special_node_idx'],\n",
    "        'special_node_score': result['special_node_score'],\n",
    "        'z_mean': float(np.mean(result['z_values'])),\n",
    "        'z_std': float(np.std(result['z_values'])),\n",
    "        'z_min': float(np.min(result['z_values'])),\n",
    "        'z_max': float(np.max(result['z_values'])),\n",
    "        'score_mean': float(np.mean(result['scores'])),\n",
    "        'score_std': float(np.std(result['scores'])),\n",
    "        'score_max': float(np.max(result['scores'])),\n",
    "        'top1_token': result['top_k_tokens'][0],\n",
    "        'top1_prob': float(result['top_k_probs'][0]),\n",
    "        'top2_token': result['top_k_tokens'][1],\n",
    "        'top2_prob': float(result['top_k_probs'][1]),\n",
    "        'top3_token': result['top_k_tokens'][2],\n",
    "        'top3_prob': float(result['top_k_probs'][2]),\n",
    "    })\n",
    "    \n",
    "    all_results.append(result)\n",
    "\n",
    "# Save summary CSV\n",
    "summary_df = pd.DataFrame(summary_records)\n",
    "summary_df.to_csv(os.path.join(OUTPUT_DIR, \"summary.csv\"), index=False)\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(f\"Extraction complete! Summary saved to {os.path.join(OUTPUT_DIR, 'summary.csv')}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "display-summary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SUMMARY STATISTICS\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_id</th>\n",
       "      <th>input_text</th>\n",
       "      <th>special_node_idx</th>\n",
       "      <th>special_node_score</th>\n",
       "      <th>z_mean</th>\n",
       "      <th>z_std</th>\n",
       "      <th>z_min</th>\n",
       "      <th>z_max</th>\n",
       "      <th>score_mean</th>\n",
       "      <th>score_std</th>\n",
       "      <th>score_max</th>\n",
       "      <th>top1_token</th>\n",
       "      <th>top1_prob</th>\n",
       "      <th>top2_token</th>\n",
       "      <th>top2_prob</th>\n",
       "      <th>top3_token</th>\n",
       "      <th>top3_prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>The capital of France is</td>\n",
       "      <td>3556</td>\n",
       "      <td>0.040837</td>\n",
       "      <td>0.055260</td>\n",
       "      <td>2.245154</td>\n",
       "      <td>-12.953125</td>\n",
       "      <td>39.656250</td>\n",
       "      <td>0.004006</td>\n",
       "      <td>0.003270</td>\n",
       "      <td>0.040837</td>\n",
       "      <td>Paris</td>\n",
       "      <td>0.893063</td>\n",
       "      <td>a</td>\n",
       "      <td>0.034541</td>\n",
       "      <td>known</td>\n",
       "      <td>0.009685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>The largest mammal on Earth is</td>\n",
       "      <td>2617</td>\n",
       "      <td>0.013166</td>\n",
       "      <td>-0.025065</td>\n",
       "      <td>2.027116</td>\n",
       "      <td>-17.750000</td>\n",
       "      <td>16.859375</td>\n",
       "      <td>0.002380</td>\n",
       "      <td>0.001803</td>\n",
       "      <td>0.013166</td>\n",
       "      <td>the</td>\n",
       "      <td>0.920029</td>\n",
       "      <td>not</td>\n",
       "      <td>0.028910</td>\n",
       "      <td>a</td>\n",
       "      <td>0.015437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>The process of photosynthesis occurs in</td>\n",
       "      <td>1573</td>\n",
       "      <td>0.101778</td>\n",
       "      <td>0.045302</td>\n",
       "      <td>1.596781</td>\n",
       "      <td>-6.320312</td>\n",
       "      <td>7.074219</td>\n",
       "      <td>0.010161</td>\n",
       "      <td>0.007873</td>\n",
       "      <td>0.101778</td>\n",
       "      <td>the</td>\n",
       "      <td>0.738508</td>\n",
       "      <td>special</td>\n",
       "      <td>0.097212</td>\n",
       "      <td>two</td>\n",
       "      <td>0.049857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>The speed of light in a vacuum is</td>\n",
       "      <td>1839</td>\n",
       "      <td>0.757371</td>\n",
       "      <td>0.039324</td>\n",
       "      <td>1.948638</td>\n",
       "      <td>-8.351562</td>\n",
       "      <td>10.031250</td>\n",
       "      <td>0.014695</td>\n",
       "      <td>0.022300</td>\n",
       "      <td>0.757371</td>\n",
       "      <td>approximately</td>\n",
       "      <td>0.490121</td>\n",
       "      <td></td>\n",
       "      <td>0.325054</td>\n",
       "      <td>a</td>\n",
       "      <td>0.119420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>The chemical symbol for gold is</td>\n",
       "      <td>3556</td>\n",
       "      <td>0.003867</td>\n",
       "      <td>0.016147</td>\n",
       "      <td>1.911584</td>\n",
       "      <td>-12.664062</td>\n",
       "      <td>16.750000</td>\n",
       "      <td>0.000439</td>\n",
       "      <td>0.000352</td>\n",
       "      <td>0.003867</td>\n",
       "      <td>Au</td>\n",
       "      <td>0.991307</td>\n",
       "      <td>A</td>\n",
       "      <td>0.001565</td>\n",
       "      <td>Ag</td>\n",
       "      <td>0.001359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>The human body has how many bones</td>\n",
       "      <td>1360</td>\n",
       "      <td>0.034450</td>\n",
       "      <td>-0.013203</td>\n",
       "      <td>1.807928</td>\n",
       "      <td>-13.453125</td>\n",
       "      <td>15.773438</td>\n",
       "      <td>0.003459</td>\n",
       "      <td>0.002779</td>\n",
       "      <td>0.034450</td>\n",
       "      <td>?</td>\n",
       "      <td>0.906257</td>\n",
       "      <td>in</td>\n",
       "      <td>0.034638</td>\n",
       "      <td>\\n</td>\n",
       "      <td>0.022375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>The Great Wall of China was built to</td>\n",
       "      <td>1415</td>\n",
       "      <td>0.090388</td>\n",
       "      <td>0.058531</td>\n",
       "      <td>1.943249</td>\n",
       "      <td>-7.703125</td>\n",
       "      <td>23.875000</td>\n",
       "      <td>0.016591</td>\n",
       "      <td>0.012910</td>\n",
       "      <td>0.090388</td>\n",
       "      <td>protect</td>\n",
       "      <td>0.532271</td>\n",
       "      <td>keep</td>\n",
       "      <td>0.365223</td>\n",
       "      <td>prevent</td>\n",
       "      <td>0.022241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Water boils at what temperature</td>\n",
       "      <td>1360</td>\n",
       "      <td>0.016751</td>\n",
       "      <td>-0.013873</td>\n",
       "      <td>1.711079</td>\n",
       "      <td>-15.875000</td>\n",
       "      <td>16.218750</td>\n",
       "      <td>0.002723</td>\n",
       "      <td>0.002143</td>\n",
       "      <td>0.016751</td>\n",
       "      <td>?</td>\n",
       "      <td>0.929356</td>\n",
       "      <td>and</td>\n",
       "      <td>0.023401</td>\n",
       "      <td>,</td>\n",
       "      <td>0.014163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>The smallest unit of matter is</td>\n",
       "      <td>3556</td>\n",
       "      <td>0.060568</td>\n",
       "      <td>0.022158</td>\n",
       "      <td>1.936282</td>\n",
       "      <td>-13.984375</td>\n",
       "      <td>16.156250</td>\n",
       "      <td>0.010735</td>\n",
       "      <td>0.008099</td>\n",
       "      <td>0.060568</td>\n",
       "      <td>an</td>\n",
       "      <td>0.475661</td>\n",
       "      <td>the</td>\n",
       "      <td>0.295584</td>\n",
       "      <td>a</td>\n",
       "      <td>0.073514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>Shakespeare wrote the play</td>\n",
       "      <td>1360</td>\n",
       "      <td>0.074688</td>\n",
       "      <td>0.032709</td>\n",
       "      <td>2.099957</td>\n",
       "      <td>-15.406250</td>\n",
       "      <td>13.406250</td>\n",
       "      <td>0.006811</td>\n",
       "      <td>0.005233</td>\n",
       "      <td>0.074688</td>\n",
       "      <td>in</td>\n",
       "      <td>0.508770</td>\n",
       "      <td>\"</td>\n",
       "      <td>0.085625</td>\n",
       "      <td>to</td>\n",
       "      <td>0.051917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>The currency of Japan is</td>\n",
       "      <td>1360</td>\n",
       "      <td>0.044719</td>\n",
       "      <td>0.026180</td>\n",
       "      <td>2.184387</td>\n",
       "      <td>-22.937500</td>\n",
       "      <td>25.312500</td>\n",
       "      <td>0.003749</td>\n",
       "      <td>0.002979</td>\n",
       "      <td>0.044719</td>\n",
       "      <td>the</td>\n",
       "      <td>0.911187</td>\n",
       "      <td>called</td>\n",
       "      <td>0.044939</td>\n",
       "      <td>known</td>\n",
       "      <td>0.035346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>Mount Everest is located in</td>\n",
       "      <td>1839</td>\n",
       "      <td>0.083838</td>\n",
       "      <td>0.026035</td>\n",
       "      <td>1.876765</td>\n",
       "      <td>-12.015625</td>\n",
       "      <td>12.898438</td>\n",
       "      <td>0.009908</td>\n",
       "      <td>0.007944</td>\n",
       "      <td>0.083838</td>\n",
       "      <td>the</td>\n",
       "      <td>0.731712</td>\n",
       "      <td>which</td>\n",
       "      <td>0.151779</td>\n",
       "      <td>Nep</td>\n",
       "      <td>0.097856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>The inventor of the telephone was</td>\n",
       "      <td>3556</td>\n",
       "      <td>0.063674</td>\n",
       "      <td>0.026467</td>\n",
       "      <td>1.962463</td>\n",
       "      <td>-9.625000</td>\n",
       "      <td>13.359375</td>\n",
       "      <td>0.005470</td>\n",
       "      <td>0.004424</td>\n",
       "      <td>0.063674</td>\n",
       "      <td>Alexander</td>\n",
       "      <td>0.865098</td>\n",
       "      <td>not</td>\n",
       "      <td>0.034324</td>\n",
       "      <td>a</td>\n",
       "      <td>0.021743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>DNA stands for</td>\n",
       "      <td>2021</td>\n",
       "      <td>0.084159</td>\n",
       "      <td>-0.005249</td>\n",
       "      <td>2.343570</td>\n",
       "      <td>-17.937500</td>\n",
       "      <td>41.250000</td>\n",
       "      <td>0.010392</td>\n",
       "      <td>0.007978</td>\n",
       "      <td>0.084159</td>\n",
       "      <td>de</td>\n",
       "      <td>0.752621</td>\n",
       "      <td>De</td>\n",
       "      <td>0.210312</td>\n",
       "      <td>\"</td>\n",
       "      <td>0.007661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>The largest ocean on Earth is</td>\n",
       "      <td>1360</td>\n",
       "      <td>0.002427</td>\n",
       "      <td>-0.019772</td>\n",
       "      <td>1.848187</td>\n",
       "      <td>-15.867188</td>\n",
       "      <td>11.695312</td>\n",
       "      <td>0.000224</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>0.002427</td>\n",
       "      <td>the</td>\n",
       "      <td>0.991793</td>\n",
       "      <td>:</td>\n",
       "      <td>0.000901</td>\n",
       "      <td>not</td>\n",
       "      <td>0.000816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>The planet closest to the Sun is</td>\n",
       "      <td>3556</td>\n",
       "      <td>0.084415</td>\n",
       "      <td>0.033811</td>\n",
       "      <td>1.824466</td>\n",
       "      <td>-8.906250</td>\n",
       "      <td>29.500000</td>\n",
       "      <td>0.006470</td>\n",
       "      <td>0.005124</td>\n",
       "      <td>0.084415</td>\n",
       "      <td>Mercur</td>\n",
       "      <td>0.903643</td>\n",
       "      <td>called</td>\n",
       "      <td>0.052473</td>\n",
       "      <td>known</td>\n",
       "      <td>0.005445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>Gravity was discovered by</td>\n",
       "      <td>1573</td>\n",
       "      <td>0.055212</td>\n",
       "      <td>0.002903</td>\n",
       "      <td>1.844908</td>\n",
       "      <td>-6.687500</td>\n",
       "      <td>49.375000</td>\n",
       "      <td>0.010889</td>\n",
       "      <td>0.008512</td>\n",
       "      <td>0.055212</td>\n",
       "      <td>Isaac</td>\n",
       "      <td>0.250890</td>\n",
       "      <td>Sir</td>\n",
       "      <td>0.169364</td>\n",
       "      <td>Gal</td>\n",
       "      <td>0.076052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>The Amazon rainforest is primarily located in</td>\n",
       "      <td>3556</td>\n",
       "      <td>0.197476</td>\n",
       "      <td>0.014813</td>\n",
       "      <td>1.492186</td>\n",
       "      <td>-7.156250</td>\n",
       "      <td>15.281250</td>\n",
       "      <td>0.020464</td>\n",
       "      <td>0.016246</td>\n",
       "      <td>0.197476</td>\n",
       "      <td>which</td>\n",
       "      <td>0.552697</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>0.270022</td>\n",
       "      <td>:</td>\n",
       "      <td>0.088430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>The freezing point of water is</td>\n",
       "      <td>1839</td>\n",
       "      <td>0.159981</td>\n",
       "      <td>0.016135</td>\n",
       "      <td>2.109716</td>\n",
       "      <td>-13.218750</td>\n",
       "      <td>14.109375</td>\n",
       "      <td>0.002350</td>\n",
       "      <td>0.004648</td>\n",
       "      <td>0.159981</td>\n",
       "      <td></td>\n",
       "      <td>0.906187</td>\n",
       "      <td>$</td>\n",
       "      <td>0.017859</td>\n",
       "      <td>approximately</td>\n",
       "      <td>0.016498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>The most abundant gas in Earth's atmosphere is</td>\n",
       "      <td>3556</td>\n",
       "      <td>0.027672</td>\n",
       "      <td>-0.002131</td>\n",
       "      <td>1.910228</td>\n",
       "      <td>-15.242188</td>\n",
       "      <td>13.109375</td>\n",
       "      <td>0.002914</td>\n",
       "      <td>0.002386</td>\n",
       "      <td>0.027672</td>\n",
       "      <td>nit</td>\n",
       "      <td>0.949443</td>\n",
       "      <td>_</td>\n",
       "      <td>0.017627</td>\n",
       "      <td>N</td>\n",
       "      <td>0.006643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>The Mona Lisa was painted by</td>\n",
       "      <td>3556</td>\n",
       "      <td>0.009859</td>\n",
       "      <td>-0.012766</td>\n",
       "      <td>2.407014</td>\n",
       "      <td>-17.953125</td>\n",
       "      <td>63.625000</td>\n",
       "      <td>0.001103</td>\n",
       "      <td>0.000863</td>\n",
       "      <td>0.009859</td>\n",
       "      <td>Leon</td>\n",
       "      <td>0.976904</td>\n",
       "      <td>the</td>\n",
       "      <td>0.009633</td>\n",
       "      <td>:</td>\n",
       "      <td>0.002185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>The longest river in the world is</td>\n",
       "      <td>3241</td>\n",
       "      <td>0.007562</td>\n",
       "      <td>-0.002416</td>\n",
       "      <td>2.030704</td>\n",
       "      <td>-16.671875</td>\n",
       "      <td>14.921875</td>\n",
       "      <td>0.000802</td>\n",
       "      <td>0.000619</td>\n",
       "      <td>0.007562</td>\n",
       "      <td>the</td>\n",
       "      <td>0.968515</td>\n",
       "      <td>not</td>\n",
       "      <td>0.005248</td>\n",
       "      <td>also</td>\n",
       "      <td>0.002513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>Photosynthesis converts carbon dioxide and wat...</td>\n",
       "      <td>633</td>\n",
       "      <td>0.052217</td>\n",
       "      <td>0.037673</td>\n",
       "      <td>1.832946</td>\n",
       "      <td>-8.656250</td>\n",
       "      <td>31.484375</td>\n",
       "      <td>0.011201</td>\n",
       "      <td>0.008447</td>\n",
       "      <td>0.052217</td>\n",
       "      <td>gl</td>\n",
       "      <td>0.772370</td>\n",
       "      <td>organ</td>\n",
       "      <td>0.078580</td>\n",
       "      <td>o</td>\n",
       "      <td>0.040469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>The study of earthquakes is called</td>\n",
       "      <td>2084</td>\n",
       "      <td>0.000336</td>\n",
       "      <td>0.022596</td>\n",
       "      <td>1.733691</td>\n",
       "      <td>-6.679688</td>\n",
       "      <td>9.671875</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000336</td>\n",
       "      <td>se</td>\n",
       "      <td>0.999115</td>\n",
       "      <td>Se</td>\n",
       "      <td>0.000687</td>\n",
       "      <td>ge</td>\n",
       "      <td>0.000106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>The first person to walk on the moon was</td>\n",
       "      <td>1573</td>\n",
       "      <td>0.029619</td>\n",
       "      <td>0.044774</td>\n",
       "      <td>2.031205</td>\n",
       "      <td>-13.359375</td>\n",
       "      <td>46.187500</td>\n",
       "      <td>0.003299</td>\n",
       "      <td>0.002711</td>\n",
       "      <td>0.029619</td>\n",
       "      <td>Neil</td>\n",
       "      <td>0.910562</td>\n",
       "      <td>Ed</td>\n",
       "      <td>0.013909</td>\n",
       "      <td>\\n</td>\n",
       "      <td>0.010841</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    input_id                                         input_text  \\\n",
       "0          1                           The capital of France is   \n",
       "1          2                     The largest mammal on Earth is   \n",
       "2          3            The process of photosynthesis occurs in   \n",
       "3          4                  The speed of light in a vacuum is   \n",
       "4          5                    The chemical symbol for gold is   \n",
       "5          6                  The human body has how many bones   \n",
       "6          7               The Great Wall of China was built to   \n",
       "7          8                    Water boils at what temperature   \n",
       "8          9                     The smallest unit of matter is   \n",
       "9         10                         Shakespeare wrote the play   \n",
       "10        11                           The currency of Japan is   \n",
       "11        12                        Mount Everest is located in   \n",
       "12        13                  The inventor of the telephone was   \n",
       "13        14                                     DNA stands for   \n",
       "14        15                      The largest ocean on Earth is   \n",
       "15        16                   The planet closest to the Sun is   \n",
       "16        17                          Gravity was discovered by   \n",
       "17        18      The Amazon rainforest is primarily located in   \n",
       "18        19                     The freezing point of water is   \n",
       "19        20     The most abundant gas in Earth's atmosphere is   \n",
       "20        21                       The Mona Lisa was painted by   \n",
       "21        22                  The longest river in the world is   \n",
       "22        23  Photosynthesis converts carbon dioxide and wat...   \n",
       "23        24                 The study of earthquakes is called   \n",
       "24        25           The first person to walk on the moon was   \n",
       "\n",
       "    special_node_idx  special_node_score    z_mean     z_std      z_min  \\\n",
       "0               3556            0.040837  0.055260  2.245154 -12.953125   \n",
       "1               2617            0.013166 -0.025065  2.027116 -17.750000   \n",
       "2               1573            0.101778  0.045302  1.596781  -6.320312   \n",
       "3               1839            0.757371  0.039324  1.948638  -8.351562   \n",
       "4               3556            0.003867  0.016147  1.911584 -12.664062   \n",
       "5               1360            0.034450 -0.013203  1.807928 -13.453125   \n",
       "6               1415            0.090388  0.058531  1.943249  -7.703125   \n",
       "7               1360            0.016751 -0.013873  1.711079 -15.875000   \n",
       "8               3556            0.060568  0.022158  1.936282 -13.984375   \n",
       "9               1360            0.074688  0.032709  2.099957 -15.406250   \n",
       "10              1360            0.044719  0.026180  2.184387 -22.937500   \n",
       "11              1839            0.083838  0.026035  1.876765 -12.015625   \n",
       "12              3556            0.063674  0.026467  1.962463  -9.625000   \n",
       "13              2021            0.084159 -0.005249  2.343570 -17.937500   \n",
       "14              1360            0.002427 -0.019772  1.848187 -15.867188   \n",
       "15              3556            0.084415  0.033811  1.824466  -8.906250   \n",
       "16              1573            0.055212  0.002903  1.844908  -6.687500   \n",
       "17              3556            0.197476  0.014813  1.492186  -7.156250   \n",
       "18              1839            0.159981  0.016135  2.109716 -13.218750   \n",
       "19              3556            0.027672 -0.002131  1.910228 -15.242188   \n",
       "20              3556            0.009859 -0.012766  2.407014 -17.953125   \n",
       "21              3241            0.007562 -0.002416  2.030704 -16.671875   \n",
       "22               633            0.052217  0.037673  1.832946  -8.656250   \n",
       "23              2084            0.000336  0.022596  1.733691  -6.679688   \n",
       "24              1573            0.029619  0.044774  2.031205 -13.359375   \n",
       "\n",
       "        z_max  score_mean  score_std  score_max     top1_token  top1_prob  \\\n",
       "0   39.656250    0.004006   0.003270   0.040837          Paris   0.893063   \n",
       "1   16.859375    0.002380   0.001803   0.013166            the   0.920029   \n",
       "2    7.074219    0.010161   0.007873   0.101778            the   0.738508   \n",
       "3   10.031250    0.014695   0.022300   0.757371  approximately   0.490121   \n",
       "4   16.750000    0.000439   0.000352   0.003867             Au   0.991307   \n",
       "5   15.773438    0.003459   0.002779   0.034450              ?   0.906257   \n",
       "6   23.875000    0.016591   0.012910   0.090388        protect   0.532271   \n",
       "7   16.218750    0.002723   0.002143   0.016751              ?   0.929356   \n",
       "8   16.156250    0.010735   0.008099   0.060568             an   0.475661   \n",
       "9   13.406250    0.006811   0.005233   0.074688             in   0.508770   \n",
       "10  25.312500    0.003749   0.002979   0.044719            the   0.911187   \n",
       "11  12.898438    0.009908   0.007944   0.083838            the   0.731712   \n",
       "12  13.359375    0.005470   0.004424   0.063674      Alexander   0.865098   \n",
       "13  41.250000    0.010392   0.007978   0.084159             de   0.752621   \n",
       "14  11.695312    0.000224   0.000176   0.002427            the   0.991793   \n",
       "15  29.500000    0.006470   0.005124   0.084415         Mercur   0.903643   \n",
       "16  49.375000    0.010889   0.008512   0.055212          Isaac   0.250890   \n",
       "17  15.281250    0.020464   0.016246   0.197476          which   0.552697   \n",
       "18  14.109375    0.002350   0.004648   0.159981                  0.906187   \n",
       "19  13.109375    0.002914   0.002386   0.027672            nit   0.949443   \n",
       "20  63.625000    0.001103   0.000863   0.009859           Leon   0.976904   \n",
       "21  14.921875    0.000802   0.000619   0.007562            the   0.968515   \n",
       "22  31.484375    0.011201   0.008447   0.052217             gl   0.772370   \n",
       "23   9.671875    0.000068   0.000052   0.000336             se   0.999115   \n",
       "24  46.187500    0.003299   0.002711   0.029619           Neil   0.910562   \n",
       "\n",
       "   top2_token  top2_prob     top3_token  top3_prob  \n",
       "0           a   0.034541          known   0.009685  \n",
       "1         not   0.028910              a   0.015437  \n",
       "2     special   0.097212            two   0.049857  \n",
       "3               0.325054              a   0.119420  \n",
       "4           A   0.001565             Ag   0.001359  \n",
       "5          in   0.034638             \\n   0.022375  \n",
       "6        keep   0.365223        prevent   0.022241  \n",
       "7         and   0.023401              ,   0.014163  \n",
       "8         the   0.295584              a   0.073514  \n",
       "9           \"   0.085625             to   0.051917  \n",
       "10     called   0.044939          known   0.035346  \n",
       "11      which   0.151779            Nep   0.097856  \n",
       "12        not   0.034324              a   0.021743  \n",
       "13         De   0.210312              \"   0.007661  \n",
       "14          :   0.000901            not   0.000816  \n",
       "15     called   0.052473          known   0.005445  \n",
       "16        Sir   0.169364            Gal   0.076052  \n",
       "17     Brazil   0.270022              :   0.088430  \n",
       "18          $   0.017859  approximately   0.016498  \n",
       "19          _   0.017627              N   0.006643  \n",
       "20        the   0.009633              :   0.002185  \n",
       "21        not   0.005248           also   0.002513  \n",
       "22      organ   0.078580              o   0.040469  \n",
       "23         Se   0.000687             ge   0.000106  \n",
       "24         Ed   0.013909             \\n   0.010841  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display summary statistics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "display(summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7b2d6ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rms_norm(x: np.ndarray, weight: np.ndarray, eps: float = 1e-5) -> np.ndarray:\n",
    "    rms = np.sqrt(np.mean(x ** 2) + eps)\n",
    "    return (x / rms) * weight\n",
    "\n",
    "def compute_logits(z: np.ndarray, W: np.ndarray, norm_weight: np.ndarray, \n",
    "                          bias: np.ndarray = None, eps: float = 1e-5) -> np.ndarray:\n",
    "    z_norm = rms_norm(z, norm_weight, eps)\n",
    "    logits = z_norm @ W.T  # [hidden_size] @ [hidden_size, vocab_size].T = [vocab_size]\n",
    "    if bias is not None:\n",
    "        logits = logits + bias\n",
    "    return logits\n",
    "\n",
    "def softmax(x: np.ndarray) -> np.ndarray:\n",
    "    x_max = np.max(x)\n",
    "    exp_x = np.exp(x - x_max)\n",
    "    return exp_x / np.sum(exp_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "97d2b464",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_l2_distance(original_logits: torch.Tensor, perturbed_logits: torch.Tensor) -> float:\n",
    "    # apply softmax to the logits\n",
    "    original_logits = F.softmax(original_logits, dim=-1)\n",
    "    perturbed_logits = F.softmax(perturbed_logits, dim=-1)\n",
    "    # Compute L2 (Euclidean) distance between two logit vectors\n",
    "    return torch.norm(original_logits - perturbed_logits, p=2).item()\n",
    "\n",
    "def compute_cosine_distance(original_logits: torch.Tensor, perturbed_logits: torch.Tensor) -> float:\n",
    "    # Compute cosine distance (1 - cosine_similarity) between two logit vectors\n",
    "    cos_sim = F.cosine_similarity(original_logits.unsqueeze(0), perturbed_logits.unsqueeze(0))\n",
    "    return (1 - cos_sim).item()\n",
    "\n",
    "def compute_kl_divergence(original_logits: torch.Tensor, perturbed_logits: torch.Tensor) -> float:\n",
    "    # Compute KL divergence: KL(original || perturbed) after softmax\n",
    "    original_probs = F.softmax(original_logits, dim=-1)\n",
    "    perturbed_log_probs = F.log_softmax(perturbed_logits, dim=-1)\n",
    "    # KL(P || Q) = sum(P * log(P/Q)) = sum(P * (log_P - log_Q))\n",
    "    kl_div = F.kl_div(perturbed_log_probs, original_probs, reduction='sum')\n",
    "    return kl_div.item()\n",
    "\n",
    "def compute_js_divergence(original_logits: torch.Tensor, perturbed_logits: torch.Tensor) -> float:\n",
    "   # Compute Jensen-Shannon divergence: 0.5*KL(P||M) + 0.5*KL(Q||M) where M = 0.5*(P+Q).\n",
    "    P = F.softmax(original_logits, dim=-1)\n",
    "    Q = F.softmax(perturbed_logits, dim=-1)\n",
    "    M = 0.5 * (P + Q)\n",
    "    \n",
    "    # KL(P || M)\n",
    "    kl_pm = F.kl_div(M.log(), P, reduction='sum')\n",
    "    # KL(Q || M)\n",
    "    kl_qm = F.kl_div(M.log(), Q, reduction='sum')\n",
    "    \n",
    "    js_div = 0.5 * (kl_pm + kl_qm)\n",
    "    return js_div.item()\n",
    "\n",
    "def compute_all_distances(original_logits: torch.Tensor, perturbed_logits: torch.Tensor) -> Dict[str, float]:\n",
    "    # Compute all distance metrics between original and perturbed logits.\n",
    "    return {\n",
    "        'l2_distance': compute_l2_distance(original_logits, perturbed_logits),\n",
    "        'cosine_distance': compute_cosine_distance(original_logits, perturbed_logits),\n",
    "        'kl_divergence': compute_kl_divergence(original_logits, perturbed_logits),\n",
    "        'js_divergence': compute_js_divergence(original_logits, perturbed_logits),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "22a86983",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_perturbation(z: np.ndarray, W: np.ndarray, norm_weight: np.ndarray,\n",
    "                         neuron_indices: List[int], epsilon: float, signs: np.ndarray,\n",
    "                         eps: float = 1e-5) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    z_perturbed = z.copy()\n",
    "    for idx in neuron_indices:\n",
    "        z_perturbed[idx] += signs[idx] * epsilon\n",
    "    \n",
    "    logits = compute_logits(z_perturbed, W, norm_weight, eps=eps)\n",
    "    probs = softmax(logits)\n",
    "    \n",
    "    return logits, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "verify-offline-sim",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved weights\n",
    "W_saved = np.load(os.path.join(OUTPUT_DIR, \"lm_head_weights.npy\"))\n",
    "norm_weight_saved = np.load(os.path.join(OUTPUT_DIR, \"final_norm_weight.npy\"))\n",
    "\n",
    "# Load model config\n",
    "with open(os.path.join(OUTPUT_DIR, \"model_config.json\"), 'r') as f:\n",
    "    config = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "71361910",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input_dir = os.path.join(OUTPUT_DIR, \"input_1\")\n",
    "z_test = np.load(os.path.join(test_input_dir, \"z_values.npy\"))\n",
    "scores_test = np.load(os.path.join(test_input_dir, \"scores.npy\"))\n",
    "signed_proj = np.load(os.path.join(test_input_dir, \"signed_projections.npy\"))\n",
    "original_logits = np.load(os.path.join(test_input_dir, \"original_logits.npy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "perturb-simulation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 3556, z[0]: -2.0195, score: 0.040837, signed_proj: 0.040837\n",
      "index: 3241, z[1]: 7.6680, score: 0.033971, signed_proj: -0.033971\n",
      "index: 1360, z[2]: -7.7617, score: 0.031319, signed_proj: 0.031319\n",
      "index: 1573, z[3]: -5.3438, score: 0.028865, signed_proj: 0.028865\n",
      "index: 2084, z[4]: -4.0859, score: 0.028382, signed_proj: 0.028382\n",
      "index: 1415, z[5]: -1.1602, score: 0.025982, signed_proj: -0.025982\n",
      "index: 1927, z[6]: -12.9531, score: 0.025024, signed_proj: -0.025024\n",
      "index: 1839, z[7]: -3.1992, score: 0.024727, signed_proj: 0.024727\n",
      "index: 1640, z[8]: -2.1484, score: 0.019970, signed_proj: 0.019970\n",
      "index: 3684, z[9]: -4.1797, score: 0.019867, signed_proj: 0.019867\n",
      "index: 4008, z[10]: 1.1855, score: 0.019569, signed_proj: -0.019569\n"
     ]
    }
   ],
   "source": [
    "sorted_indices = np.argsort(scores_test)[::-1]\n",
    "z_test_sorted = z_test[sorted_indices]\n",
    "signed_proj_sorted = signed_proj[sorted_indices]\n",
    "scores_test_sorted = scores_test[sorted_indices] \n",
    "\n",
    "for i in range(min(11, len(z_test_sorted))):\n",
    "    print(f\"index: {sorted_indices[i]}, z[{i}]: {z_test_sorted[i]:.4f}, score: {scores_test_sorted[i]:.6f}, signed_proj: {signed_proj_sorted[i]:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "207bebe7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[np.int64(3241)]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signs = np.sign(signed_proj)\n",
    "neuron_subset = [sorted_indices[i] for i in range(1,2)]\n",
    "neuron_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "9a5af7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pert_logits, pert_probs = simulate_perturbation(\n",
    "        z_test, W_saved, norm_weight_saved, \n",
    "        neuron_subset, epsilon=1.0, signs=signs,\n",
    "        eps=config['rms_norm_eps']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "c975dd23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.int64(3681), np.float32(0.8895704), 'Paris')"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top1_idx_new = np.argmax(pert_logits)\n",
    "top1_idx_new, pert_probs[top1_idx_new],tokenizer.decode([top1_idx_new])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c470986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (np.int64(3681), np.float32(0.88960004), 'Paris') only changing first neuron epsilon 1\n",
    "(np.int64(3681), np.float32(0.8895704), 'Paris')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "b66c95d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.int64(3681), np.float32(0.89304), 'Paris')"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_probs = softmax(original_logits)\n",
    "top1_idx_orig = np.argmax(original_logits)\n",
    "#print top1_idx_new and the probability and token original\n",
    "top1_idx_orig, original_probs[top1_idx_orig],tokenizer.decode([top1_idx_orig])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "43067e6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'l2_distance': 0.0037430727388709784,\n",
       " 'cosine_distance': 0.00015932321548461914,\n",
       " 'kl_divergence': 7.244921289384365e-05,\n",
       " 'js_divergence': 1.8136139260604978e-05}"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_logits_torch = torch.from_numpy(original_logits).float()\n",
    "pert_logits_torch = torch.from_numpy(pert_logits).float()\n",
    "compute_all_distances(original_logits_torch, pert_logits_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46703450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original predictions\n",
    "\n",
    "\n",
    "print(f\"\\nOriginal top-1: idx={top1_idx_orig}, prob={original_probs[top1_idx_orig]:.4f}\")\n",
    "\n",
    "# Simulate perturbation with increasing neurons\n",
    "for n_neurons in [1, 3, 5, 10]:\n",
    "    neuron_subset = top_neuron_indices[:n_neurons].tolist()\n",
    "    \n",
    "    \n",
    "    print(f\"After perturbing {n_neurons} neurons: top-1 idx={top1_idx_new}, prob={pert_probs[top1_idx_new]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analysis-helpers",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions defined:\n",
      "  - load_input_data(output_dir, input_id): Load all data for a specific input\n",
      "  - load_model_weights(output_dir): Load model weights for offline simulation\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# HELPER FUNCTIONS FOR ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "def load_input_data(output_dir: str, input_id: int) -> Dict[str, Any]:\n",
    "    \"\"\"Load all saved data for a specific input.\"\"\"\n",
    "    input_dir = os.path.join(output_dir, f\"input_{input_id}\")\n",
    "    \n",
    "    data = {\n",
    "        'z_values': np.load(os.path.join(input_dir, \"z_values.npy\")),\n",
    "        'scores': np.load(os.path.join(input_dir, \"scores.npy\")),\n",
    "        'signed_projections': np.load(os.path.join(input_dir, \"signed_projections.npy\")),\n",
    "        'gradient': np.load(os.path.join(input_dir, \"gradient.npy\")),\n",
    "        'top_k_indices': np.load(os.path.join(input_dir, \"top_k_indices.npy\")),\n",
    "        'top_k_logits': np.load(os.path.join(input_dir, \"top_k_logits.npy\")),\n",
    "        'top_k_probs': np.load(os.path.join(input_dir, \"top_k_probs.npy\")),\n",
    "        'original_logits': np.load(os.path.join(input_dir, \"original_logits.npy\")),\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(input_dir, \"metadata.json\"), 'r') as f:\n",
    "        data['metadata'] = json.load(f)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def load_model_weights(output_dir: str) -> Dict[str, Any]:\n",
    "    weights = {\n",
    "        'W': np.load(os.path.join(output_dir, \"lm_head_weights.npy\")),\n",
    "        'norm_weight': np.load(os.path.join(output_dir, \"final_norm_weight.npy\")),\n",
    "    }\n",
    "    \n",
    "    bias_path = os.path.join(output_dir, \"lm_head_bias.npy\")\n",
    "    if os.path.exists(bias_path):\n",
    "        weights['bias'] = np.load(bias_path)\n",
    "    else:\n",
    "        weights['bias'] = None\n",
    "    \n",
    "    with open(os.path.join(output_dir, \"model_config.json\"), 'r') as f:\n",
    "        weights['config'] = json.load(f)\n",
    "    \n",
    "    return weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "z-value-analysis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Z-Value Distribution Analysis:\n",
      "============================================================\n",
      "\n",
      "Input 1: 'The capital of France is'\n",
      "  Z-values:  mean=0.0553, std=2.2452, min=-12.9531, max=39.6562\n",
      "  Scores:    mean=0.004006, std=0.003270, max=0.040837\n",
      "  Special node: idx=3556, z=-2.0195\n",
      "  Top 5 neurons: [np.int64(3556), np.int64(3241), np.int64(1360), np.int64(1573), np.int64(2084)]\n",
      "    Scores: ['0.0408', '0.0340', '0.0313', '0.0289', '0.0284']\n",
      "    Z-values: ['-2.0195', '7.6680', '-7.7617', '-5.3438', '-4.0859']\n",
      "\n",
      "Input 2: 'The largest mammal on Earth is'\n",
      "  Z-values:  mean=-0.0251, std=2.0271, min=-17.7500, max=16.8594\n",
      "  Scores:    mean=0.002380, std=0.001803, max=0.013166\n",
      "  Special node: idx=2617, z=3.9473\n",
      "  Top 5 neurons: [np.int64(2617), np.int64(3684), np.int64(1415), np.int64(3518), np.int64(3241)]\n",
      "    Scores: ['0.0132', '0.0127', '0.0121', '0.0119', '0.0117']\n",
      "    Z-values: ['3.9473', '-10.6797', '5.1953', '-1.0791', '16.8594']\n",
      "\n",
      "Input 3: 'The process of photosynthesis occurs in'\n",
      "  Z-values:  mean=0.0453, std=1.5968, min=-6.3203, max=7.0742\n",
      "  Scores:    mean=0.010161, std=0.007873, max=0.101778\n",
      "  Special node: idx=1573, z=-1.5293\n",
      "  Top 5 neurons: [np.int64(1573), np.int64(3556), np.int64(3241), np.int64(1360), np.int64(787)]\n",
      "    Scores: ['0.1018', '0.0933', '0.0748', '0.0612', '0.0583']\n",
      "    Z-values: ['-1.5293', '-4.0664', '7.0742', '-1.1836', '2.0234']\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Z-VALUE DISTRIBUTION ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nZ-Value Distribution Analysis:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for input_id in [1, 2, 3]:\n",
    "    data = load_input_data(OUTPUT_DIR, input_id)\n",
    "    z = data['z_values']\n",
    "    scores = data['scores']\n",
    "    \n",
    "    print(f\"\\nInput {input_id}: '{data['metadata']['input_text']}'\")\n",
    "    print(f\"  Z-values:  mean={np.mean(z):.4f}, std={np.std(z):.4f}, min={np.min(z):.4f}, max={np.max(z):.4f}\")\n",
    "    print(f\"  Scores:    mean={np.mean(scores):.6f}, std={np.std(scores):.6f}, max={np.max(scores):.6f}\")\n",
    "    print(f\"  Special node: idx={data['metadata']['special_node_idx']}, z={z[data['metadata']['special_node_idx']]:.4f}\")\n",
    "    \n",
    "    # Top 5 neurons by score\n",
    "    top5_idx = np.argsort(scores)[::-1][:5]\n",
    "    print(f\"  Top 5 neurons: {list(top5_idx)}\")\n",
    "    print(f\"    Scores: {[f'{scores[i]:.4f}' for i in top5_idx]}\")\n",
    "    print(f\"    Z-values: {[f'{z[i]:.4f}' for i in top5_idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATA EXTRACTION COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nOutput directory: {OUTPUT_DIR}\")\n",
    "print(f\"\\nSaved files:\")\n",
    "print(f\"  - lm_head_weights.npy: Last layer weights [vocab_size x hidden_size]\")\n",
    "print(f\"  - final_norm_weight.npy: RMSNorm gamma [hidden_size]\")\n",
    "print(f\"  - model_config.json: Model configuration\")\n",
    "print(f\"  - summary.csv: Summary statistics for all inputs\")\n",
    "print(f\"  - input_{{id}}/: Per-input data directories\")\n",
    "print(f\"    - z_values.npy: Pre-norm activations [hidden_size]\")\n",
    "print(f\"    - scores.npy: Neuron impact scores [hidden_size]\")\n",
    "print(f\"    - signed_projections.npy: Signed projections [hidden_size]\")\n",
    "print(f\"    - gradient.npy: Swap gradient [hidden_size]\")\n",
    "print(f\"    - original_logits.npy: Full logits [vocab_size]\")\n",
    "print(f\"    - top_k_*.npy: Top-k predictions\")\n",
    "print(f\"    - metadata.json: Input metadata\")\n",
    "print(\"\\nYou can now analyze z-values and simulate outputs offline!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
