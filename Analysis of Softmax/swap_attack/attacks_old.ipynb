{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bb5b82ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm # For progress bars\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f19f764d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "MODEL_PATH = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f99c036b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea2bfc9fb47e4ed39b1785fce1899126",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = LlamaTokenizer.from_pretrained(MODEL_PATH)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495577bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_original_activations_and_logits(model, inputs):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_hidden_states=True)\n",
    "    \n",
    "    original_hidden_states = outputs.hidden_states\n",
    "    \n",
    "    return outputs.logits, original_hidden_states\n",
    "\n",
    "def create_malicious_output(tokenizer, original_logits):\n",
    "    # the very last token in the sequence\n",
    "    last_token_logits = original_logits[0, -1, :].clone()\n",
    "    \n",
    "    # Find index of highest logit\n",
    "    correct_token_idx = torch.argmax(last_token_logits).item()\n",
    "    \n",
    "    # a very unlikely token\n",
    "    incorrect_token_idx = torch.argmin(last_token_logits).item()\n",
    "    \n",
    "    print(\"--- Logit Swap Attack ---\")\n",
    "    print(f\"Original top prediction: '{tokenizer.decode(correct_token_idx)}' (ID: {correct_token_idx})\")\n",
    "    print(f\"Target swap token:     '{tokenizer.decode(incorrect_token_idx)}' (ID: {incorrect_token_idx})\")\n",
    "    \n",
    "    # malicious target by swapping the values\n",
    "    malicious_target_logits = last_token_logits.clone()\n",
    "    correct_value = malicious_target_logits[correct_token_idx]\n",
    "    incorrect_value = malicious_target_logits[incorrect_token_idx]\n",
    "    \n",
    "    malicious_target_logits[correct_token_idx] = incorrect_value\n",
    "    malicious_target_logits[incorrect_token_idx] = correct_value\n",
    "    \n",
    "    print(f\"New top prediction after swap: '{tokenizer.decode(torch.argmax(malicious_target_logits))}'\\n\")\n",
    "    \n",
    "    return malicious_target_logits.detach()\n",
    "\n",
    "def reconstruct_internal_state(model, \n",
    "                               original_hidden_states, \n",
    "                               malicious_target_logits,\n",
    "                               inputs,\n",
    "                                 epoch=200, lr=0.01):\n",
    "    # first hidden state is the input embedding\n",
    "    input_embeddings = original_hidden_states[0].detach()\n",
    "    \n",
    "    reconstructed_states = []\n",
    "    for i in range(1, len(original_hidden_states)):\n",
    "        # only need to reconstruct states for last token position\n",
    "        state = original_hidden_states[i][0, -1, :].clone().detach().to(DEVICE)\n",
    "        reconstructed_states.append(state.requires_grad_(True))\n",
    "\n",
    "    # setup the optimizer to update our list of trainable state tensors\n",
    "    optimizer = torch.optim.Adam(reconstructed_states, lr=lr)\n",
    "    loss_function = torch.nn.MSELoss()\n",
    "    \n",
    "    print(f\"--- Reconstructing Internal State (Optimizing {len(reconstructed_states)} tensors) ---\")\n",
    "    \n",
    "    for step in tqdm(range(epoch), desc=\"Optimization Progress\"):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        total_consistency_loss = 0.0\n",
    "        \n",
    "        # start with the fixed embedding of the second to last token\n",
    "        current_hidden_state = original_hidden_states[0][0, -1, :].unsqueeze(0).unsqueeze(0).detach()\n",
    "        \n",
    "        # run the forward pass layer by layer\n",
    "        for i, layer in enumerate(model.model.layers):\n",
    "            # simplifying by using the previous layers output as input\n",
    "            layer_output = layer(current_hidden_state, \n",
    "                                 position_ids=position_ids,\n",
    "                                 attention_mask=attention_mask,\n",
    "                                 use_cache=False)[0]\n",
    "            \n",
    "            # trainable guess for the input of layer `i+1`.\n",
    "            consistency_loss = loss_function(layer_output.squeeze(), reconstructed_states[i])\n",
    "            total_consistency_loss += consistency_loss\n",
    "            \n",
    "            # output of this layer becomes the input for the next\n",
    "            current_hidden_state = layer_output\n",
    "\n",
    "        # final hidden state after the last layer\n",
    "        final_hidden_state = current_hidden_state\n",
    "        \n",
    "        final_hidden_state = model.model.norm(final_hidden_state)\n",
    "        reconstructed_logits = model.lm_head(final_hidden_state).squeeze()\n",
    "        \n",
    "        # penalize the difference between our result and the malicious target\n",
    "        target_loss = loss_function(reconstructed_logits, malicious_target_logits)\n",
    "        \n",
    "        total_loss = target_loss + total_consistency_loss\n",
    "        \n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if step % 50 == 0:\n",
    "            print(f\"Step {step}: Total Loss={total_loss.item():.4f}, Target Loss={target_loss.item():.4f}, Consistency Loss={total_consistency_loss.item():.4f}\")\n",
    "\n",
    "    return [state.detach() for state in reconstructed_states]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a039f076",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_reconstruction(original_hidden_states, reconstructed_states):\n",
    "    analysis_results = []\n",
    "    \n",
    "    # Compare the last token's hidden state across all layers\n",
    "    for i in range(len(reconstructed_states)):\n",
    "        original = original_hidden_states[i+1][0, -1, :].to(DEVICE)\n",
    "        reconstructed = reconstructed_states[i]\n",
    "        \n",
    "        # 1. Calculate overall MSE for the layer\n",
    "        mse = F.mse_loss(original, reconstructed).item()\n",
    "        \n",
    "        # 2. Find the neuron with the minimum absolute difference\n",
    "        abs_diff = torch.abs(original - reconstructed)\n",
    "        min_diff_val, min_diff_idx = torch.min(abs_diff, dim=0)\n",
    "        \n",
    "        # 3. Store detailed results for this layer\n",
    "        analysis_results.append({\n",
    "            'layer_index': i,\n",
    "            'mse_error': mse,\n",
    "            'min_abs_difference': min_diff_val.item(),\n",
    "            'min_diff_neuron_index': min_diff_idx.item()\n",
    "        })\n",
    "        \n",
    "    print(\"\\n--- Analysis of Reconstruction ---\")\n",
    "    mean_mse = np.mean([res['mse_error'] for res in analysis_results])\n",
    "    print(f\"Mean Squared Error across all layers: {mean_mse:.6f}\")\n",
    "    \n",
    "    best_layer = min(analysis_results, key=lambda x: x['min_abs_difference'])\n",
    "    print(f\"Most plausible neuron found in Layer {best_layer['layer_index']}:\")\n",
    "    print(f\"  - Neuron Index: {best_layer['min_diff_neuron_index']}\")\n",
    "    print(f\"  - Absolute Difference: {best_layer['min_abs_difference']:.8f}\")\n",
    "\n",
    "    mse_values = [res['mse_error'] for res in analysis_results]\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(mse_values, marker='o', linestyle='-')\n",
    "    plt.title(\"Reconstruction Error (MSE) vs. Layer Depth\")\n",
    "    plt.xlabel(\"Decoder Layer Index\")\n",
    "    plt.ylabel(\"Mean Squared Error\")\n",
    "    plt.grid(True)\n",
    "    save_path = 'attack_success.pdf'\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return analysis_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d9196c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Logit Swap Attack ---\n",
      "Original top prediction: 'networking' (ID: 28127)\n",
      "Target swap token:     'lichkeit' (ID: 23143)\n",
      "New top prediction after swap: 'media'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "#PROMPT = \"The capital of Turkey is\"\n",
    "#PROMPT = \"My favorite color is\"\n",
    "PROMPT = \"Facebook is social\"\n",
    "inputs = tokenizer(PROMPT, return_tensors=\"pt\").to(DEVICE)\n",
    "\n",
    "sequence_length = inputs['input_ids'].shape[1]\n",
    "\n",
    "#STEP 1\n",
    "original_logits, original_hidden_states = get_original_activations_and_logits(model, inputs)\n",
    "malicious_target = create_malicious_output(tokenizer, original_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "006087c0",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "reconstruct_internal_state() missing 1 required positional argument: 'attention_mask'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#STEP 2\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m reconstructed_hidden_states \u001b[38;5;241m=\u001b[39m reconstruct_internal_state(\n\u001b[0;32m      3\u001b[0m     model,\n\u001b[0;32m      4\u001b[0m     original_hidden_states,\n\u001b[0;32m      5\u001b[0m     malicious_target,\n\u001b[0;32m      6\u001b[0m     inputs,\n\u001b[0;32m      7\u001b[0m     epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m250\u001b[39m,\n\u001b[0;32m      8\u001b[0m     lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.05\u001b[39m\n\u001b[0;32m      9\u001b[0m )\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m#STEP 3\u001b[39;00m\n\u001b[0;32m     12\u001b[0m analysis_data \u001b[38;5;241m=\u001b[39m analyze_reconstruction(original_hidden_states, reconstructed_hidden_states)\n",
      "\u001b[1;31mTypeError\u001b[0m: reconstruct_internal_state() missing 1 required positional argument: 'attention_mask'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#STEP 2\n",
    "reconstructed_hidden_states = reconstruct_internal_state(\n",
    "    model,\n",
    "    original_hidden_states,\n",
    "    malicious_target,\n",
    "    inputs,\n",
    "    epoch=250,\n",
    "    lr=0.05\n",
    ")\n",
    "\n",
    "#STEP 3\n",
    "analysis_data = analyze_reconstruction(original_hidden_states, reconstructed_hidden_states)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdfd030",
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 4: Save the analysis to a CSV file\n",
    "filename=f\"analysis_{PROMPT.replace(' ', '_')}.csv\"\n",
    "df = pd.DataFrame(analysis_data)\n",
    "df.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "695361a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0079,  0.0270, -0.0012,  ...,  0.0032,  0.0051, -0.0337]],\n",
       "       dtype=torch.float16, grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fc675a",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = inputs['input_ids'].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dca5354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Reconstructing Internal State (Optimizing 32 tensors) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:   0%|          | 0/250 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (32) must match the size of tensor b (128) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 29\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# run the forward pass layer by layer\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(model\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mlayers):\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;66;03m# simplifying by using the previous layers output as input\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m layer(current_hidden_state, \n\u001b[0;32m     30\u001b[0m                             position_embeddings\u001b[38;5;241m=\u001b[39mposition_embeddings,\n\u001b[0;32m     31\u001b[0m                             attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m     32\u001b[0m                             use_cache\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;66;03m# trainable guess for the input of layer `i+1`.\u001b[39;00m\n\u001b[0;32m     35\u001b[0m     consistency_loss \u001b[38;5;241m=\u001b[39m loss_function(layer_output\u001b[38;5;241m.\u001b[39msqueeze(), reconstructed_states[i])\n",
      "File \u001b[1;32mc:\\Users\\hskay\\anaconda3\\Lib\\site-packages\\transformers\\modeling_layers.py:94\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     91\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(message)\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m---> 94\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\hskay\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\hskay\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\hskay\\anaconda3\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:289\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, position_ids, past_key_value, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[0;32m    287\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m hidden_states, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn(\n\u001b[0;32m    290\u001b[0m     hidden_states\u001b[38;5;241m=\u001b[39mhidden_states,\n\u001b[0;32m    291\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m    292\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m    293\u001b[0m     past_key_value\u001b[38;5;241m=\u001b[39mpast_key_value,\n\u001b[0;32m    294\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m    295\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[0;32m    296\u001b[0m     position_embeddings\u001b[38;5;241m=\u001b[39mposition_embeddings,\n\u001b[0;32m    297\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    298\u001b[0m )\n\u001b[0;32m    299\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[0;32m    301\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hskay\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\hskay\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\hskay\\anaconda3\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:237\u001b[0m, in \u001b[0;36mLlamaAttention.forward\u001b[1;34m(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position, **kwargs)\u001b[0m\n\u001b[0;32m    234\u001b[0m value_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_proj(hidden_states)\u001b[38;5;241m.\u001b[39mview(hidden_shape)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    236\u001b[0m cos, sin \u001b[38;5;241m=\u001b[39m position_embeddings\n\u001b[1;32m--> 237\u001b[0m query_states, key_states \u001b[38;5;241m=\u001b[39m apply_rotary_pos_emb(query_states, key_states, cos, sin)\n\u001b[0;32m    239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;66;03m# sin and cos are specific to RoPE models; cache_position needed for the static cache\u001b[39;00m\n\u001b[0;32m    241\u001b[0m     cache_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msin\u001b[39m\u001b[38;5;124m\"\u001b[39m: sin, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcos\u001b[39m\u001b[38;5;124m\"\u001b[39m: cos, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcache_position\u001b[39m\u001b[38;5;124m\"\u001b[39m: cache_position}\n",
      "File \u001b[1;32mc:\\Users\\hskay\\anaconda3\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:135\u001b[0m, in \u001b[0;36mapply_rotary_pos_emb\u001b[1;34m(q, k, cos, sin, position_ids, unsqueeze_dim)\u001b[0m\n\u001b[0;32m    133\u001b[0m cos \u001b[38;5;241m=\u001b[39m cos\u001b[38;5;241m.\u001b[39munsqueeze(unsqueeze_dim)\n\u001b[0;32m    134\u001b[0m sin \u001b[38;5;241m=\u001b[39m sin\u001b[38;5;241m.\u001b[39munsqueeze(unsqueeze_dim)\n\u001b[1;32m--> 135\u001b[0m q_embed \u001b[38;5;241m=\u001b[39m (q \u001b[38;5;241m*\u001b[39m cos) \u001b[38;5;241m+\u001b[39m (rotate_half(q) \u001b[38;5;241m*\u001b[39m sin)\n\u001b[0;32m    136\u001b[0m k_embed \u001b[38;5;241m=\u001b[39m (k \u001b[38;5;241m*\u001b[39m cos) \u001b[38;5;241m+\u001b[39m (rotate_half(k) \u001b[38;5;241m*\u001b[39m sin)\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m q_embed, k_embed\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (32) must match the size of tensor b (128) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "epoch=250\n",
    "lr=0.05\n",
    "# first hidden state is the input embedding\n",
    "input_embeddings = original_hidden_states[0].detach()\n",
    "\n",
    "reconstructed_states = []\n",
    "for i in range(1, len(original_hidden_states)):\n",
    "    # only need to reconstruct states for last token position\n",
    "    state = original_hidden_states[i][0, -1, :].clone().detach().to(DEVICE)\n",
    "    reconstructed_states.append(state.requires_grad_(True))\n",
    "\n",
    "# setup the optimizer to update our list of trainable state tensors\n",
    "optimizer = torch.optim.Adam(reconstructed_states, lr=lr)\n",
    "loss_function = torch.nn.MSELoss()\n",
    "\n",
    "print(f\"--- Reconstructing Internal State (Optimizing {len(reconstructed_states)} tensors) ---\")\n",
    "\n",
    "for step in tqdm(range(epoch), desc=\"Optimization Progress\"):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    total_consistency_loss = 0.0\n",
    "    \n",
    "    # start with the fixed embedding of the second to last token\n",
    "    current_hidden_state = original_hidden_states[0][0, -1, :].unsqueeze(0).unsqueeze(0).detach()\n",
    "    \n",
    "    # run the forward pass layer by layer\n",
    "    for i, layer in enumerate(model.model.layers):\n",
    "        # simplifying by using the previous layers output as input\n",
    "        layer_output = layer(\n",
    "                current_hidden_state,\n",
    "                attention_mask=inputs['attention_mask'],\n",
    "                position_ids=torch.arange(seq_length, device=DEVICE).unsqueeze(0),\n",
    "                use_cache=False\n",
    "            )[0]\n",
    "        \n",
    "        # trainable guess for the input of layer `i+1`.\n",
    "        consistency_loss = loss_function(layer_output.squeeze(), reconstructed_states[i])\n",
    "        total_consistency_loss += consistency_loss\n",
    "        \n",
    "        # output of this layer becomes the input for the next\n",
    "        current_hidden_state = layer_output\n",
    "\n",
    "    # final hidden state after the last layer\n",
    "    final_hidden_state = current_hidden_state\n",
    "    \n",
    "    final_hidden_state = model.model.norm(final_hidden_state)\n",
    "    reconstructed_logits = model.lm_head(final_hidden_state).squeeze()\n",
    "    \n",
    "    # penalize the difference between our result and the malicious target\n",
    "    target_loss = loss_function(reconstructed_logits, malicious_target_logits)\n",
    "    \n",
    "    total_loss = target_loss + total_consistency_loss\n",
    "    \n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if step % 50 == 0:\n",
    "        print(f\"Step {step}: Total Loss={total_loss.item():.4f}, Target Loss={target_loss.item():.4f}, Consistency Loss={total_consistency_loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6f6f2780",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_internal_state(model, \n",
    "                               original_hidden_states, \n",
    "                               malicious_target_logits,\n",
    "                               inputs,\n",
    "                               epoch=200, lr=0.01):\n",
    "    # Get the sequence length\n",
    "    seq_length = inputs['input_ids'].shape[1]\n",
    "    \n",
    "    # Initialize reconstructed states for each layer (except the first embedding layer)\n",
    "    reconstructed_states = []\n",
    "    for i in range(1, len(original_hidden_states)):\n",
    "        # Only reconstruct the last token's hidden state for each layer\n",
    "        state = original_hidden_states[i][0, -1, :].clone().detach().to(DEVICE)\n",
    "        reconstructed_states.append(state.requires_grad_(True))\n",
    "\n",
    "    # Setup optimizer\n",
    "    optimizer = torch.optim.Adam(reconstructed_states, lr=lr)\n",
    "    loss_function = torch.nn.MSELoss()\n",
    "    \n",
    "    print(f\"--- Reconstructing Internal State (Optimizing {len(reconstructed_states)} tensors) ---\")\n",
    "    \n",
    "    for step in tqdm(range(epoch), desc=\"Optimization Progress\"):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        total_consistency_loss = 0.0\n",
    "        \n",
    "        # Create a custom forward pass that uses our reconstructed states\n",
    "        # Start with the original embeddings\n",
    "        current_hidden_states = original_hidden_states[0].clone()\n",
    "        \n",
    "        # Process through each layer\n",
    "        for i, layer in enumerate(model.model.layers):\n",
    "            # Get the original layer output for comparison\n",
    "            with torch.no_grad():\n",
    "                original_layer_output = layer(\n",
    "                    current_hidden_states,\n",
    "                    attention_mask=inputs['attention_mask'],\n",
    "                    position_ids=torch.arange(seq_length, device=DEVICE).unsqueeze(0),\n",
    "                    use_cache=False\n",
    "                )[0]\n",
    "            \n",
    "            # Calculate consistency loss between original output and our reconstruction\n",
    "            # for the last token position\n",
    "            consistency_loss = loss_function(\n",
    "                original_layer_output[0, -1, :], \n",
    "                reconstructed_states[i]\n",
    "            )\n",
    "            total_consistency_loss += consistency_loss\n",
    "            \n",
    "            # For the next layer's input, use the original hidden states but replace\n",
    "            # the last token with our reconstructed version\n",
    "            current_hidden_states = original_layer_output.clone()\n",
    "            current_hidden_states[0, -1, :] = reconstructed_states[i]\n",
    "\n",
    "        # Apply final layer norm and compute logits\n",
    "        final_hidden_state = model.model.norm(current_hidden_states)\n",
    "        reconstructed_logits = model.lm_head(final_hidden_state[0, -1, :])\n",
    "        \n",
    "        # Target loss: difference between reconstructed logits and malicious target\n",
    "        target_loss = loss_function(reconstructed_logits, malicious_target_logits)\n",
    "        \n",
    "        total_loss = target_loss + total_consistency_loss\n",
    "        \n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if step % 50 == 0:\n",
    "            print(f\"Step {step}: Total Loss={total_loss.item():.4f}, Target Loss={target_loss.item():.4f}, Consistency Loss={total_consistency_loss.item():.4f}\")\n",
    "\n",
    "    return [state.detach() for state in reconstructed_states]\n",
    "def reconstruct_internal_state_v2(model, \n",
    "                                  original_hidden_states, \n",
    "                                  malicious_target_logits,\n",
    "                                  inputs,\n",
    "                                  epoch=200, lr=0.01):\n",
    "    \n",
    "    # Initialize reconstructed states for the last token of each layer\n",
    "    reconstructed_states = []\n",
    "    for i in range(1, len(original_hidden_states)):\n",
    "        state = original_hidden_states[i][0, -1, :].clone().detach().to(DEVICE)\n",
    "        reconstructed_states.append(state.requires_grad_(True))\n",
    "\n",
    "    optimizer = torch.optim.Adam(reconstructed_states, lr=lr)\n",
    "    loss_function = torch.nn.MSELoss()\n",
    "    \n",
    "    print(f\"--- Reconstructing Internal State (Optimizing {len(reconstructed_states)} tensors) ---\")\n",
    "    \n",
    "    for step in tqdm(range(epoch), desc=\"Optimization Progress\"):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Hook to replace hidden states with our reconstructed versions\n",
    "        def create_hook(layer_idx, reconstructed_state):\n",
    "            def hook(module, input, output):\n",
    "                # Replace the last token's hidden state with our reconstruction\n",
    "                modified_output = output[0].clone()\n",
    "                modified_output[0, -1, :] = reconstructed_state\n",
    "                return (modified_output,) + output[1:]\n",
    "            return hook\n",
    "        \n",
    "        # Register hooks for each layer\n",
    "        hooks = []\n",
    "        for i, layer in enumerate(model.model.layers):\n",
    "            if i < len(reconstructed_states):\n",
    "                hook = layer.register_forward_hook(create_hook(i, reconstructed_states[i]))\n",
    "                hooks.append(hook)\n",
    "        \n",
    "        try:\n",
    "            # Forward pass with modified hidden states\n",
    "            with torch.enable_grad():\n",
    "                outputs = model(**inputs, output_hidden_states=True)\n",
    "                reconstructed_logits = outputs.logits[0, -1, :]\n",
    "                \n",
    "                # Target loss\n",
    "                target_loss = loss_function(reconstructed_logits, malicious_target_logits)\n",
    "                \n",
    "                # Consistency loss\n",
    "                total_consistency_loss = 0.0\n",
    "                for i, reconstructed_state in enumerate(reconstructed_states):\n",
    "                    if i + 1 < len(outputs.hidden_states):\n",
    "                        original_state = original_hidden_states[i + 1][0, -1, :]\n",
    "                        consistency_loss = loss_function(original_state, reconstructed_state)\n",
    "                        total_consistency_loss += consistency_loss\n",
    "                \n",
    "                total_loss = target_loss + total_consistency_loss\n",
    "                \n",
    "        finally:\n",
    "            # Remove hooks\n",
    "            for hook in hooks:\n",
    "                hook.remove()\n",
    "        \n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if step % 50 == 0:\n",
    "            print(f\"Step {step}: Total Loss={total_loss.item():.4f}, Target Loss={target_loss.item():.4f}, Consistency Loss={total_consistency_loss.item():.4f}\")\n",
    "\n",
    "    return [state.detach() for state in reconstructed_states]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5ef5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "PROMPT = \"The capital of Turkey is\"\n",
    "PROMPT = \"My favorite color is\"\n",
    "inputs = tokenizer(PROMPT, return_tensors=\"pt\").to(DEVICE)\n",
    "\n",
    "# Remove the manual position_ids and attention_mask creation\n",
    "# sequence_length = inputs['input_ids'].shape[1]\n",
    "# last_token_position = sequence_length - 1\n",
    "# position_ids = torch.tensor([[last_token_position]], device=DEVICE, dtype=torch.long)\n",
    "# attention_mask = torch.ones((1, 1), dtype=torch.long, device=DEVICE)\n",
    "\n",
    "#STEP 1\n",
    "original_logits, original_hidden_states = get_original_activations_and_logits(model, inputs)\n",
    "malicious_target = create_malicious_output(tokenizer, original_logits)\n",
    "\n",
    "#STEP 2\n",
    "reconstructed_hidden_states = reconstruct_internal_state(\n",
    "    model,\n",
    "    original_hidden_states,\n",
    "    malicious_target,\n",
    "    inputs,  # Pass the full inputs\n",
    "    epoch=250,\n",
    "    lr=0.05\n",
    ")\n",
    "\n",
    "#STEP 3\n",
    "analysis_data = analyze_reconstruction(original_hidden_states, reconstructed_hidden_states)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3e7fc97f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Reconstructing Internal State (Optimizing 32 tensors) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:   0%|          | 0/250 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#STEP 2\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m reconstructed_hidden_states \u001b[38;5;241m=\u001b[39m reconstruct_internal_state_v2(  \u001b[38;5;66;03m# or reconstruct_internal_state_v2\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     model,\n\u001b[0;32m      4\u001b[0m     original_hidden_states,\n\u001b[0;32m      5\u001b[0m     malicious_target,\n\u001b[0;32m      6\u001b[0m     inputs,\n\u001b[0;32m      7\u001b[0m     epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m250\u001b[39m,\n\u001b[0;32m      8\u001b[0m     lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.05\u001b[39m\n\u001b[0;32m      9\u001b[0m )\n",
      "Cell \u001b[1;32mIn[45], line 110\u001b[0m, in \u001b[0;36mreconstruct_internal_state_v2\u001b[1;34m(model, original_hidden_states, malicious_target_logits, inputs, epoch, lr)\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    108\u001b[0m     \u001b[38;5;66;03m# Forward pass with modified hidden states\u001b[39;00m\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39menable_grad():\n\u001b[1;32m--> 110\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, output_hidden_states\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    111\u001b[0m         reconstructed_logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\n\u001b[0;32m    113\u001b[0m         \u001b[38;5;66;03m# Target loss\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hskay\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\hskay\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\hskay\\anaconda3\\Lib\\site-packages\\transformers\\utils\\generic.py:959\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    957\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_dict_passed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    958\u001b[0m     return_dict \u001b[38;5;241m=\u001b[39m return_dict_passed\n\u001b[1;32m--> 959\u001b[0m output \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    960\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    961\u001b[0m     output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[1;32mc:\\Users\\hskay\\anaconda3\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:460\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[0;32m    428\u001b[0m \u001b[38;5;129m@can_return_tuple\u001b[39m\n\u001b[0;32m    429\u001b[0m \u001b[38;5;129m@auto_docstring\u001b[39m\n\u001b[0;32m    430\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    441\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Unpack[TransformersKwargs],\n\u001b[0;32m    442\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m CausalLMOutputWithPast:\n\u001b[0;32m    443\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    444\u001b[0m \u001b[38;5;124;03m    Example:\u001b[39;00m\n\u001b[0;32m    445\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    458\u001b[0m \u001b[38;5;124;03m    \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\u001b[39;00m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;124;03m    ```\"\"\"\u001b[39;00m\n\u001b[1;32m--> 460\u001b[0m     outputs: BaseModelOutputWithPast \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\n\u001b[0;32m    461\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m    462\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m    463\u001b[0m         position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m    464\u001b[0m         past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m    465\u001b[0m         inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[0;32m    466\u001b[0m         use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m    467\u001b[0m         cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[0;32m    468\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    469\u001b[0m     )\n\u001b[0;32m    471\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[0;32m    472\u001b[0m     \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hskay\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\hskay\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\hskay\\anaconda3\\Lib\\site-packages\\transformers\\utils\\generic.py:1083\u001b[0m, in \u001b[0;36mcheck_model_inputs.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1080\u001b[0m                 module\u001b[38;5;241m.\u001b[39mforward \u001b[38;5;241m=\u001b[39m make_capture_wrapper(module, original_forward, key, specs\u001b[38;5;241m.\u001b[39mindex)\n\u001b[0;32m   1081\u001b[0m                 monkey_patched_layers\u001b[38;5;241m.\u001b[39mappend((module, original_forward))\n\u001b[1;32m-> 1083\u001b[0m outputs \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1084\u001b[0m \u001b[38;5;66;03m# Restore original forward methods\u001b[39;00m\n\u001b[0;32m   1085\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module, original_forward \u001b[38;5;129;01min\u001b[39;00m monkey_patched_layers:\n",
      "File \u001b[1;32mc:\\Users\\hskay\\anaconda3\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:390\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, cache_position, use_cache, **kwargs)\u001b[0m\n\u001b[0;32m    387\u001b[0m position_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrotary_emb(hidden_states, position_ids)\n\u001b[0;32m    389\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m decoder_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers]:\n\u001b[1;32m--> 390\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m decoder_layer(\n\u001b[0;32m    391\u001b[0m         hidden_states,\n\u001b[0;32m    392\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mcausal_mask,\n\u001b[0;32m    393\u001b[0m         position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m    394\u001b[0m         past_key_value\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m    395\u001b[0m         cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[0;32m    396\u001b[0m         position_embeddings\u001b[38;5;241m=\u001b[39mposition_embeddings,\n\u001b[0;32m    397\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    398\u001b[0m     )\n\u001b[0;32m    400\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(hidden_states)\n\u001b[0;32m    401\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m BaseModelOutputWithPast(\n\u001b[0;32m    402\u001b[0m     last_hidden_state\u001b[38;5;241m=\u001b[39mhidden_states,\n\u001b[0;32m    403\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m    404\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\hskay\\anaconda3\\Lib\\site-packages\\transformers\\modeling_layers.py:94\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     91\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(message)\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m---> 94\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\hskay\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\hskay\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1857\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1854\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[0;32m   1856\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1857\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[0;32m   1858\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1859\u001b[0m     \u001b[38;5;66;03m# run always called hooks if they have not already been run\u001b[39;00m\n\u001b[0;32m   1860\u001b[0m     \u001b[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001b[39;00m\n\u001b[0;32m   1861\u001b[0m     \u001b[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001b[39;00m\n\u001b[0;32m   1862\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m _global_forward_hooks\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[1;32mc:\\Users\\hskay\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1818\u001b[0m, in \u001b[0;36mModule._call_impl.<locals>.inner\u001b[1;34m()\u001b[0m\n\u001b[0;32m   1816\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, args, kwargs, result)\n\u001b[0;32m   1817\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1818\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, args, result)\n\u001b[0;32m   1820\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hook_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1821\u001b[0m     result \u001b[38;5;241m=\u001b[39m hook_result\n",
      "Cell \u001b[1;32mIn[45], line 96\u001b[0m, in \u001b[0;36mreconstruct_internal_state_v2.<locals>.create_hook.<locals>.hook\u001b[1;34m(module, input, output)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhook\u001b[39m(module, \u001b[38;5;28minput\u001b[39m, output):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;66;03m# Replace the last token's hidden state with our reconstruction\u001b[39;00m\n\u001b[0;32m     95\u001b[0m     modified_output \u001b[38;5;241m=\u001b[39m output[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mclone()\n\u001b[1;32m---> 96\u001b[0m     modified_output[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :] \u001b[38;5;241m=\u001b[39m reconstructed_state\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (modified_output,) \u001b[38;5;241m+\u001b[39m output[\u001b[38;5;241m1\u001b[39m:]\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for tensor of dimension 2"
     ]
    }
   ],
   "source": [
    "#STEP 2\n",
    "reconstructed_hidden_states = reconstruct_internal_state_v2(  # or reconstruct_internal_state_v2\n",
    "    model,\n",
    "    original_hidden_states,\n",
    "    malicious_target,\n",
    "    inputs,\n",
    "    epoch=250,\n",
    "    lr=0.05\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
