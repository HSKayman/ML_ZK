{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "875f3e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hskay\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from tqdm import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70077c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "MODEL_PATH = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ea7339d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.33s/it]\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map='auto' if torch.cuda.is_available() else None\n",
    ")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2c0c83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to capture activations\n",
    "def get_activation(name, activations_dict):\n",
    "    def hook(module, input, output):\n",
    "        # Handle different output types\n",
    "        if isinstance(output, tuple):\n",
    "            activations_dict[name] = output[0].detach().clone()\n",
    "        else:\n",
    "            activations_dict[name] = output.detach().clone()\n",
    "    return hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96e30929",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_malicious_output(tokenizer, original_logits):\n",
    "\n",
    "    # Clone to avoid modifying the original tensor\n",
    "    malicious_target_logits = original_logits.clone()\n",
    "    \n",
    "    # We'll attack the prediction for the last meaningful token\n",
    "    # For simplicity, we target the last token position in this example\n",
    "    last_token_logits = malicious_target_logits[0, -1, :]\n",
    "    \n",
    "    # Find the index of the highest logit (correct prediction)\n",
    "    correct_token_idx = torch.argmax(last_token_logits).item()\n",
    "    \n",
    "    # Find the index of the lowest logit (a very unlikely token)\n",
    "    incorrect_token_idx = torch.argmin(last_token_logits).item()\n",
    "    \n",
    "   \n",
    "    # Swap the values\n",
    "    correct_value = last_token_logits[correct_token_idx]\n",
    "    incorrect_value = last_token_logits[incorrect_token_idx]\n",
    "    \n",
    "    malicious_target_logits[0, -1, correct_token_idx] = incorrect_value\n",
    "    malicious_target_logits[0, -1, incorrect_token_idx] = correct_value\n",
    "\n",
    "    print(f\"\\nLogit Swap: Swapping '{tokenizer.decode(correct_token_idx)}' with '{tokenizer.decode(incorrect_token_idx)}'\")\n",
    "    print(f\"Before Swap - Correct Token Logit: {last_token_logits[correct_token_idx].item()}, Incorrect Token Logit: {last_token_logits[incorrect_token_idx].item()}\")\n",
    "    print(f\"Before Swap - Predicted Token: '{tokenizer.decode(torch.argmax(last_token_logits).item())}'\")\n",
    "    print(f\"After Swap - Predicted Token will be: '{tokenizer.decode(torch.argmax(malicious_target_logits[0, -1, :]).item())}'\\n\")\n",
    "    \n",
    "    return malicious_target_logits.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222d290e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample inputs for analysis\n",
    "def generate_sample_inputs(tokenizer, seq_length=8):    \n",
    "    sample_texts = [\n",
    "        \"The capital of France is\",\n",
    "        \"The largest mammal on Earth is\",\n",
    "        \"The process of photosynthesis occurs in\"\n",
    "    ]\n",
    "    \n",
    "    inputs = []\n",
    "    for i in range(len(sample_texts)):\n",
    "        tokenized = tokenizer(\n",
    "            sample_texts[i], \n",
    "            return_tensors=\"pt\", \n",
    "            padding='max_length', \n",
    "            truncation=True, \n",
    "            max_length=seq_length\n",
    "        )\n",
    "        inputs.append(tokenized.input_ids.to(model.device))\n",
    "    \n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba4620b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_activation_differences_llama(model, X_data, n_samples=5, n_reconstructions=3):\n",
    "    results = []\n",
    "    \n",
    "    layer_names = [f'model.layers.{i}' for i in range(len(model.model.layers))]\n",
    "    \n",
    "    for sample_idx in tqdm(range(min(n_samples, len(X_data))), desc=\"Processing samples\"):\n",
    "        original_input = X_data[sample_idx]\n",
    "        \n",
    "        # Get original activations\n",
    "        original_activations = {}\n",
    "        hooks = []\n",
    "        for layer_name in layer_names:\n",
    "            layer_module = model.get_submodule(layer_name)\n",
    "            hooks.append(layer_module.register_forward_hook(get_activation(layer_name, original_activations)))\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            original_output = model(original_input).logits\n",
    "        \n",
    "        for hook in hooks:\n",
    "            hook.remove()\n",
    "        \n",
    "        # --- ATTACK STEP ---\n",
    "        # Create the malicious target for reconstruction\n",
    "        malicious_target_logits = create_malicious_output(tokenizer, original_output)\n",
    "        \n",
    "        # Multiple reconstruction attempts\n",
    "        for recon_idx in range(n_reconstructions):\n",
    "            seq_length = original_input.shape[1]\n",
    "            embedding_dim = model.config.hidden_size\n",
    "            \n",
    "            reconstructed_embeddings = torch.randn(\n",
    "                1, seq_length, embedding_dim,\n",
    "                device=model.device,\n",
    "                dtype=torch.float32,\n",
    "                requires_grad=True\n",
    "            )\n",
    "            \n",
    "            optimizer = optim.Adam([reconstructed_embeddings], lr=0.01)\n",
    "            \n",
    "            # Reconstruction optimization\n",
    "            for iteration in tqdm(range(5000), desc=f\"Recon {recon_idx+1}/{n_reconstructions}\", leave=False): \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                embeddings_model_dtype = reconstructed_embeddings.to(model.dtype)\n",
    "                output = model(inputs_embeds=embeddings_model_dtype).logits\n",
    "                \n",
    "                # MODIFIED: Loss now matches the MALICIOUS target\n",
    "                loss = nn.functional.mse_loss(output.float(), malicious_target_logits.float())\n",
    "                \n",
    "                reg_loss = 0.001 * torch.mean(reconstructed_embeddings ** 2)\n",
    "                total_loss = loss + reg_loss\n",
    "                \n",
    "                total_loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                if total_loss.item() < 1e-4:\n",
    "                    break\n",
    "            \n",
    "            print(f\"\\nSample {sample_idx}, Recon {recon_idx}, Final Loss: {total_loss.item():.6f}\")\n",
    "\n",
    "            # Get reconstructed activations\n",
    "            reconstructed_activations = {}\n",
    "            hooks = []\n",
    "            for layer_name in layer_names:\n",
    "                layer_module = model.get_submodule(layer_name)\n",
    "                hooks.append(layer_module.register_forward_hook(get_activation(layer_name, reconstructed_activations)))\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                embeddings_model_dtype = reconstructed_embeddings.to(model.dtype)\n",
    "                _ = model(inputs_embeds=embeddings_model_dtype)\n",
    "            \n",
    "            for hook in hooks:\n",
    "                hook.remove()\n",
    "            \n",
    "            # Calculate differences for each layer\n",
    "            row = {'sample_idx': sample_idx, 'reconstruction_idx': recon_idx}\n",
    "            all_layer_max_diffs = []\n",
    "            \n",
    "            for layer_name in layer_names:\n",
    "                if layer_name in original_activations and layer_name in reconstructed_activations:\n",
    "                    orig_act = original_activations[layer_name].flatten().float()\n",
    "                    recon_act = reconstructed_activations[layer_name].flatten().float()\n",
    "                    \n",
    "                    abs_diff = torch.abs(orig_act - recon_act)\n",
    "                    \n",
    "                    layer_num = layer_name.split('.')[-1]\n",
    "                    row[f'layer_{layer_num}_min_abs_diff'] = abs_diff.min().item()\n",
    "                    row[f'layer_{layer_num}_mean_abs_diff'] = abs_diff.mean().item()\n",
    "                    row[f'layer_{layer_num}_max_abs_diff'] = abs_diff.max().item()\n",
    "                    \n",
    "                    all_layer_max_diffs.append(abs_diff.max().item())\n",
    "            \n",
    "            if all_layer_max_diffs:\n",
    "                row['all_layers_max_diff'] = max(all_layer_max_diffs)\n",
    "                row['all_layers_min_of_max'] = min(all_layer_max_diffs)\n",
    "            \n",
    "            results.append(row)\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b138e55c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating sample inputs...\n",
      "Generated 3 samples\n",
      "Generating activation differences for Llama-2 layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing samples:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logit Swap: Swapping '\t' with 'Хронологија'\n",
      "Before Swap - Correct Token Logit: -12.78125, Incorrect Token Logit: -12.78125\n",
      "Before Swap - Predicted Token: '\n",
      "'\n",
      "After Swap - Predicted Token will be: '\n",
      "'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing samples:   0%|          | 0/2 [00:27<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling `cublasGemmEx( handle, opa, opb, m, n, k, &falpha, a, CUDA_R_16F, lda, b, CUDA_R_16F, ldb, &fbeta, c, CUDA_R_16F, ldc, CUDA_R_32F, CUBLAS_GEMM_DEFAULT_TENSOR_OP)`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerating activation differences for Llama-2 layers...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Using fewer samples/reconstructions for a quicker demonstration\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m results_df \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_activation_differences_llama\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_reconstructions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# %%\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Save results\u001b[39;00m\n\u001b[0;32m     14\u001b[0m results_df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mllama2_swap_attack_results.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[7], line 53\u001b[0m, in \u001b[0;36mgenerate_activation_differences_llama\u001b[1;34m(model, X_data, n_samples, n_reconstructions)\u001b[0m\n\u001b[0;32m     50\u001b[0m reg_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.001\u001b[39m \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(reconstructed_embeddings \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     51\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m+\u001b[39m reg_loss\n\u001b[1;32m---> 53\u001b[0m \u001b[43mtotal_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m total_loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1e-4\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling `cublasGemmEx( handle, opa, opb, m, n, k, &falpha, a, CUDA_R_16F, lda, b, CUDA_R_16F, ldb, &fbeta, c, CUDA_R_16F, ldc, CUDA_R_32F, CUBLAS_GEMM_DEFAULT_TENSOR_OP)`"
     ]
    }
   ],
   "source": [
    "# Generate sample data\n",
    "print(\"Generating sample inputs...\")\n",
    "X_data = generate_sample_inputs(tokenizer, seq_length=15) \n",
    "print(f\"Generated {len(X_data)} samples\")\n",
    "\n",
    "# %%\n",
    "# Generate results\n",
    "print(\"Generating activation differences for Llama-2 layers...\")\n",
    "# Using fewer samples/reconstructions for a quicker demonstration\n",
    "results_df = generate_activation_differences_llama(model, X_data, n_samples=3, n_reconstructions=3)\n",
    "\n",
    "# %%\n",
    "# Save results\n",
    "results_df.to_csv('llama2_swap_attack_results.csv', index=False)\n",
    "print(f\"\\nResults saved to 'llama2_swap_attack_results.csv'. Shape: {results_df.shape}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(results_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7a3e2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
