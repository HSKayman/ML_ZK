{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcc5fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "import os\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import json\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868e3f7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83ed238",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_1_PATH = \"meta-llama/Llama-2-7b-chat-hf\" \n",
    "\n",
    "# %%\n",
    "tokenizer = LlamaTokenizer.from_pretrained(MODEL_1_PATH)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    MODEL_1_PATH,\n",
    "    torch_dtype=torch.float16,   \n",
    "    device_map=\"auto\"           \n",
    ")\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96472f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_prompt_logits(prompt: str, \n",
    "                          model: LlamaForCausalLM, \n",
    "                          tokenizer: LlamaTokenizer, \n",
    "                          top_k: int = 5):\n",
    "    \n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "    input_ids = inputs.input_ids[0] # Get the 1D tensor of token IDs\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits[0] \n",
    "        \n",
    "    probabilities = F.softmax(logits, dim=-1)\n",
    "    \n",
    "    analysis_results = []\n",
    "\n",
    "    # Iterate through the sequence, token by token\n",
    "    for i in range(len(input_ids) - 1):\n",
    "        current_token_id = input_ids[i].item()\n",
    "        current_token_str = tokenizer.decode(current_token_id)\n",
    "        \n",
    "        actual_next_token_id = input_ids[i+1].item()\n",
    "        actual_next_token_str = tokenizer.decode(actual_next_token_id)\n",
    "\n",
    "        logits_for_next = logits[i]\n",
    "        probs_for_next = probabilities[i]\n",
    "\n",
    "        actual_token_logit = logits_for_next[actual_next_token_id].item()\n",
    "        actual_token_prob = probs_for_next[actual_next_token_id].item()\n",
    "\n",
    "        top_k_probs, top_k_ids = torch.topk(probs_for_next, top_k)\n",
    "        \n",
    "        top_k_predictions_data = []\n",
    "        for j in range(top_k):\n",
    "            pred_id = top_k_ids[j].item()\n",
    "            pred_prob = top_k_probs[j].item()\n",
    "            pred_logit = logits_for_next[pred_id].item() # Get corresponding logit\n",
    "            pred_str = tokenizer.decode(pred_id)\n",
    "            \n",
    "            top_k_predictions_data.append({\n",
    "                \"word\": pred_str,\n",
    "                \"token_id\": pred_id,\n",
    "                \"probability\": pred_prob,\n",
    "                \"logit\": pred_logit\n",
    "            })\n",
    "\n",
    "        analysis_results.append({\n",
    "            \"step\": i,\n",
    "            \"current_token\": f\"{current_token_str} (ID: {current_token_id})\",\n",
    "            \"actual_next_token\": f\"{actual_next_token_str} (ID: {actual_next_token_id})\",\n",
    "            \"actual_token_logit\": actual_token_logit,\n",
    "            \"actual_token_prob\": actual_token_prob,\n",
    "            \"top_k_predictions\": top_k_predictions_data\n",
    "        })\n",
    "\n",
    "    last_logits = logits[-1]\n",
    "    last_probs = probabilities[-1]\n",
    "    \n",
    "    top_k_probs, top_k_ids = torch.topk(last_probs, top_k)\n",
    "    top_k_predictions_data = []\n",
    "    \n",
    "    for j in range(top_k):\n",
    "        pred_id = top_k_ids[j].item()\n",
    "        pred_prob = top_k_probs[j].item()\n",
    "        pred_logit = last_logits[pred_id].item()\n",
    "        pred_str = tokenizer.decode(pred_id)\n",
    "        \n",
    "        top_k_predictions_data.append({\n",
    "            \"word\": pred_str,\n",
    "            \"token_id\": pred_id,\n",
    "            \"probability\": pred_prob,\n",
    "            \"logit\": pred_logit\n",
    "        })\n",
    "\n",
    "    analysis_results.append({\n",
    "        \"step\": len(input_ids) - 1,\n",
    "        \"current_token\": f\"{tokenizer.decode(input_ids[-1].item())} (ID: {input_ids[-1].item()})\",\n",
    "        \"actual_next_token\": \"N/A (End of Prompt)\",\n",
    "        \"actual_token_logit\": \"N/A\",\n",
    "        \"actual_token_prob\": \"N/A\",\n",
    "        \"top_k_predictions\": top_k_predictions_data\n",
    "    })\n",
    "    \n",
    "    return analysis_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcef90b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_analysis(analysis_results):\n",
    "    for result in analysis_results:\n",
    "        print(f\"\\n==========================================\")\n",
    "        print(f\" Step {result['step']} | Input Token: {result['current_token']}\")\n",
    "        print(f\"==========================================\")\n",
    "        \n",
    "        if result['actual_next_token'] != \"N/A (End of Prompt)\":\n",
    "            print(f\"Actual Next Token: {result['actual_next_token']}\")\n",
    "            print(f\"  > Model's Logit for this token: {result['actual_token_logit']:.4f}\")\n",
    "            print(f\"  > Model's Prob for this token:  {result['actual_token_prob']:.4f}\")\n",
    "        else:\n",
    "            print(\"--- End of Prompt ---\")\n",
    "\n",
    "        print(\"\\nModel's Top-5 Predictions for *this* position:\")\n",
    "        print(\"----------------------------------------------\")\n",
    "        print(f\"{'Rank':<5} | {'Word':<15} | {'Token ID':<8} | {'Logit':<10} | {'Probability':<10}\")\n",
    "        print(f\"-------------------------------------------------------------------\")\n",
    "        \n",
    "        for rank, pred in enumerate(result['top_k_predictions'], 1):\n",
    "            word_str = f\"'{pred['word']}'\"\n",
    "            print(f\"{rank:<5} | {word_str:<15} | {pred['token_id']:<8} | {pred['logit']:<10.4f} | {pred['probability']:<10.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80506dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "def save_analysis_to_file(analysis_results, output_filename: str):\n",
    "    print(f\"\\n--- Redirecting detailed analysis to '{output_filename}' ---\")\n",
    "    \n",
    "    # Save the current standard output\n",
    "    original_stdout = sys.stdout \n",
    "    \n",
    "    try:\n",
    "        # Redirect standard output to the file\n",
    "        with open(output_filename, 'w') as f:\n",
    "            sys.stdout = f\n",
    "            print_analysis(analysis_results)\n",
    "        print(f\"✅ Successfully saved detailed analysis to '{output_filename}'\")\n",
    "        \n",
    "    finally:\n",
    "        # Restore the original standard output (the console)\n",
    "        sys.stdout = original_stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37f1c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_all_logits_for_last_token(prompt: str, \n",
    "                                   model: LlamaForCausalLM, \n",
    "                                   tokenizer: LlamaTokenizer, \n",
    "                                   output_filename: str = \"last_token_logits.json\"):\n",
    "    print(f\"Analyzing prompt: '{prompt}'\")\n",
    "    \n",
    "    #Get model outputs\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # for the very last token prediction\n",
    "    \n",
    "    last_logits = outputs.logits[0, -1, :]\n",
    "    \n",
    "    # probabilities for the entire vocabulary\n",
    "    last_probs = F.softmax(last_logits, dim=-1)\n",
    "    \n",
    "    # 4. Sort \n",
    "    sorted_probs, sorted_indices = torch.sort(last_probs, descending=True)\n",
    "    \n",
    "    # 5. all token data\n",
    "    all_logits_data = []\n",
    "    for i in range(len(sorted_indices)):\n",
    "        token_id = sorted_indices[i].item()\n",
    "        prob = sorted_probs[i].item()\n",
    "        logit = last_logits[token_id].item() \n",
    "        token_str = tokenizer.decode(token_id)\n",
    "        \n",
    "        all_logits_data.append({\n",
    "            'rank': i + 1,\n",
    "            'token': token_str,\n",
    "            'token_id': token_id,\n",
    "            'probability': prob,\n",
    "            'logit': logit\n",
    "        })\n",
    "        \n",
    "    with open(output_filename, 'w') as f:\n",
    "        json.dump(all_logits_data, f, indent=4)\n",
    "        \n",
    "    print(f\"✅ Successfully saved all {len(all_logits_data)} logits to '{output_filename}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94865c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_texts = [\n",
    "        \"The capital of France is\",\n",
    "        \"The largest mammal on Earth is\",\n",
    "        \"The process of photosynthesis occurs in\"\n",
    "    ]\n",
    "\n",
    "\n",
    "for i in range(len(sample_texts)):\n",
    "    print(f\"\\n--- Sample Prompt {i+1} ---\")\n",
    "    print(sample_texts[i])\n",
    "    target_prompt = sample_texts[i]\n",
    "    detailed_analysis_file = sample_texts[i].replace(\" \", \"_\").lower() + \"_detailed_analysis.txt\"\n",
    "    all_logits_file = sample_texts[i].replace(\" \", \"_\").lower() + \"_all_logits.json\"\n",
    "\n",
    "\n",
    "    analysis = analyze_prompt_logits(target_prompt, model, tokenizer, top_k=50)\n",
    "    save_analysis_to_file(analysis, detailed_analysis_file)\n",
    "    save_all_logits_for_last_token(target_prompt, model, tokenizer, output_filename=all_logits_file)\n",
    "\n",
    "    # --- Optional: Print a brief summary to the console ---\n",
    "    print(\"\\n--- Console Summary---\")\n",
    "    print_analysis([analysis[-1]])\n",
    "\n",
    "\n",
    "# Clean up to free VRAM\n",
    "print(\"\\nCleaning up model and tokenizer from memory.\")\n",
    "del model\n",
    "del tokenizer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
