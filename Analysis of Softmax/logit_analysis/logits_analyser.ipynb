{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcc5fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "import os\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import json\n",
    "import gc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868e3f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83ed238",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_1_PATH = \"meta-llama/Llama-2-7b-chat-hf\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed312e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = LlamaTokenizer.from_pretrained(MODEL_1_PATH)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    MODEL_1_PATH,\n",
    "    torch_dtype=torch.float16,  \n",
    "    device_map=\"auto\"           \n",
    ")\n",
    "\n",
    "model.eval() # Set model to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96472f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_prompt_logits(prompt: str, \n",
    "                          model: LlamaForCausalLM, \n",
    "                          tokenizer: LlamaTokenizer, \n",
    "                          top_k: int = 5):\n",
    "    \n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "    input_ids = inputs.input_ids[0] # Get the 1D tensor of token IDs\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits[0] \n",
    "        \n",
    "    probabilities = F.softmax(logits, dim=-1)\n",
    "    \n",
    "    analysis_results = []\n",
    "\n",
    "    # Iterate through the sequence, token by token\n",
    "    for i in range(len(input_ids) - 1):\n",
    "        current_token_id = input_ids[i].item()\n",
    "        current_token_str = tokenizer.decode(current_token_id)\n",
    "        \n",
    "        actual_next_token_id = input_ids[i+1].item()\n",
    "        actual_next_token_str = tokenizer.decode(actual_next_token_id)\n",
    "\n",
    "        logits_for_next = logits[i]\n",
    "        probs_for_next = probabilities[i]\n",
    "\n",
    "        actual_token_logit = logits_for_next[actual_next_token_id].item()\n",
    "        actual_token_prob = probs_for_next[actual_next_token_id].item()\n",
    "\n",
    "        top_k_probs, top_k_ids = torch.topk(probs_for_next, top_k)\n",
    "        \n",
    "        top_k_predictions_data = []\n",
    "        for j in range(top_k):\n",
    "            pred_id = top_k_ids[j].item()\n",
    "            pred_prob = top_k_probs[j].item()\n",
    "            pred_logit = logits_for_next[pred_id].item() # Get corresponding logit\n",
    "            pred_str = tokenizer.decode(pred_id)\n",
    "            \n",
    "            top_k_predictions_data.append({\n",
    "                \"word\": pred_str,\n",
    "                \"token_id\": pred_id,\n",
    "                \"probability\": pred_prob,\n",
    "                \"logit\": pred_logit\n",
    "            })\n",
    "\n",
    "        analysis_results.append({\n",
    "            \"step\": i,\n",
    "            \"current_token\": f\"{current_token_str} (ID: {current_token_id})\",\n",
    "            \"actual_next_token\": f\"{actual_next_token_str} (ID: {actual_next_token_id})\",\n",
    "            \"actual_token_logit\": actual_token_logit,\n",
    "            \"actual_token_prob\": actual_token_prob,\n",
    "            \"top_k_predictions\": top_k_predictions_data\n",
    "        })\n",
    "\n",
    "    last_logits = logits[-1]\n",
    "    last_probs = probabilities[-1]\n",
    "    \n",
    "    top_k_probs, top_k_ids = torch.topk(last_probs, top_k)\n",
    "    top_k_predictions_data = []\n",
    "    \n",
    "    for j in range(top_k):\n",
    "        pred_id = top_k_ids[j].item()\n",
    "        pred_prob = top_k_probs[j].item()\n",
    "        pred_logit = last_logits[pred_id].item()\n",
    "        pred_str = tokenizer.decode(pred_id)\n",
    "        \n",
    "        top_k_predictions_data.append({\n",
    "            \"word\": pred_str,\n",
    "            \"token_id\": pred_id,\n",
    "            \"probability\": pred_prob,\n",
    "            \"logit\": pred_logit\n",
    "        })\n",
    "\n",
    "    analysis_results.append({\n",
    "        \"step\": len(input_ids) - 1,\n",
    "        \"current_token\": f\"{tokenizer.decode(input_ids[-1].item())} (ID: {input_ids[-1].item()})\",\n",
    "        \"actual_next_token\": \"N/A (End of Prompt)\",\n",
    "        \"actual_token_logit\": \"N/A\",\n",
    "        \"actual_token_prob\": \"N/A\",\n",
    "        \"top_k_predictions\": top_k_predictions_data\n",
    "    })\n",
    "    \n",
    "    return analysis_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcef90b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_analysis(analysis_results):\n",
    "    for result in analysis_results:\n",
    "        print(f\"\\n==========================================\")\n",
    "        print(f\" Step {result['step']} | Input Token: {result['current_token']}\")\n",
    "        print(f\"==========================================\")\n",
    "        \n",
    "        if result['actual_next_token'] != \"N/A (End of Prompt)\":\n",
    "            print(f\"Actual Next Token: {result['actual_next_token']}\")\n",
    "            print(f\"  > Model's Logit for this token: {result['actual_token_logit']:.4f}\")\n",
    "            print(f\"  > Model's Prob for this token:  {result['actual_token_prob']:.4f}\")\n",
    "        else:\n",
    "            print(\"--- End of Prompt ---\")\n",
    "\n",
    "        print(\"\\nModel's Top-5 Predictions for *this* position:\")\n",
    "        print(\"----------------------------------------------\")\n",
    "        print(f\"{'Rank':<5} | {'Word':<15} | {'Token ID':<8} | {'Logit':<10} | {'Probability':<10}\")\n",
    "        print(f\"-------------------------------------------------------------------\")\n",
    "        \n",
    "        for rank, pred in enumerate(result['top_k_predictions'], 1):\n",
    "            word_str = f\"'{pred['word']}'\"\n",
    "            print(f\"{rank:<5} | {word_str:<15} | {pred['token_id']:<8} | {pred['logit']:<10.4f} | {pred['probability']:<10.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94865c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_to_analyze = \"The capital of France is\"\n",
    "\n",
    "# Get the analysis\n",
    "analysis = analyze_prompt_logits(prompt_to_analyze, model, tokenizer, top_k=5)\n",
    "\n",
    "# Print the results\n",
    "print_analysis(analysis)\n",
    "\n",
    "# Clean up to free VRAM\n",
    "del model\n",
    "del tokenizer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c07766",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86312be6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0806015c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867325e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
