{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#!pip install torch \n",
    "#!pip install torchsummary\n",
    "#!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datetime import datetime\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(CNN, self).__init__()\n",
    "        # Input: 1x224x224\n",
    "        self.features = nn.Sequential(\n",
    "            # Conv1: (224+4-5)/1 + 1 = 224\n",
    "            nn.Conv2d(1, 6, kernel_size=5, stride=1, padding=2),  # Output: 6x224x224\n",
    "            nn.ReLU(inplace=True),\n",
    "            # MaxPool1: (224-2)/2 + 1 = 112\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # Output: 6x112x112\n",
    "            \n",
    "            # Conv2: (112+4-5)/1 + 1 = 112\n",
    "            nn.Conv2d(6, 16, kernel_size=5, stride=1, padding=2),  # Output: 16x112x112\n",
    "            nn.ReLU(inplace=True),\n",
    "            # MaxPool2: (112-2)/2 + 1 = 56\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # Output: 16x56x56\n",
    "\n",
    "            # Conv3: (56+2-3)/2 + 1 = 28\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1), # Output: 32x28x28\n",
    "            nn.ReLU(inplace=True),\n",
    "            # MaxPool3: (28-2)/2 + 1 = 14\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), # Output: 32x14x14\n",
    "            \n",
    "            # Conv4: (14+2-3)/2 + 1 = 7\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1), # Output: 64x7x7\n",
    "            nn.ReLU(inplace=True),\n",
    "            # MaxPool4: (7-2)/2 + 1 = 3\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), # Output: 64x3x3\n",
    "        )\n",
    "        \n",
    "        # Calculate flattened size: 64 * 3 * 3 = 576\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(64 * 3 * 3, 256),  # 3136 -> 512\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(64, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convolutional layers\n",
    "        x = self.features(x)\n",
    "\n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 6, 224, 224]             156\n",
      "              ReLU-2          [-1, 6, 224, 224]               0\n",
      "         MaxPool2d-3          [-1, 6, 112, 112]               0\n",
      "            Conv2d-4         [-1, 16, 112, 112]           2,416\n",
      "              ReLU-5         [-1, 16, 112, 112]               0\n",
      "         MaxPool2d-6           [-1, 16, 56, 56]               0\n",
      "            Conv2d-7           [-1, 32, 28, 28]           4,640\n",
      "              ReLU-8           [-1, 32, 28, 28]               0\n",
      "         MaxPool2d-9           [-1, 32, 14, 14]               0\n",
      "           Conv2d-10             [-1, 64, 7, 7]          18,496\n",
      "             ReLU-11             [-1, 64, 7, 7]               0\n",
      "        MaxPool2d-12             [-1, 64, 3, 3]               0\n",
      "          Dropout-13                  [-1, 576]               0\n",
      "           Linear-14                  [-1, 256]         147,712\n",
      "             ReLU-15                  [-1, 256]               0\n",
      "          Dropout-16                  [-1, 256]               0\n",
      "           Linear-17                   [-1, 64]          16,448\n",
      "             ReLU-18                   [-1, 64]               0\n",
      "           Linear-19                    [-1, 2]             130\n",
      "================================================================\n",
      "Total params: 189,998\n",
      "Trainable params: 189,998\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.19\n",
      "Forward/backward pass size (MB): 9.11\n",
      "Params size (MB): 0.72\n",
      "Estimated Total Size (MB): 10.02\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model, (1, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Convert input picture to tensor\n",
    "matrix_converter = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),  # Convert to grayscale\n",
    "    transforms.Resize((224, 224)),                  # Resize to desired dimensions\n",
    "    transforms.ToTensor()    \n",
    "])\n",
    "\n",
    "# Set data set directory\n",
    "data_dir = 'data_for_model_1/train/'\n",
    "\n",
    "# Load data set\n",
    "dataset = datasets.ImageFolder(data_dir,transform=matrix_converter)\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noof classes: 2\n",
      "Classes: ['cat', 'dog']\n",
      "Total samples: 3000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Noof classes: {len(dataset.classes)}\")\n",
    "print(f\"Classes: {dataset.classes}\")\n",
    "print(f\"Total samples: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/40], Batch [10/94], Loss: 0.6984\n",
      "Epoch [1/40], Batch [20/94], Loss: 0.6926\n",
      "Epoch [1/40], Batch [30/94], Loss: 0.7026\n",
      "Epoch [1/40], Batch [40/94], Loss: 0.6839\n",
      "Epoch [1/40], Batch [50/94], Loss: 0.6904\n",
      "Epoch [1/40], Batch [60/94], Loss: 0.6940\n",
      "Epoch [1/40], Batch [70/94], Loss: 0.6939\n",
      "Epoch [1/40], Batch [80/94], Loss: 0.6980\n",
      "Epoch [1/40], Batch [90/94], Loss: 0.6932\n",
      "Epoch [1/40]\n",
      "Loss: 0.0217\n",
      "Accuracy: 49.23%\n",
      "------------------------------\n",
      "Epoch [2/40], Batch [10/94], Loss: 0.7007\n",
      "Epoch [2/40], Batch [20/94], Loss: 0.6980\n",
      "Epoch [2/40], Batch [30/94], Loss: 0.6973\n",
      "Epoch [2/40], Batch [40/94], Loss: 0.6943\n",
      "Epoch [2/40], Batch [50/94], Loss: 0.6899\n",
      "Epoch [2/40], Batch [60/94], Loss: 0.6969\n",
      "Epoch [2/40], Batch [70/94], Loss: 0.6912\n",
      "Epoch [2/40], Batch [80/94], Loss: 0.6949\n",
      "Epoch [2/40], Batch [90/94], Loss: 0.6947\n",
      "Epoch [2/40]\n",
      "Loss: 0.0217\n",
      "Accuracy: 50.00%\n",
      "------------------------------\n",
      "Epoch [3/40], Batch [10/94], Loss: 0.6946\n",
      "Epoch [3/40], Batch [20/94], Loss: 0.6912\n",
      "Epoch [3/40], Batch [30/94], Loss: 0.6950\n",
      "Epoch [3/40], Batch [40/94], Loss: 0.6920\n",
      "Epoch [3/40], Batch [50/94], Loss: 0.6889\n",
      "Epoch [3/40], Batch [60/94], Loss: 0.6928\n",
      "Epoch [3/40], Batch [70/94], Loss: 0.6928\n",
      "Epoch [3/40], Batch [80/94], Loss: 0.6944\n",
      "Epoch [3/40], Batch [90/94], Loss: 0.6890\n",
      "Epoch [3/40]\n",
      "Loss: 0.0217\n",
      "Accuracy: 50.37%\n",
      "------------------------------\n",
      "Epoch [4/40], Batch [10/94], Loss: 0.6963\n",
      "Epoch [4/40], Batch [20/94], Loss: 0.6906\n",
      "Epoch [4/40], Batch [30/94], Loss: 0.6692\n",
      "Epoch [4/40], Batch [40/94], Loss: 0.6990\n",
      "Epoch [4/40], Batch [50/94], Loss: 0.6940\n",
      "Epoch [4/40], Batch [60/94], Loss: 0.6858\n",
      "Epoch [4/40], Batch [70/94], Loss: 0.6938\n",
      "Epoch [4/40], Batch [80/94], Loss: 0.6657\n",
      "Epoch [4/40], Batch [90/94], Loss: 0.7090\n",
      "Epoch [4/40]\n",
      "Loss: 0.0216\n",
      "Accuracy: 51.97%\n",
      "------------------------------\n",
      "Epoch [5/40], Batch [10/94], Loss: 0.6721\n",
      "Epoch [5/40], Batch [20/94], Loss: 0.6890\n",
      "Epoch [5/40], Batch [30/94], Loss: 0.6746\n",
      "Epoch [5/40], Batch [40/94], Loss: 0.7136\n",
      "Epoch [5/40], Batch [50/94], Loss: 0.6924\n",
      "Epoch [5/40], Batch [60/94], Loss: 0.6699\n",
      "Epoch [5/40], Batch [70/94], Loss: 0.7332\n",
      "Epoch [5/40], Batch [80/94], Loss: 0.6868\n",
      "Epoch [5/40], Batch [90/94], Loss: 0.6920\n",
      "Epoch [5/40]\n",
      "Loss: 0.0216\n",
      "Accuracy: 53.90%\n",
      "------------------------------\n",
      "Epoch [6/40], Batch [10/94], Loss: 0.6730\n",
      "Epoch [6/40], Batch [20/94], Loss: 0.6936\n",
      "Epoch [6/40], Batch [30/94], Loss: 0.6821\n",
      "Epoch [6/40], Batch [40/94], Loss: 0.6931\n",
      "Epoch [6/40], Batch [50/94], Loss: 0.6823\n",
      "Epoch [6/40], Batch [60/94], Loss: 0.6633\n",
      "Epoch [6/40], Batch [70/94], Loss: 0.6785\n",
      "Epoch [6/40], Batch [80/94], Loss: 0.6940\n",
      "Epoch [6/40], Batch [90/94], Loss: 0.6815\n",
      "Epoch [6/40]\n",
      "Loss: 0.0214\n",
      "Accuracy: 55.87%\n",
      "------------------------------\n",
      "Epoch [7/40], Batch [10/94], Loss: 0.6979\n",
      "Epoch [7/40], Batch [20/94], Loss: 0.6351\n",
      "Epoch [7/40], Batch [30/94], Loss: 0.6407\n",
      "Epoch [7/40], Batch [40/94], Loss: 0.6397\n",
      "Epoch [7/40], Batch [50/94], Loss: 0.6664\n",
      "Epoch [7/40], Batch [60/94], Loss: 0.7016\n",
      "Epoch [7/40], Batch [70/94], Loss: 0.7527\n",
      "Epoch [7/40], Batch [80/94], Loss: 0.7123\n",
      "Epoch [7/40], Batch [90/94], Loss: 0.6284\n",
      "Epoch [7/40]\n",
      "Loss: 0.0211\n",
      "Accuracy: 57.33%\n",
      "------------------------------\n",
      "Epoch [8/40], Batch [10/94], Loss: 0.7427\n",
      "Epoch [8/40], Batch [20/94], Loss: 0.6476\n",
      "Epoch [8/40], Batch [30/94], Loss: 0.7211\n",
      "Epoch [8/40], Batch [40/94], Loss: 0.7304\n",
      "Epoch [8/40], Batch [50/94], Loss: 0.5554\n",
      "Epoch [8/40], Batch [60/94], Loss: 0.9323\n",
      "Epoch [8/40], Batch [70/94], Loss: 0.6313\n",
      "Epoch [8/40], Batch [80/94], Loss: 0.7138\n",
      "Epoch [8/40], Batch [90/94], Loss: 0.6604\n",
      "Epoch [8/40]\n",
      "Loss: 0.0209\n",
      "Accuracy: 58.73%\n",
      "------------------------------\n",
      "Epoch [9/40], Batch [10/94], Loss: 0.6797\n",
      "Epoch [9/40], Batch [20/94], Loss: 0.6822\n",
      "Epoch [9/40], Batch [30/94], Loss: 0.6683\n",
      "Epoch [9/40], Batch [40/94], Loss: 0.5972\n",
      "Epoch [9/40], Batch [50/94], Loss: 0.6820\n",
      "Epoch [9/40], Batch [60/94], Loss: 0.6340\n",
      "Epoch [9/40], Batch [70/94], Loss: 0.6378\n",
      "Epoch [9/40], Batch [80/94], Loss: 0.6718\n",
      "Epoch [9/40], Batch [90/94], Loss: 0.7323\n",
      "Epoch [9/40]\n",
      "Loss: 0.0209\n",
      "Accuracy: 59.57%\n",
      "------------------------------\n",
      "Epoch [10/40], Batch [10/94], Loss: 0.6402\n",
      "Epoch [10/40], Batch [20/94], Loss: 0.6740\n",
      "Epoch [10/40], Batch [30/94], Loss: 0.6807\n",
      "Epoch [10/40], Batch [40/94], Loss: 0.5604\n",
      "Epoch [10/40], Batch [50/94], Loss: 0.6875\n",
      "Epoch [10/40], Batch [60/94], Loss: 0.7104\n",
      "Epoch [10/40], Batch [70/94], Loss: 0.6247\n",
      "Epoch [10/40], Batch [80/94], Loss: 0.5812\n",
      "Epoch [10/40], Batch [90/94], Loss: 0.6907\n",
      "Epoch [10/40]\n",
      "Loss: 0.0207\n",
      "Accuracy: 59.80%\n",
      "------------------------------\n",
      "Epoch [11/40], Batch [10/94], Loss: 0.6816\n",
      "Epoch [11/40], Batch [20/94], Loss: 0.6608\n",
      "Epoch [11/40], Batch [30/94], Loss: 0.7210\n",
      "Epoch [11/40], Batch [40/94], Loss: 0.5816\n",
      "Epoch [11/40], Batch [50/94], Loss: 0.5943\n",
      "Epoch [11/40], Batch [60/94], Loss: 0.7155\n",
      "Epoch [11/40], Batch [70/94], Loss: 0.6236\n",
      "Epoch [11/40], Batch [80/94], Loss: 0.5843\n",
      "Epoch [11/40], Batch [90/94], Loss: 0.6156\n",
      "Epoch [11/40]\n",
      "Loss: 0.0205\n",
      "Accuracy: 60.63%\n",
      "------------------------------\n",
      "Epoch [12/40], Batch [10/94], Loss: 0.6823\n",
      "Epoch [12/40], Batch [20/94], Loss: 0.5410\n",
      "Epoch [12/40], Batch [30/94], Loss: 0.5653\n",
      "Epoch [12/40], Batch [40/94], Loss: 0.6112\n",
      "Epoch [12/40], Batch [50/94], Loss: 0.6609\n",
      "Epoch [12/40], Batch [60/94], Loss: 0.7030\n",
      "Epoch [12/40], Batch [70/94], Loss: 0.6564\n",
      "Epoch [12/40], Batch [80/94], Loss: 0.5961\n",
      "Epoch [12/40], Batch [90/94], Loss: 0.6739\n",
      "Epoch [12/40]\n",
      "Loss: 0.0200\n",
      "Accuracy: 62.80%\n",
      "------------------------------\n",
      "Epoch [13/40], Batch [10/94], Loss: 0.5718\n",
      "Epoch [13/40], Batch [20/94], Loss: 0.6169\n",
      "Epoch [13/40], Batch [30/94], Loss: 0.5661\n",
      "Epoch [13/40], Batch [40/94], Loss: 0.5944\n",
      "Epoch [13/40], Batch [50/94], Loss: 0.6140\n",
      "Epoch [13/40], Batch [60/94], Loss: 0.6150\n",
      "Epoch [13/40], Batch [70/94], Loss: 0.5499\n",
      "Epoch [13/40], Batch [80/94], Loss: 0.5983\n",
      "Epoch [13/40], Batch [90/94], Loss: 0.5849\n",
      "Epoch [13/40]\n",
      "Loss: 0.0194\n",
      "Accuracy: 65.37%\n",
      "------------------------------\n",
      "Epoch [14/40], Batch [10/94], Loss: 0.5709\n",
      "Epoch [14/40], Batch [20/94], Loss: 0.5361\n",
      "Epoch [14/40], Batch [30/94], Loss: 0.6171\n",
      "Epoch [14/40], Batch [40/94], Loss: 0.6157\n",
      "Epoch [14/40], Batch [50/94], Loss: 0.5540\n",
      "Epoch [14/40], Batch [60/94], Loss: 0.5631\n",
      "Epoch [14/40], Batch [70/94], Loss: 0.6363\n",
      "Epoch [14/40], Batch [80/94], Loss: 0.6041\n",
      "Epoch [14/40], Batch [90/94], Loss: 0.5685\n",
      "Epoch [14/40]\n",
      "Loss: 0.0189\n",
      "Accuracy: 67.50%\n",
      "------------------------------\n",
      "Epoch [15/40], Batch [10/94], Loss: 0.5069\n",
      "Epoch [15/40], Batch [20/94], Loss: 0.5593\n",
      "Epoch [15/40], Batch [30/94], Loss: 0.5457\n",
      "Epoch [15/40], Batch [40/94], Loss: 0.5765\n",
      "Epoch [15/40], Batch [50/94], Loss: 0.6956\n",
      "Epoch [15/40], Batch [60/94], Loss: 0.5087\n",
      "Epoch [15/40], Batch [70/94], Loss: 0.4624\n",
      "Epoch [15/40], Batch [80/94], Loss: 0.5729\n",
      "Epoch [15/40], Batch [90/94], Loss: 0.5322\n",
      "Epoch [15/40]\n",
      "Loss: 0.0183\n",
      "Accuracy: 69.40%\n",
      "------------------------------\n",
      "Epoch [16/40], Batch [10/94], Loss: 0.5929\n",
      "Epoch [16/40], Batch [20/94], Loss: 0.4215\n",
      "Epoch [16/40], Batch [30/94], Loss: 0.5868\n",
      "Epoch [16/40], Batch [40/94], Loss: 0.5343\n",
      "Epoch [16/40], Batch [50/94], Loss: 0.6411\n",
      "Epoch [16/40], Batch [60/94], Loss: 0.5082\n",
      "Epoch [16/40], Batch [70/94], Loss: 0.5676\n",
      "Epoch [16/40], Batch [80/94], Loss: 0.5357\n",
      "Epoch [16/40], Batch [90/94], Loss: 0.4049\n",
      "Epoch [16/40]\n",
      "Loss: 0.0183\n",
      "Accuracy: 69.57%\n",
      "------------------------------\n",
      "Epoch [17/40], Batch [10/94], Loss: 0.5480\n",
      "Epoch [17/40], Batch [20/94], Loss: 0.5397\n",
      "Epoch [17/40], Batch [30/94], Loss: 0.6125\n",
      "Epoch [17/40], Batch [40/94], Loss: 0.6923\n",
      "Epoch [17/40], Batch [50/94], Loss: 0.8177\n",
      "Epoch [17/40], Batch [60/94], Loss: 0.8091\n",
      "Epoch [17/40], Batch [70/94], Loss: 0.5630\n",
      "Epoch [17/40], Batch [80/94], Loss: 0.5033\n",
      "Epoch [17/40], Batch [90/94], Loss: 0.4897\n",
      "Epoch [17/40]\n",
      "Loss: 0.0171\n",
      "Accuracy: 73.30%\n",
      "------------------------------\n",
      "Epoch [18/40], Batch [10/94], Loss: 0.4880\n",
      "Epoch [18/40], Batch [20/94], Loss: 0.5446\n",
      "Epoch [18/40], Batch [30/94], Loss: 0.4689\n",
      "Epoch [18/40], Batch [40/94], Loss: 0.4772\n",
      "Epoch [18/40], Batch [50/94], Loss: 0.6709\n",
      "Epoch [18/40], Batch [60/94], Loss: 0.5893\n",
      "Epoch [18/40], Batch [70/94], Loss: 0.4896\n",
      "Epoch [18/40], Batch [80/94], Loss: 0.6446\n",
      "Epoch [18/40], Batch [90/94], Loss: 0.6704\n",
      "Epoch [18/40]\n",
      "Loss: 0.0173\n",
      "Accuracy: 71.53%\n",
      "------------------------------\n",
      "Epoch [19/40], Batch [10/94], Loss: 0.4777\n",
      "Epoch [19/40], Batch [20/94], Loss: 0.5227\n",
      "Epoch [19/40], Batch [30/94], Loss: 0.5675\n",
      "Epoch [19/40], Batch [40/94], Loss: 0.3714\n",
      "Epoch [19/40], Batch [50/94], Loss: 0.7596\n",
      "Epoch [19/40], Batch [60/94], Loss: 0.6450\n",
      "Epoch [19/40], Batch [70/94], Loss: 0.4560\n",
      "Epoch [19/40], Batch [80/94], Loss: 0.5994\n",
      "Epoch [19/40], Batch [90/94], Loss: 0.3798\n",
      "Epoch [19/40]\n",
      "Loss: 0.0163\n",
      "Accuracy: 74.27%\n",
      "------------------------------\n",
      "Epoch [20/40], Batch [10/94], Loss: 0.4849\n",
      "Epoch [20/40], Batch [20/94], Loss: 0.4242\n",
      "Epoch [20/40], Batch [30/94], Loss: 0.4116\n",
      "Epoch [20/40], Batch [40/94], Loss: 0.4580\n",
      "Epoch [20/40], Batch [50/94], Loss: 0.4909\n",
      "Epoch [20/40], Batch [60/94], Loss: 0.4512\n",
      "Epoch [20/40], Batch [70/94], Loss: 0.4651\n",
      "Epoch [20/40], Batch [80/94], Loss: 0.5279\n",
      "Epoch [20/40], Batch [90/94], Loss: 0.6155\n",
      "Epoch [20/40]\n",
      "Loss: 0.0164\n",
      "Accuracy: 74.03%\n",
      "------------------------------\n",
      "Epoch [21/40], Batch [10/94], Loss: 0.4985\n",
      "Epoch [21/40], Batch [20/94], Loss: 0.4725\n",
      "Epoch [21/40], Batch [30/94], Loss: 0.3637\n",
      "Epoch [21/40], Batch [40/94], Loss: 0.4879\n",
      "Epoch [21/40], Batch [50/94], Loss: 0.6929\n",
      "Epoch [21/40], Batch [60/94], Loss: 0.4546\n",
      "Epoch [21/40], Batch [70/94], Loss: 0.4260\n",
      "Epoch [21/40], Batch [80/94], Loss: 0.5358\n",
      "Epoch [21/40], Batch [90/94], Loss: 0.3052\n",
      "Epoch [21/40]\n",
      "Loss: 0.0155\n",
      "Accuracy: 75.77%\n",
      "------------------------------\n",
      "Epoch [22/40], Batch [10/94], Loss: 0.5077\n",
      "Epoch [22/40], Batch [20/94], Loss: 0.4603\n",
      "Epoch [22/40], Batch [30/94], Loss: 0.4007\n",
      "Epoch [22/40], Batch [40/94], Loss: 0.6764\n",
      "Epoch [22/40], Batch [50/94], Loss: 0.4678\n",
      "Epoch [22/40], Batch [60/94], Loss: 0.3731\n",
      "Epoch [22/40], Batch [70/94], Loss: 0.4734\n",
      "Epoch [22/40], Batch [80/94], Loss: 0.4323\n",
      "Epoch [22/40], Batch [90/94], Loss: 0.3459\n",
      "Epoch [22/40]\n",
      "Loss: 0.0152\n",
      "Accuracy: 76.30%\n",
      "------------------------------\n",
      "Epoch [23/40], Batch [10/94], Loss: 0.5486\n",
      "Epoch [23/40], Batch [20/94], Loss: 0.3502\n",
      "Epoch [23/40], Batch [30/94], Loss: 0.4098\n",
      "Epoch [23/40], Batch [40/94], Loss: 0.4040\n",
      "Epoch [23/40], Batch [50/94], Loss: 0.5717\n",
      "Epoch [23/40], Batch [60/94], Loss: 0.4086\n",
      "Epoch [23/40], Batch [70/94], Loss: 0.5844\n",
      "Epoch [23/40], Batch [80/94], Loss: 0.3344\n",
      "Epoch [23/40], Batch [90/94], Loss: 0.3747\n",
      "Epoch [23/40]\n",
      "Loss: 0.0149\n",
      "Accuracy: 77.27%\n",
      "------------------------------\n",
      "Epoch [24/40], Batch [10/94], Loss: 0.5012\n",
      "Epoch [24/40], Batch [20/94], Loss: 0.4717\n",
      "Epoch [24/40], Batch [30/94], Loss: 0.4469\n",
      "Epoch [24/40], Batch [40/94], Loss: 0.5807\n",
      "Epoch [24/40], Batch [50/94], Loss: 0.4201\n",
      "Epoch [24/40], Batch [60/94], Loss: 0.5449\n",
      "Epoch [24/40], Batch [70/94], Loss: 0.4242\n",
      "Epoch [24/40], Batch [80/94], Loss: 0.3490\n",
      "Epoch [24/40], Batch [90/94], Loss: 0.4302\n",
      "Epoch [24/40]\n",
      "Loss: 0.0148\n",
      "Accuracy: 77.27%\n",
      "------------------------------\n",
      "Epoch [25/40], Batch [10/94], Loss: 0.4790\n",
      "Epoch [25/40], Batch [20/94], Loss: 0.8194\n",
      "Epoch [25/40], Batch [30/94], Loss: 0.3451\n",
      "Epoch [25/40], Batch [40/94], Loss: 0.4658\n",
      "Epoch [25/40], Batch [50/94], Loss: 0.5377\n",
      "Epoch [25/40], Batch [60/94], Loss: 0.1950\n",
      "Epoch [25/40], Batch [70/94], Loss: 0.3619\n",
      "Epoch [25/40], Batch [80/94], Loss: 0.5447\n",
      "Epoch [25/40], Batch [90/94], Loss: 0.3801\n",
      "Epoch [25/40]\n",
      "Loss: 0.0141\n",
      "Accuracy: 79.23%\n",
      "------------------------------\n",
      "Epoch [26/40], Batch [10/94], Loss: 0.3492\n",
      "Epoch [26/40], Batch [20/94], Loss: 0.4780\n",
      "Epoch [26/40], Batch [30/94], Loss: 0.5490\n",
      "Epoch [26/40], Batch [40/94], Loss: 0.5181\n",
      "Epoch [26/40], Batch [50/94], Loss: 0.4587\n",
      "Epoch [26/40], Batch [60/94], Loss: 0.3523\n",
      "Epoch [26/40], Batch [70/94], Loss: 0.4364\n",
      "Epoch [26/40], Batch [80/94], Loss: 0.4720\n",
      "Epoch [26/40], Batch [90/94], Loss: 0.5268\n",
      "Epoch [26/40]\n",
      "Loss: 0.0142\n",
      "Accuracy: 78.70%\n",
      "------------------------------\n",
      "Epoch [27/40], Batch [10/94], Loss: 0.3135\n",
      "Epoch [27/40], Batch [20/94], Loss: 0.4146\n",
      "Epoch [27/40], Batch [30/94], Loss: 0.3773\n",
      "Epoch [27/40], Batch [40/94], Loss: 0.3242\n",
      "Epoch [27/40], Batch [50/94], Loss: 0.6030\n",
      "Epoch [27/40], Batch [60/94], Loss: 0.3901\n",
      "Epoch [27/40], Batch [70/94], Loss: 0.2193\n",
      "Epoch [27/40], Batch [80/94], Loss: 0.3306\n",
      "Epoch [27/40], Batch [90/94], Loss: 0.4378\n",
      "Epoch [27/40]\n",
      "Loss: 0.0136\n",
      "Accuracy: 79.93%\n",
      "------------------------------\n",
      "Epoch [28/40], Batch [10/94], Loss: 0.2274\n",
      "Epoch [28/40], Batch [20/94], Loss: 0.3344\n",
      "Epoch [28/40], Batch [30/94], Loss: 0.5385\n",
      "Epoch [28/40], Batch [40/94], Loss: 0.4091\n",
      "Epoch [28/40], Batch [50/94], Loss: 0.3632\n",
      "Epoch [28/40], Batch [60/94], Loss: 0.5265\n",
      "Epoch [28/40], Batch [70/94], Loss: 0.3587\n",
      "Epoch [28/40], Batch [80/94], Loss: 0.3398\n",
      "Epoch [28/40], Batch [90/94], Loss: 0.4948\n",
      "Epoch [28/40]\n",
      "Loss: 0.0134\n",
      "Accuracy: 80.53%\n",
      "------------------------------\n",
      "Epoch [29/40], Batch [10/94], Loss: 0.5079\n",
      "Epoch [29/40], Batch [20/94], Loss: 0.4845\n",
      "Epoch [29/40], Batch [30/94], Loss: 0.2330\n",
      "Epoch [29/40], Batch [40/94], Loss: 0.3872\n",
      "Epoch [29/40], Batch [50/94], Loss: 0.4292\n",
      "Epoch [29/40], Batch [60/94], Loss: 0.6290\n",
      "Epoch [29/40], Batch [70/94], Loss: 0.4944\n",
      "Epoch [29/40], Batch [80/94], Loss: 0.3140\n",
      "Epoch [29/40], Batch [90/94], Loss: 0.3315\n",
      "Epoch [29/40]\n",
      "Loss: 0.0129\n",
      "Accuracy: 81.00%\n",
      "------------------------------\n",
      "Epoch [30/40], Batch [10/94], Loss: 0.4122\n",
      "Epoch [30/40], Batch [20/94], Loss: 0.3067\n",
      "Epoch [30/40], Batch [30/94], Loss: 0.3414\n",
      "Epoch [30/40], Batch [40/94], Loss: 0.4585\n",
      "Epoch [30/40], Batch [50/94], Loss: 0.2783\n",
      "Epoch [30/40], Batch [60/94], Loss: 0.4353\n",
      "Epoch [30/40], Batch [70/94], Loss: 0.3171\n",
      "Epoch [30/40], Batch [80/94], Loss: 0.5572\n",
      "Epoch [30/40], Batch [90/94], Loss: 0.4984\n",
      "Epoch [30/40]\n",
      "Loss: 0.0121\n",
      "Accuracy: 82.47%\n",
      "------------------------------\n",
      "Epoch [31/40], Batch [10/94], Loss: 0.2667\n",
      "Epoch [31/40], Batch [20/94], Loss: 0.2570\n",
      "Epoch [31/40], Batch [30/94], Loss: 0.2613\n",
      "Epoch [31/40], Batch [40/94], Loss: 0.6473\n",
      "Epoch [31/40], Batch [50/94], Loss: 0.3524\n",
      "Epoch [31/40], Batch [60/94], Loss: 0.3493\n",
      "Epoch [31/40], Batch [70/94], Loss: 0.3362\n",
      "Epoch [31/40], Batch [80/94], Loss: 0.4102\n",
      "Epoch [31/40], Batch [90/94], Loss: 0.3988\n",
      "Epoch [31/40]\n",
      "Loss: 0.0122\n",
      "Accuracy: 81.83%\n",
      "------------------------------\n",
      "Epoch [32/40], Batch [10/94], Loss: 0.3266\n",
      "Epoch [32/40], Batch [20/94], Loss: 0.4562\n",
      "Epoch [32/40], Batch [30/94], Loss: 0.2885\n",
      "Epoch [32/40], Batch [40/94], Loss: 0.3237\n",
      "Epoch [32/40], Batch [50/94], Loss: 0.2907\n",
      "Epoch [32/40], Batch [60/94], Loss: 0.4230\n",
      "Epoch [32/40], Batch [70/94], Loss: 0.2726\n",
      "Epoch [32/40], Batch [80/94], Loss: 0.5022\n",
      "Epoch [32/40], Batch [90/94], Loss: 0.2718\n",
      "Epoch [32/40]\n",
      "Loss: 0.0120\n",
      "Accuracy: 82.50%\n",
      "------------------------------\n",
      "Epoch [33/40], Batch [10/94], Loss: 0.4350\n",
      "Epoch [33/40], Batch [20/94], Loss: 0.3251\n",
      "Epoch [33/40], Batch [30/94], Loss: 0.2354\n",
      "Epoch [33/40], Batch [40/94], Loss: 0.3278\n",
      "Epoch [33/40], Batch [50/94], Loss: 0.1518\n",
      "Epoch [33/40], Batch [60/94], Loss: 0.4360\n",
      "Epoch [33/40], Batch [70/94], Loss: 0.3214\n",
      "Epoch [33/40], Batch [80/94], Loss: 0.4124\n",
      "Epoch [33/40], Batch [90/94], Loss: 0.3603\n",
      "Epoch [33/40]\n",
      "Loss: 0.0113\n",
      "Accuracy: 83.93%\n",
      "------------------------------\n",
      "Epoch [34/40], Batch [10/94], Loss: 0.4731\n",
      "Epoch [34/40], Batch [20/94], Loss: 0.3285\n",
      "Epoch [34/40], Batch [30/94], Loss: 0.3292\n",
      "Epoch [34/40], Batch [40/94], Loss: 0.1524\n",
      "Epoch [34/40], Batch [50/94], Loss: 0.3033\n",
      "Epoch [34/40], Batch [60/94], Loss: 0.3280\n",
      "Epoch [34/40], Batch [70/94], Loss: 0.4813\n",
      "Epoch [34/40], Batch [80/94], Loss: 0.5510\n",
      "Epoch [34/40], Batch [90/94], Loss: 0.3034\n",
      "Epoch [34/40]\n",
      "Loss: 0.0107\n",
      "Accuracy: 84.47%\n",
      "------------------------------\n",
      "Epoch [35/40], Batch [10/94], Loss: 0.3242\n",
      "Epoch [35/40], Batch [20/94], Loss: 0.3461\n",
      "Epoch [35/40], Batch [30/94], Loss: 0.1957\n",
      "Epoch [35/40], Batch [40/94], Loss: 0.2751\n",
      "Epoch [35/40], Batch [50/94], Loss: 0.4003\n",
      "Epoch [35/40], Batch [60/94], Loss: 0.4534\n",
      "Epoch [35/40], Batch [70/94], Loss: 0.5287\n",
      "Epoch [35/40], Batch [80/94], Loss: 0.5275\n",
      "Epoch [35/40], Batch [90/94], Loss: 0.3369\n",
      "Epoch [35/40]\n",
      "Loss: 0.0107\n",
      "Accuracy: 85.33%\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train() \n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    epoch_start_time = time.time()\n",
    "    for batch_idx, (images, targets) in enumerate(train_loader):\n",
    "        # Forward pass\n",
    "        predictions = model(images)\n",
    "        loss = loss_func(predictions, targets)\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        _, predicted = torch.max(predictions.data, 1)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()  # clear gradients\n",
    "        loss.backward()        # compute gradients\n",
    "        optimizer.step()       # update parameters\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if (batch_idx + 1) % 10 == 0:\n",
    "            print(f\"Epoch [{epoch + 1}/{num_epochs}], \"\n",
    "                    f\"Batch [{batch_idx + 1}/{len(train_loader)}], \"\n",
    "                    f\"Loss: {loss.item():.4f}\")\n",
    "            \n",
    "    # print average loss for the epoch\n",
    "    avg_loss = total_loss / len(dataset)\n",
    "    accuracy = 100 * correct / len(dataset)\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}]\")\n",
    "    print(f\"Loss: {avg_loss:.4f}\")\n",
    "    print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "    print(\"-\" * 30)\n",
    "    if 85 < accuracy:\n",
    "        break\n",
    "    \n",
    "\n",
    "    if device.type == 'cuda':\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy: 85.33%\n"
     ]
    }
   ],
   "source": [
    "times = datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "torch.save({\n",
    "'model_state_dict': model.state_dict(),\n",
    "'optimizer_state_dict': optimizer.state_dict(),\n",
    "'final_accuracy': accuracy,\n",
    "}, f'CNN_model_1_final_{times}.pth')\n",
    "\n",
    "print(f\"Best accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
