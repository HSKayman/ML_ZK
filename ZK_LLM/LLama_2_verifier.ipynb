{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78b02841",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e10562b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "import os\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8863042",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "MODEL_1_PATH = \"meta-llama/Llama-2-7b-chat-hf\" \n",
    "MODEL_2_PATH = \"meta-llama/Llama-2-7b-hf\"       \n",
    "\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3911279d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d47dfd2e32824b019e31d542be70ba65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88625522a5644c7f84522bb0dc48df53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 10\u001b[0m\n\u001b[0;32m      2\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mpad_token \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39meos_token\n\u001b[0;32m      4\u001b[0m model_1 \u001b[38;5;241m=\u001b[39m LlamaForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m      5\u001b[0m     MODEL_1_PATH,\n\u001b[0;32m      6\u001b[0m     torch_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, \n\u001b[0;32m      7\u001b[0m     device_map\u001b[38;5;241m=\u001b[39mDEVICE\n\u001b[0;32m      8\u001b[0m )\n\u001b[1;32m---> 10\u001b[0m model_2 \u001b[38;5;241m=\u001b[39m LlamaForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m     11\u001b[0m     MODEL_2_PATH,\n\u001b[0;32m     12\u001b[0m     torch_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, \n\u001b[0;32m     13\u001b[0m     device_map\u001b[38;5;241m=\u001b[39mDEVICE\n\u001b[0;32m     14\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\hskay\\anaconda3\\Lib\\site-packages\\transformers\\modeling_utils.py:316\u001b[0m, in \u001b[0;36mrestore_default_torch_dtype.<locals>._wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    314\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    318\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[1;32mc:\\Users\\hskay\\anaconda3\\Lib\\site-packages\\transformers\\modeling_utils.py:5061\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   5051\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   5052\u001b[0m         torch\u001b[38;5;241m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[0;32m   5054\u001b[0m     (\n\u001b[0;32m   5055\u001b[0m         model,\n\u001b[0;32m   5056\u001b[0m         missing_keys,\n\u001b[0;32m   5057\u001b[0m         unexpected_keys,\n\u001b[0;32m   5058\u001b[0m         mismatched_keys,\n\u001b[0;32m   5059\u001b[0m         offload_index,\n\u001b[0;32m   5060\u001b[0m         error_msgs,\n\u001b[1;32m-> 5061\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_load_pretrained_model(\n\u001b[0;32m   5062\u001b[0m         model,\n\u001b[0;32m   5063\u001b[0m         state_dict,\n\u001b[0;32m   5064\u001b[0m         checkpoint_files,\n\u001b[0;32m   5065\u001b[0m         pretrained_model_name_or_path,\n\u001b[0;32m   5066\u001b[0m         ignore_mismatched_sizes\u001b[38;5;241m=\u001b[39mignore_mismatched_sizes,\n\u001b[0;32m   5067\u001b[0m         sharded_metadata\u001b[38;5;241m=\u001b[39msharded_metadata,\n\u001b[0;32m   5068\u001b[0m         device_map\u001b[38;5;241m=\u001b[39mdevice_map,\n\u001b[0;32m   5069\u001b[0m         disk_offload_folder\u001b[38;5;241m=\u001b[39moffload_folder,\n\u001b[0;32m   5070\u001b[0m         offload_state_dict\u001b[38;5;241m=\u001b[39moffload_state_dict,\n\u001b[0;32m   5071\u001b[0m         dtype\u001b[38;5;241m=\u001b[39mtorch_dtype,\n\u001b[0;32m   5072\u001b[0m         hf_quantizer\u001b[38;5;241m=\u001b[39mhf_quantizer,\n\u001b[0;32m   5073\u001b[0m         keep_in_fp32_regex\u001b[38;5;241m=\u001b[39mkeep_in_fp32_regex,\n\u001b[0;32m   5074\u001b[0m         device_mesh\u001b[38;5;241m=\u001b[39mdevice_mesh,\n\u001b[0;32m   5075\u001b[0m         key_mapping\u001b[38;5;241m=\u001b[39mkey_mapping,\n\u001b[0;32m   5076\u001b[0m         weights_only\u001b[38;5;241m=\u001b[39mweights_only,\n\u001b[0;32m   5077\u001b[0m     )\n\u001b[0;32m   5078\u001b[0m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[0;32m   5079\u001b[0m model\u001b[38;5;241m.\u001b[39mtie_weights()\n",
      "File \u001b[1;32mc:\\Users\\hskay\\anaconda3\\Lib\\site-packages\\transformers\\modeling_utils.py:5524\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[1;34m(cls, model, state_dict, checkpoint_files, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, device_map, disk_offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_regex, device_mesh, key_mapping, weights_only)\u001b[0m\n\u001b[0;32m   5521\u001b[0m         args_list \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mtqdm(args_list, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading checkpoint shards\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   5523\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m args \u001b[38;5;129;01min\u001b[39;00m args_list:\n\u001b[1;32m-> 5524\u001b[0m         _error_msgs, disk_offload_index, cpu_offload_index \u001b[38;5;241m=\u001b[39m load_shard_file(args)\n\u001b[0;32m   5525\u001b[0m         error_msgs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m _error_msgs\n\u001b[0;32m   5527\u001b[0m \u001b[38;5;66;03m# Adjust offloaded weights name and save if needed\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hskay\\anaconda3\\Lib\\site-packages\\transformers\\modeling_utils.py:974\u001b[0m, in \u001b[0;36mload_shard_file\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m    972\u001b[0m \u001b[38;5;66;03m# Skip it with fsdp on ranks other than 0\u001b[39;00m\n\u001b[0;32m    973\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (is_fsdp_enabled() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_local_dist_rank_0() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_quantized):\n\u001b[1;32m--> 974\u001b[0m     disk_offload_index, cpu_offload_index \u001b[38;5;241m=\u001b[39m _load_state_dict_into_meta_model(\n\u001b[0;32m    975\u001b[0m         model_to_load,\n\u001b[0;32m    976\u001b[0m         state_dict,\n\u001b[0;32m    977\u001b[0m         shard_file,\n\u001b[0;32m    978\u001b[0m         expected_keys,\n\u001b[0;32m    979\u001b[0m         reverse_key_renaming_mapping,\n\u001b[0;32m    980\u001b[0m         device_map\u001b[38;5;241m=\u001b[39mdevice_map,\n\u001b[0;32m    981\u001b[0m         disk_offload_folder\u001b[38;5;241m=\u001b[39mdisk_offload_folder,\n\u001b[0;32m    982\u001b[0m         disk_offload_index\u001b[38;5;241m=\u001b[39mdisk_offload_index,\n\u001b[0;32m    983\u001b[0m         cpu_offload_folder\u001b[38;5;241m=\u001b[39mcpu_offload_folder,\n\u001b[0;32m    984\u001b[0m         cpu_offload_index\u001b[38;5;241m=\u001b[39mcpu_offload_index,\n\u001b[0;32m    985\u001b[0m         hf_quantizer\u001b[38;5;241m=\u001b[39mhf_quantizer,\n\u001b[0;32m    986\u001b[0m         is_safetensors\u001b[38;5;241m=\u001b[39mis_offloaded_safetensors,\n\u001b[0;32m    987\u001b[0m         keep_in_fp32_regex\u001b[38;5;241m=\u001b[39mkeep_in_fp32_regex,\n\u001b[0;32m    988\u001b[0m         unexpected_keys\u001b[38;5;241m=\u001b[39munexpected_keys,\n\u001b[0;32m    989\u001b[0m         device_mesh\u001b[38;5;241m=\u001b[39mdevice_mesh,\n\u001b[0;32m    990\u001b[0m     )\n\u001b[0;32m    992\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m error_msgs, disk_offload_index, cpu_offload_index\n",
      "File \u001b[1;32mc:\\Users\\hskay\\anaconda3\\Lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m    116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\hskay\\anaconda3\\Lib\\site-packages\\torch\\autograd\\grad_mode.py:84\u001b[0m, in \u001b[0;36mno_grad.__exit__\u001b[1;34m(self, exc_type, exc_value, traceback)\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprev \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mis_grad_enabled()\n\u001b[0;32m     82\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m---> 84\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, exc_type: Any, exc_value: Any, traceback: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     85\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprev)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tokenizer = LlamaTokenizer.from_pretrained(MODEL_1_PATH)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model_1 = LlamaForCausalLM.from_pretrained(\n",
    "    MODEL_1_PATH,\n",
    "    torch_dtype=torch.float32, \n",
    "    device_map=DEVICE\n",
    ")\n",
    "\n",
    "model_2 = LlamaForCausalLM.from_pretrained(\n",
    "    MODEL_2_PATH,\n",
    "    torch_dtype=torch.float32, \n",
    "    device_map=DEVICE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bb5afc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ea8373",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = model_1.to(DEVICE)\n",
    "model_2 = model_2.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdd67eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_weights(model, move_to_cpu=False,layer_range=None):\n",
    "    weights = {}\n",
    "    \n",
    "    # Embedding weights\n",
    "    embed_weight = model.model.embed_tokens.weight\n",
    "    weights['embed_tokens'] = embed_weight.detach().cpu() if move_to_cpu else embed_weight.detach()\n",
    "    \n",
    "    # Layer-specific weights\n",
    "    if layer_range is None:\n",
    "        layer_range = range(len(model.model.layers))\n",
    "    \n",
    "    for i in layer_range:\n",
    "        if i >= len(model.model.layers):\n",
    "            continue\n",
    "            \n",
    "        layer = model.model.layers[i]\n",
    "        layer_prefix = f\"layer_{i}\"\n",
    "        \n",
    "        # Self-attention weights\n",
    "        weights[f\"{layer_prefix}_q_proj\"] = layer.self_attn.q_proj.weight.detach().cpu() if move_to_cpu else layer.self_attn.q_proj.weight.detach()\n",
    "        weights[f\"{layer_prefix}_k_proj\"] = layer.self_attn.k_proj.weight.detach().cpu() if move_to_cpu else layer.self_attn.k_proj.weight.detach()\n",
    "        weights[f\"{layer_prefix}_v_proj\"] = layer.self_attn.v_proj.weight.detach().cpu() if move_to_cpu else layer.self_attn.v_proj.weight.detach()\n",
    "        weights[f\"{layer_prefix}_o_proj\"] = layer.self_attn.o_proj.weight.detach().cpu() if move_to_cpu else layer.self_attn.o_proj.weight.detach()\n",
    "        \n",
    "        # MLP weights\n",
    "        weights[f\"{layer_prefix}_gate_proj\"] = layer.mlp.gate_proj.weight.detach().cpu() if move_to_cpu else layer.mlp.gate_proj.weight.detach()\n",
    "        weights[f\"{layer_prefix}_up_proj\"] = layer.mlp.up_proj.weight.detach().cpu() if move_to_cpu else layer.mlp.up_proj.weight.detach()\n",
    "        weights[f\"{layer_prefix}_down_proj\"] = layer.mlp.down_proj.weight.detach().cpu() if move_to_cpu else layer.mlp.down_proj.weight.detach()\n",
    "        \n",
    "        # Layer norm weights\n",
    "        weights[f\"{layer_prefix}_input_layernorm\"] = layer.input_layernorm.weight.detach().cpu() if move_to_cpu else layer.input_layernorm.weight.detach()\n",
    "        weights[f\"{layer_prefix}_post_attention_layernorm\"] = layer.post_attention_layernorm.weight.detach().cpu() if move_to_cpu else layer.post_attention_layernorm.weight.detach()\n",
    "    \n",
    "    # Final layer norm and LM head\n",
    "    weights['final_norm'] = model.model.norm.weight.detach().cpu() if move_to_cpu else model.model.norm.weight.detach()\n",
    "    weights['lm_head'] = model.lm_head.weight.detach().cpu() if move_to_cpu else model.lm_head.weight.detach()\n",
    "    \n",
    "    return weights\n",
    "\n",
    "def calculate_weight_differences(weights_1, weights_2):\n",
    "    differences = {}\n",
    "    \n",
    "    common_keys = set(weights_1.keys()) & set(weights_2.keys())\n",
    "    print(f\"Comparing {len(common_keys)} weight matrices...\")\n",
    "    \n",
    "    for i, key in enumerate(common_keys):\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Processing {i+1}/{len(common_keys)}: {key}\")\n",
    "            \n",
    "        w1 = weights_1[key]\n",
    "        w2 = weights_2[key]\n",
    "        \n",
    "        if w1.shape != w2.shape:\n",
    "            print(f\"Warning: Shape mismatch for {key}: {w1.shape} vs {w2.shape}\")\n",
    "            continue\n",
    "        \n",
    "        # Calculate difference matrix\n",
    "        diff_matrix = w1 - w2\n",
    "        \n",
    "        # Calculate various norms and statistics\n",
    "        frobenius_norm = torch.norm(diff_matrix, p='fro').item()\n",
    "        frobenius_norm_relative = frobenius_norm / (torch.norm(w1, p='fro').item() + 1e-10)\n",
    "        \n",
    "        spectral_norm = torch.norm(diff_matrix, p=2).item()\n",
    "        spectral_norm_relative = spectral_norm / (torch.norm(w1, p=2).item() + 1e-10)\n",
    "        \n",
    "        # Element-wise statistics\n",
    "        abs_diff = torch.abs(diff_matrix)\n",
    "        mean_abs_diff = torch.mean(abs_diff).item()\n",
    "        max_abs_diff = torch.max(abs_diff).item()\n",
    "        std_diff = torch.std(diff_matrix).item()\n",
    "        \n",
    "        # Percentage of significantly different weights (threshold = 1e-6)\n",
    "        significant_diff_ratio = (abs_diff > 1e-6).float().mean().item()\n",
    "        \n",
    "        # Cosine similarity\n",
    "        w1_flat = w1.flatten()\n",
    "        w2_flat = w2.flatten()\n",
    "        cosine_sim = F.cosine_similarity(w1_flat.unsqueeze(0), w2_flat.unsqueeze(0)).item()\n",
    "        \n",
    "        differences[key] = {\n",
    "            'frobenius_norm': frobenius_norm,\n",
    "            'frobenius_norm_relative': frobenius_norm_relative,\n",
    "            'spectral_norm': spectral_norm,\n",
    "            'spectral_norm_relative': spectral_norm_relative,\n",
    "            'mean_abs_difference': mean_abs_diff,\n",
    "            'max_abs_difference': max_abs_diff,\n",
    "            'std_difference': std_diff,\n",
    "            'significant_diff_ratio': significant_diff_ratio,\n",
    "            'cosine_similarity': cosine_sim,\n",
    "            'weight_shape': w1.shape,\n",
    "            'total_parameters': w1.numel()\n",
    "        }\n",
    "    \n",
    "    return differences\n",
    "\n",
    "def analyze_weight_patterns(weight_differences):\n",
    "    analysis = {\n",
    "        'by_component_type': defaultdict(list),\n",
    "        'by_layer_depth': defaultdict(list),\n",
    "        'summary_stats': {}\n",
    "    }\n",
    "    \n",
    "    # Group by component type\n",
    "    for layer_name, diff_data in weight_differences.items():\n",
    "        if any(x in layer_name for x in ['q_proj', 'k_proj', 'v_proj', 'o_proj']):\n",
    "            component_type = 'attention'\n",
    "        elif any(x in layer_name for x in ['gate_proj', 'up_proj', 'down_proj']):\n",
    "            component_type = 'mlp'\n",
    "        elif 'layernorm' in layer_name or 'norm' in layer_name:\n",
    "            component_type = 'normalization'\n",
    "        elif 'embed' in layer_name:\n",
    "            component_type = 'embedding'\n",
    "        elif 'lm_head' in layer_name:\n",
    "            component_type = 'output'\n",
    "        else:\n",
    "            component_type = 'other'\n",
    "        \n",
    "        analysis['by_component_type'][component_type].append({\n",
    "            'layer_name': layer_name,\n",
    "            'frobenius_norm': diff_data['frobenius_norm'],\n",
    "            'frobenius_norm_relative': diff_data['frobenius_norm_relative'],\n",
    "            'significant_diff_ratio': diff_data['significant_diff_ratio'],\n",
    "            'cosine_similarity': diff_data['cosine_similarity']\n",
    "        })\n",
    "    \n",
    "    # Group by layer depth\n",
    "    for layer_name, diff_data in weight_differences.items():\n",
    "        if 'layer_' in layer_name:\n",
    "            try:\n",
    "                layer_num = int(layer_name.split('_')[1])\n",
    "                analysis['by_layer_depth'][layer_num].append({\n",
    "                    'layer_name': layer_name,\n",
    "                    'frobenius_norm': diff_data['frobenius_norm'],\n",
    "                    'frobenius_norm_relative': diff_data['frobenius_norm_relative'],\n",
    "                    'cosine_similarity': diff_data['cosine_similarity']\n",
    "                })\n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "    # Calculate summary statistics\n",
    "    all_frobenius = [data['frobenius_norm'] for data in weight_differences.values()]\n",
    "    all_frobenius_rel = [data['frobenius_norm_relative'] for data in weight_differences.values()]\n",
    "    all_significant_ratios = [data['significant_diff_ratio'] for data in weight_differences.values()]\n",
    "    all_cosine_sims = [data['cosine_similarity'] for data in weight_differences.values()]\n",
    "    \n",
    "    analysis['summary_stats'] = {\n",
    "        'total_layers_compared': len(weight_differences),\n",
    "        'mean_frobenius_norm': np.mean(all_frobenius),\n",
    "        'std_frobenius_norm': np.std(all_frobenius),\n",
    "        'max_frobenius_norm': np.max(all_frobenius),\n",
    "        'min_frobenius_norm': np.min(all_frobenius),\n",
    "        'mean_frobenius_norm_relative': np.mean(all_frobenius_rel),\n",
    "        'mean_significant_diff_ratio': np.mean(all_significant_ratios),\n",
    "        'mean_cosine_similarity': np.mean(all_cosine_sims),\n",
    "        'min_cosine_similarity': np.min(all_cosine_sims),\n",
    "        'total_parameters_compared': sum(data['total_parameters'] for data in weight_differences.values())\n",
    "    }\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "def print_weight_analysis_summary(analysis):\n",
    "    print(\"=\"*70)\n",
    "    print(\"LLAMA MODEL WEIGHT DIFFERENCE ANALYSIS SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Overall statistics\n",
    "    stats = analysis['summary_stats']\n",
    "    print(f\"\\n📊 OVERALL STATISTICS:\")\n",
    "    print(f\"  • Total layers compared: {stats['total_layers_compared']}\")\n",
    "    print(f\"  • Total parameters compared: {stats['total_parameters_compared']:,}\")\n",
    "    print(f\"  • Mean Frobenius norm: {stats['mean_frobenius_norm']:.2e}\")\n",
    "    print(f\"  • Mean relative Frobenius norm: {stats['mean_frobenius_norm_relative']:.8f}\")\n",
    "    print(f\"  • Max Frobenius norm: {stats['max_frobenius_norm']:.2e}\")\n",
    "    print(f\"  • Min Frobenius norm: {stats['min_frobenius_norm']:.2e}\")\n",
    "    print(f\"  • Mean cosine similarity: {stats['mean_cosine_similarity']:.8f}\")\n",
    "    print(f\"  • Min cosine similarity: {stats['min_cosine_similarity']:.8f}\")\n",
    "    print(f\"  • Mean significant difference ratio: {stats['mean_significant_diff_ratio']:.4f}\")\n",
    "    \n",
    "    # Component type analysis\n",
    "    print(f\"\\n🔧 BY COMPONENT TYPE:\")\n",
    "    for comp_type, comp_data in analysis['by_component_type'].items():\n",
    "        frob_norms = [item['frobenius_norm_relative'] for item in comp_data]\n",
    "        cosine_sims = [item['cosine_similarity'] for item in comp_data]\n",
    "        sig_ratios = [item['significant_diff_ratio'] for item in comp_data]\n",
    "        \n",
    "        print(f\"  {comp_type.upper()}:\")\n",
    "        print(f\"    - Count: {len(comp_data)} layers\")\n",
    "        print(f\"    - Mean relative Frobenius: {np.mean(frob_norms):.8f} ± {np.std(frob_norms):.8f}\")\n",
    "        print(f\"    - Mean cosine similarity: {np.mean(cosine_sims):.8f} ± {np.std(cosine_sims):.8f}\")\n",
    "        print(f\"    - Mean sig. diff ratio: {np.mean(sig_ratios):.4f}\")\n",
    "    \n",
    "    # Layer depth analysis (if available)\n",
    "    if analysis['by_layer_depth']:\n",
    "        print(f\"\\n📈 BY LAYER DEPTH:\")\n",
    "        for depth in sorted(analysis['by_layer_depth'].keys())[:10]:  # Show first 10 layers\n",
    "            depth_data = analysis['by_layer_depth'][depth]\n",
    "            frob_norms = [item['frobenius_norm_relative'] for item in depth_data]\n",
    "            cosine_sims = [item['cosine_similarity'] for item in depth_data]\n",
    "            \n",
    "            print(f\"  Layer {depth}: Frob={np.mean(frob_norms):.6f}, Cosine={np.mean(cosine_sims):.6f}\")\n",
    "    \n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec280794",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_1 = get_model_weights(model_1)\n",
    "weights_2 = get_model_weights(model_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566bf7c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing 291 weight matrices...\n",
      "Processing 1/291: layer_12_up_proj\n",
      "Processing 11/291: layer_30_v_proj\n",
      "Processing 21/291: layer_18_gate_proj\n",
      "Processing 31/291: layer_1_down_proj\n",
      "Processing 41/291: layer_13_down_proj\n",
      "Processing 51/291: layer_0_k_proj\n",
      "Processing 61/291: layer_8_v_proj\n",
      "Processing 71/291: layer_26_down_proj\n",
      "Processing 81/291: layer_30_o_proj\n",
      "Processing 91/291: layer_15_post_attention_layernorm\n",
      "Processing 101/291: layer_26_gate_proj\n",
      "Processing 111/291: layer_27_input_layernorm\n",
      "Processing 121/291: layer_5_down_proj\n",
      "Processing 131/291: layer_18_down_proj\n",
      "Processing 141/291: layer_29_post_attention_layernorm\n",
      "Processing 151/291: layer_12_o_proj\n",
      "Processing 161/291: layer_9_down_proj\n",
      "Processing 171/291: layer_13_o_proj\n",
      "Processing 181/291: layer_3_o_proj\n",
      "Processing 191/291: layer_1_v_proj\n",
      "Processing 201/291: layer_4_v_proj\n",
      "Processing 211/291: layer_2_input_layernorm\n",
      "Processing 221/291: layer_25_o_proj\n",
      "Processing 231/291: layer_6_up_proj\n",
      "Processing 241/291: layer_10_up_proj\n",
      "Processing 251/291: layer_21_post_attention_layernorm\n",
      "Processing 261/291: layer_16_q_proj\n",
      "Processing 271/291: layer_12_gate_proj\n",
      "Processing 281/291: layer_23_q_proj\n",
      "Processing 291/291: layer_9_v_proj\n"
     ]
    }
   ],
   "source": [
    "weight_differences = calculate_weight_differences(weights_1, weights_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff68a2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis = analyze_weight_patterns(weight_differences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f8f2a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "LLAMA MODEL WEIGHT DIFFERENCE ANALYSIS SUMMARY\n",
      "======================================================================\n",
      "\n",
      "📊 OVERALL STATISTICS:\n",
      "  • Total layers compared: 291\n",
      "  • Total parameters compared: 6,738,415,616\n",
      "  • Mean Frobenius norm: 5.00e+00\n",
      "  • Mean relative Frobenius norm: 0.05240598\n",
      "  • Max Frobenius norm: 2.33e+01\n",
      "  • Min Frobenius norm: 6.37e-02\n",
      "  • Mean cosine similarity: 1.00115807\n",
      "  • Min cosine similarity: 0.99230218\n",
      "  • Mean significant difference ratio: 0.9642\n",
      "\n",
      "🔧 BY COMPONENT TYPE:\n",
      "  MLP:\n",
      "    - Count: 96 layers\n",
      "    - Mean relative Frobenius: 0.06610288 ± 0.00257659\n",
      "    - Mean cosine similarity: 1.00398319 ± 0.00041742\n",
      "    - Mean sig. diff ratio: 0.9729\n",
      "  ATTENTION:\n",
      "    - Count: 128 layers\n",
      "    - Mean relative Frobenius: 0.06287519 ± 0.01413015\n",
      "    - Mean cosine similarity: 0.99922307 ± 0.00117901\n",
      "    - Mean sig. diff ratio: 0.9713\n",
      "  NORMALIZATION:\n",
      "    - Count: 65 layers\n",
      "    - Mean relative Frobenius: 0.01038052 ± 0.00265356\n",
      "    - Mean cosine similarity: 0.99998514 ± 0.00004352\n",
      "    - Mean sig. diff ratio: 0.9371\n",
      "  EMBEDDING:\n",
      "    - Count: 1 layers\n",
      "    - Mean relative Frobenius: 0.05791559 ± 0.00000000\n",
      "    - Mean cosine similarity: 1.02927232 ± 0.00000000\n",
      "    - Mean sig. diff ratio: 0.9617\n",
      "  OUTPUT:\n",
      "    - Count: 1 layers\n",
      "    - Mean relative Frobenius: 0.12358926 ± 0.00000000\n",
      "    - Mean cosine similarity: 1.02575266 ± 0.00000000\n",
      "    - Mean sig. diff ratio: 0.9832\n",
      "\n",
      "📈 BY LAYER DEPTH:\n",
      "  Layer 0: Frob=0.061750, Cosine=1.000780\n",
      "  Layer 1: Frob=0.057145, Cosine=1.000603\n",
      "  Layer 2: Frob=0.048795, Cosine=1.001271\n",
      "  Layer 3: Frob=0.051349, Cosine=1.001073\n",
      "  Layer 4: Frob=0.050610, Cosine=1.001082\n",
      "  Layer 5: Frob=0.049889, Cosine=1.001132\n",
      "  Layer 6: Frob=0.052620, Cosine=1.001073\n",
      "  Layer 7: Frob=0.052589, Cosine=1.001031\n",
      "  Layer 8: Frob=0.052245, Cosine=1.001056\n",
      "  Layer 9: Frob=0.051850, Cosine=1.001013\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print_weight_analysis_summary(analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c01ef25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'layer_12_up_proj': {'frobenius_norm': 8.063419342041016,\n",
       "  'frobenius_norm_relative': 0.06631053670767212,\n",
       "  'spectral_norm': 8.063419342041016,\n",
       "  'spectral_norm_relative': 0.06631053670767212,\n",
       "  'mean_abs_difference': 0.0009561135084368289,\n",
       "  'max_abs_difference': 0.007568359375,\n",
       "  'std_difference': 0.0012020444264635444,\n",
       "  'significant_diff_ratio': 0.9728776812553406,\n",
       "  'cosine_similarity': 1.00386381149292,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_19_v_proj': {'frobenius_norm': 4.668221473693848,\n",
       "  'frobenius_norm_relative': 0.06699923482386681,\n",
       "  'spectral_norm': 4.668221473693848,\n",
       "  'spectral_norm_relative': 0.06699923482386681,\n",
       "  'mean_abs_difference': 0.0009058689465746284,\n",
       "  'max_abs_difference': 0.0067138671875,\n",
       "  'std_difference': 0.0011400870280340314,\n",
       "  'significant_diff_ratio': 0.9733886122703552,\n",
       "  'cosine_similarity': 0.9988657236099243,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_22_down_proj': {'frobenius_norm': 8.437965393066406,\n",
       "  'frobenius_norm_relative': 0.06938864292926306,\n",
       "  'spectral_norm': 8.437965393066406,\n",
       "  'spectral_norm_relative': 0.06938864292926306,\n",
       "  'mean_abs_difference': 0.0010020476765930653,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0012579604517668486,\n",
       "  'significant_diff_ratio': 0.9741316437721252,\n",
       "  'cosine_similarity': 1.0036357641220093,\n",
       "  'weight_shape': torch.Size([4096, 11008]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_24_k_proj': {'frobenius_norm': 5.324514389038086,\n",
       "  'frobenius_norm_relative': 0.059489671274193294,\n",
       "  'spectral_norm': 5.324514389038086,\n",
       "  'spectral_norm_relative': 0.059489671274193294,\n",
       "  'mean_abs_difference': 0.0010336291743442416,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0013004497159272432,\n",
       "  'significant_diff_ratio': 0.9712492227554321,\n",
       "  'cosine_similarity': 0.9993242025375366,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_12_v_proj': {'frobenius_norm': 4.320816993713379,\n",
       "  'frobenius_norm_relative': 0.07194933687314177,\n",
       "  'spectral_norm': 4.320816993713379,\n",
       "  'spectral_norm_relative': 0.07194933687314177,\n",
       "  'mean_abs_difference': 0.0008379047503694892,\n",
       "  'max_abs_difference': 0.0059814453125,\n",
       "  'std_difference': 0.0010553072206676006,\n",
       "  'significant_diff_ratio': 0.9752076864242554,\n",
       "  'cosine_similarity': 0.9984627366065979,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_19_up_proj': {'frobenius_norm': 8.35304069519043,\n",
       "  'frobenius_norm_relative': 0.06806836381789393,\n",
       "  'spectral_norm': 8.35304069519043,\n",
       "  'spectral_norm_relative': 0.06806836381789393,\n",
       "  'mean_abs_difference': 0.0009911861270666122,\n",
       "  'max_abs_difference': 0.0076904296875,\n",
       "  'std_difference': 0.001245292485691607,\n",
       "  'significant_diff_ratio': 0.9735836386680603,\n",
       "  'cosine_similarity': 1.0036407709121704,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_16_up_proj': {'frobenius_norm': 8.210671424865723,\n",
       "  'frobenius_norm_relative': 0.06646132275667017,\n",
       "  'spectral_norm': 8.210671424865723,\n",
       "  'spectral_norm_relative': 0.06646132275667017,\n",
       "  'mean_abs_difference': 0.0009741014800965786,\n",
       "  'max_abs_difference': 0.00726318359375,\n",
       "  'std_difference': 0.0012241010554134846,\n",
       "  'significant_diff_ratio': 0.9729949831962585,\n",
       "  'cosine_similarity': 1.003686785697937,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_13_input_layernorm': {'frobenius_norm': 0.2945097088813782,\n",
       "  'frobenius_norm_relative': 0.011838565976030533,\n",
       "  'spectral_norm': 0.2945097088813782,\n",
       "  'spectral_norm_relative': 0.011838565976030533,\n",
       "  'mean_abs_difference': 0.004400855395942926,\n",
       "  'max_abs_difference': 0.009765625,\n",
       "  'std_difference': 0.0013448738027364016,\n",
       "  'significant_diff_ratio': 0.99853515625,\n",
       "  'cosine_similarity': 0.9999964833259583,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_16_input_layernorm': {'frobenius_norm': 0.3085192143917084,\n",
       "  'frobenius_norm_relative': 0.012009522646199868,\n",
       "  'spectral_norm': 0.3085192143917084,\n",
       "  'spectral_norm_relative': 0.012009522646199868,\n",
       "  'mean_abs_difference': 0.004629045724868774,\n",
       "  'max_abs_difference': 0.009765625,\n",
       "  'std_difference': 0.0013472562422975898,\n",
       "  'significant_diff_ratio': 0.998046875,\n",
       "  'cosine_similarity': 0.9999971389770508,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_9_input_layernorm': {'frobenius_norm': 0.22860167920589447,\n",
       "  'frobenius_norm_relative': 0.010403072978888222,\n",
       "  'spectral_norm': 0.22860167920589447,\n",
       "  'spectral_norm_relative': 0.010403072978888222,\n",
       "  'mean_abs_difference': 0.003312712535262108,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0013359826989471912,\n",
       "  'significant_diff_ratio': 0.974365234375,\n",
       "  'cosine_similarity': 0.9999927878379822,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_30_v_proj': {'frobenius_norm': 4.893336296081543,\n",
       "  'frobenius_norm_relative': 0.05758839753103985,\n",
       "  'spectral_norm': 4.893336296081543,\n",
       "  'spectral_norm_relative': 0.05758839753103985,\n",
       "  'mean_abs_difference': 0.0009484292240813375,\n",
       "  'max_abs_difference': 0.00701904296875,\n",
       "  'std_difference': 0.001195305958390236,\n",
       "  'significant_diff_ratio': 0.9689257740974426,\n",
       "  'cosine_similarity': 0.9994920492172241,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_31_down_proj': {'frobenius_norm': 7.2006378173828125,\n",
       "  'frobenius_norm_relative': 0.057758148942714964,\n",
       "  'spectral_norm': 7.2006378173828125,\n",
       "  'spectral_norm_relative': 0.057758148942714964,\n",
       "  'mean_abs_difference': 0.0008355370373465121,\n",
       "  'max_abs_difference': 0.015625,\n",
       "  'std_difference': 0.0010742039885371923,\n",
       "  'significant_diff_ratio': 0.96783047914505,\n",
       "  'cosine_similarity': 1.004289984703064,\n",
       "  'weight_shape': torch.Size([4096, 11008]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_4_k_proj': {'frobenius_norm': 4.679013729095459,\n",
       "  'frobenius_norm_relative': 0.04297787407947939,\n",
       "  'spectral_norm': 4.679013729095459,\n",
       "  'spectral_norm_relative': 0.04297787407947939,\n",
       "  'mean_abs_difference': 0.0009050709777511656,\n",
       "  'max_abs_difference': 0.0068359375,\n",
       "  'std_difference': 0.001143536064773798,\n",
       "  'significant_diff_ratio': 0.9615519046783447,\n",
       "  'cosine_similarity': 1.0005054473876953,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_23_input_layernorm': {'frobenius_norm': 0.32350799441337585,\n",
       "  'frobenius_norm_relative': 0.00997589121351512,\n",
       "  'spectral_norm': 0.32350799441337585,\n",
       "  'spectral_norm_relative': 0.00997589121351512,\n",
       "  'mean_abs_difference': 0.004676243755966425,\n",
       "  'max_abs_difference': 0.015625,\n",
       "  'std_difference': 0.001919575734063983,\n",
       "  'significant_diff_ratio': 0.9619140625,\n",
       "  'cosine_similarity': 0.9999938607215881,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_11_k_proj': {'frobenius_norm': 5.303076267242432,\n",
       "  'frobenius_norm_relative': 0.05489474742685855,\n",
       "  'spectral_norm': 5.303076267242432,\n",
       "  'spectral_norm_relative': 0.05489474742685855,\n",
       "  'mean_abs_difference': 0.0010285121388733387,\n",
       "  'max_abs_difference': 0.00732421875,\n",
       "  'std_difference': 0.0012952511897310615,\n",
       "  'significant_diff_ratio': 0.9695286750793457,\n",
       "  'cosine_similarity': 0.9999884366989136,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_1_post_attention_layernorm': {'frobenius_norm': 0.08544612675905228,\n",
       "  'frobenius_norm_relative': 0.013116989526030529,\n",
       "  'spectral_norm': 0.08544612675905228,\n",
       "  'spectral_norm_relative': 0.013116989526030529,\n",
       "  'mean_abs_difference': 0.0010895447339862585,\n",
       "  'max_abs_difference': 0.0048828125,\n",
       "  'std_difference': 0.00096520921215415,\n",
       "  'significant_diff_ratio': 0.8798828125,\n",
       "  'cosine_similarity': 0.9999590516090393,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_5_q_proj': {'frobenius_norm': 4.787556171417236,\n",
       "  'frobenius_norm_relative': 0.04466815304352225,\n",
       "  'spectral_norm': 4.787556171417236,\n",
       "  'spectral_norm_relative': 0.04466815304352225,\n",
       "  'mean_abs_difference': 0.00092760642291978,\n",
       "  'max_abs_difference': 0.0072021484375,\n",
       "  'std_difference': 0.0011700175236910582,\n",
       "  'significant_diff_ratio': 0.9619293808937073,\n",
       "  'cosine_similarity': 1.0003111362457275,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_18_post_attention_layernorm': {'frobenius_norm': 0.1800546646118164,\n",
       "  'frobenius_norm_relative': 0.008440091581621647,\n",
       "  'spectral_norm': 0.1800546646118164,\n",
       "  'spectral_norm_relative': 0.008440091581621647,\n",
       "  'mean_abs_difference': 0.0024721622467041016,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0013647815212607384,\n",
       "  'significant_diff_ratio': 0.886474609375,\n",
       "  'cosine_similarity': 0.9999916553497314,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_2_o_proj': {'frobenius_norm': 4.414910793304443,\n",
       "  'frobenius_norm_relative': 0.07811529437129157,\n",
       "  'spectral_norm': 4.414910793304443,\n",
       "  'spectral_norm_relative': 0.07811529437129157,\n",
       "  'mean_abs_difference': 0.0008579519344493747,\n",
       "  'max_abs_difference': 0.005950927734375,\n",
       "  'std_difference': 0.0010780743323266506,\n",
       "  'significant_diff_ratio': 0.977191150188446,\n",
       "  'cosine_similarity': 0.9981061220169067,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_0_o_proj': {'frobenius_norm': 4.033174514770508,\n",
       "  'frobenius_norm_relative': 0.1380827882238269,\n",
       "  'spectral_norm': 4.033174514770508,\n",
       "  'spectral_norm_relative': 0.1380827882238269,\n",
       "  'mean_abs_difference': 0.0007826546207070351,\n",
       "  'max_abs_difference': 0.005859375,\n",
       "  'std_difference': 0.0009846893372014165,\n",
       "  'significant_diff_ratio': 0.9886963963508606,\n",
       "  'cosine_similarity': 0.9923021793365479,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_18_gate_proj': {'frobenius_norm': 8.674532890319824,\n",
       "  'frobenius_norm_relative': 0.06679616217164384,\n",
       "  'spectral_norm': 8.674532890319824,\n",
       "  'spectral_norm_relative': 0.06679616217164384,\n",
       "  'mean_abs_difference': 0.0010299839777871966,\n",
       "  'max_abs_difference': 0.008056640625,\n",
       "  'std_difference': 0.0012933069374412298,\n",
       "  'significant_diff_ratio': 0.973410964012146,\n",
       "  'cosine_similarity': 1.0038163661956787,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_10_v_proj': {'frobenius_norm': 4.3216423988342285,\n",
       "  'frobenius_norm_relative': 0.07410424787087458,\n",
       "  'spectral_norm': 4.3216423988342285,\n",
       "  'spectral_norm_relative': 0.07410424787087458,\n",
       "  'mean_abs_difference': 0.0008387219277210534,\n",
       "  'max_abs_difference': 0.006000518798828125,\n",
       "  'std_difference': 0.0010554458713158965,\n",
       "  'significant_diff_ratio': 0.9758978486061096,\n",
       "  'cosine_similarity': 0.9984674453735352,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_10_input_layernorm': {'frobenius_norm': 0.24399803578853607,\n",
       "  'frobenius_norm_relative': 0.011048059870821136,\n",
       "  'spectral_norm': 0.24399803578853607,\n",
       "  'spectral_norm_relative': 0.011048059870821136,\n",
       "  'mean_abs_difference': 0.003567485371604562,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0013447717064991593,\n",
       "  'significant_diff_ratio': 0.985595703125,\n",
       "  'cosine_similarity': 0.9999926686286926,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_4_gate_proj': {'frobenius_norm': 7.7505693435668945,\n",
       "  'frobenius_norm_relative': 0.06016439243461498,\n",
       "  'spectral_norm': 7.7505693435668945,\n",
       "  'spectral_norm_relative': 0.06016439243461498,\n",
       "  'mean_abs_difference': 0.0009204488596878946,\n",
       "  'max_abs_difference': 0.00732421875,\n",
       "  'std_difference': 0.0011556372046470642,\n",
       "  'significant_diff_ratio': 0.9704627394676208,\n",
       "  'cosine_similarity': 1.0040373802185059,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_13_gate_proj': {'frobenius_norm': 8.498128890991211,\n",
       "  'frobenius_norm_relative': 0.06730882475302699,\n",
       "  'spectral_norm': 8.498128890991211,\n",
       "  'spectral_norm_relative': 0.06730882475302699,\n",
       "  'mean_abs_difference': 0.001008506747893989,\n",
       "  'max_abs_difference': 0.00732421875,\n",
       "  'std_difference': 0.0012670363066717982,\n",
       "  'significant_diff_ratio': 0.9737247228622437,\n",
       "  'cosine_similarity': 1.003535270690918,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_11_v_proj': {'frobenius_norm': 4.336440086364746,\n",
       "  'frobenius_norm_relative': 0.07132196361360946,\n",
       "  'spectral_norm': 4.336440086364746,\n",
       "  'spectral_norm_relative': 0.07132196361360946,\n",
       "  'mean_abs_difference': 0.0008401564555242658,\n",
       "  'max_abs_difference': 0.006855010986328125,\n",
       "  'std_difference': 0.001059110858477652,\n",
       "  'significant_diff_ratio': 0.9749435186386108,\n",
       "  'cosine_similarity': 0.9983332753181458,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_23_v_proj': {'frobenius_norm': 4.85582971572876,\n",
       "  'frobenius_norm_relative': 0.06339599946682227,\n",
       "  'spectral_norm': 4.85582971572876,\n",
       "  'spectral_norm_relative': 0.06339599946682227,\n",
       "  'mean_abs_difference': 0.0009431222570128739,\n",
       "  'max_abs_difference': 0.007293701171875,\n",
       "  'std_difference': 0.0011859370861202478,\n",
       "  'significant_diff_ratio': 0.9718872308731079,\n",
       "  'cosine_similarity': 0.9992371797561646,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_21_down_proj': {'frobenius_norm': 8.404852867126465,\n",
       "  'frobenius_norm_relative': 0.06913933601079915,\n",
       "  'spectral_norm': 8.404852867126465,\n",
       "  'spectral_norm_relative': 0.06913933601079915,\n",
       "  'mean_abs_difference': 0.000997869879938662,\n",
       "  'max_abs_difference': 0.0096435546875,\n",
       "  'std_difference': 0.001253011403605342,\n",
       "  'significant_diff_ratio': 0.9740197658538818,\n",
       "  'cosine_similarity': 1.0036449432373047,\n",
       "  'weight_shape': torch.Size([4096, 11008]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_8_gate_proj': {'frobenius_norm': 8.190411567687988,\n",
       "  'frobenius_norm_relative': 0.0631909509507113,\n",
       "  'spectral_norm': 8.190411567687988,\n",
       "  'spectral_norm_relative': 0.0631909509507113,\n",
       "  'mean_abs_difference': 0.0009723561233840883,\n",
       "  'max_abs_difference': 0.007568359375,\n",
       "  'std_difference': 0.0012212072033435106,\n",
       "  'significant_diff_ratio': 0.9719893932342529,\n",
       "  'cosine_similarity': 1.0041007995605469,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_11_q_proj': {'frobenius_norm': 5.268446922302246,\n",
       "  'frobenius_norm_relative': 0.05438147991427175,\n",
       "  'spectral_norm': 5.268446922302246,\n",
       "  'spectral_norm_relative': 0.05438147991427175,\n",
       "  'mean_abs_difference': 0.0010217539966106415,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0012868057237938046,\n",
       "  'significant_diff_ratio': 0.9690695405006409,\n",
       "  'cosine_similarity': 0.9999977946281433,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_1_down_proj': {'frobenius_norm': 7.356190204620361,\n",
       "  'frobenius_norm_relative': 0.06365712666394607,\n",
       "  'spectral_norm': 7.356190204620361,\n",
       "  'spectral_norm_relative': 0.06365712666394607,\n",
       "  'mean_abs_difference': 0.0008733381982892752,\n",
       "  'max_abs_difference': 0.015625,\n",
       "  'std_difference': 0.0010966553818434477,\n",
       "  'significant_diff_ratio': 0.9719694256782532,\n",
       "  'cosine_similarity': 1.0045015811920166,\n",
       "  'weight_shape': torch.Size([4096, 11008]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_3_post_attention_layernorm': {'frobenius_norm': 0.12203644216060638,\n",
       "  'frobenius_norm_relative': 0.011173604217130586,\n",
       "  'spectral_norm': 0.12203644216060638,\n",
       "  'spectral_norm_relative': 0.011173604217130586,\n",
       "  'mean_abs_difference': 0.0016355335246771574,\n",
       "  'max_abs_difference': 0.0048828125,\n",
       "  'std_difference': 0.0010282741859555244,\n",
       "  'significant_diff_ratio': 0.88671875,\n",
       "  'cosine_similarity': 0.9999821782112122,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_19_o_proj': {'frobenius_norm': 4.870305061340332,\n",
       "  'frobenius_norm_relative': 0.07062704341751912,\n",
       "  'spectral_norm': 4.870305061340332,\n",
       "  'spectral_norm_relative': 0.07062704341751912,\n",
       "  'mean_abs_difference': 0.0009452314116060734,\n",
       "  'max_abs_difference': 0.0067138671875,\n",
       "  'std_difference': 0.0011892324546352029,\n",
       "  'significant_diff_ratio': 0.9746370911598206,\n",
       "  'cosine_similarity': 0.998685359954834,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_29_up_proj': {'frobenius_norm': 8.521784782409668,\n",
       "  'frobenius_norm_relative': 0.06693721046333605,\n",
       "  'spectral_norm': 8.521784782409668,\n",
       "  'spectral_norm_relative': 0.06693721046333605,\n",
       "  'mean_abs_difference': 0.0010106184054166079,\n",
       "  'max_abs_difference': 0.0107421875,\n",
       "  'std_difference': 0.0012705420376732945,\n",
       "  'significant_diff_ratio': 0.9732277393341064,\n",
       "  'cosine_similarity': 1.003395915031433,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_2_q_proj': {'frobenius_norm': 4.333070278167725,\n",
       "  'frobenius_norm_relative': 0.04011743892081072,\n",
       "  'spectral_norm': 4.333070278167725,\n",
       "  'spectral_norm_relative': 0.04011743892081072,\n",
       "  'mean_abs_difference': 0.0008362624794244766,\n",
       "  'max_abs_difference': 0.015625,\n",
       "  'std_difference': 0.0010591002646833658,\n",
       "  'significant_diff_ratio': 0.9600001573562622,\n",
       "  'cosine_similarity': 1.0010275840759277,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_30_post_attention_layernorm': {'frobenius_norm': 0.3302463889122009,\n",
       "  'frobenius_norm_relative': 0.010899518116902545,\n",
       "  'spectral_norm': 0.3302463889122009,\n",
       "  'spectral_norm_relative': 0.010899518116902545,\n",
       "  'mean_abs_difference': 0.0049591064453125,\n",
       "  'max_abs_difference': 0.009765625,\n",
       "  'std_difference': 0.0014271489344537258,\n",
       "  'significant_diff_ratio': 0.999755859375,\n",
       "  'cosine_similarity': 0.9999991655349731,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_17_post_attention_layernorm': {'frobenius_norm': 0.17321394383907318,\n",
       "  'frobenius_norm_relative': 0.008561483445090095,\n",
       "  'spectral_norm': 0.17321394383907318,\n",
       "  'spectral_norm_relative': 0.008561483445090095,\n",
       "  'mean_abs_difference': 0.0023543834686279297,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0013578921789303422,\n",
       "  'significant_diff_ratio': 0.869384765625,\n",
       "  'cosine_similarity': 0.9999908804893494,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_27_up_proj': {'frobenius_norm': 8.54171085357666,\n",
       "  'frobenius_norm_relative': 0.06837060127795463,\n",
       "  'spectral_norm': 8.54171085357666,\n",
       "  'spectral_norm_relative': 0.06837060127795463,\n",
       "  'mean_abs_difference': 0.0010138978250324726,\n",
       "  'max_abs_difference': 0.009765625,\n",
       "  'std_difference': 0.0012734532356262207,\n",
       "  'significant_diff_ratio': 0.9737817645072937,\n",
       "  'cosine_similarity': 1.0034101009368896,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_1_o_proj': {'frobenius_norm': 4.284850597381592,\n",
       "  'frobenius_norm_relative': 0.12753447212455707,\n",
       "  'spectral_norm': 4.284850597381592,\n",
       "  'spectral_norm_relative': 0.12753447212455707,\n",
       "  'mean_abs_difference': 0.0008328189142048359,\n",
       "  'max_abs_difference': 0.005859375,\n",
       "  'std_difference': 0.0010462022619321942,\n",
       "  'significant_diff_ratio': 0.9874928593635559,\n",
       "  'cosine_similarity': 0.9937450885772705,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_1_q_proj': {'frobenius_norm': 4.074953556060791,\n",
       "  'frobenius_norm_relative': 0.0379502716177451,\n",
       "  'spectral_norm': 4.074953556060791,\n",
       "  'spectral_norm_relative': 0.0379502716177451,\n",
       "  'mean_abs_difference': 0.0007840266334824264,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0009960058378055692,\n",
       "  'significant_diff_ratio': 0.9630439281463623,\n",
       "  'cosine_similarity': 1.0009145736694336,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_13_down_proj': {'frobenius_norm': 7.961238861083984,\n",
       "  'frobenius_norm_relative': 0.06574574966223729,\n",
       "  'spectral_norm': 7.961238861083984,\n",
       "  'spectral_norm_relative': 0.06574574966223729,\n",
       "  'mean_abs_difference': 0.000944170169532299,\n",
       "  'max_abs_difference': 0.015625,\n",
       "  'std_difference': 0.001186797278933227,\n",
       "  'significant_diff_ratio': 0.9727351069450378,\n",
       "  'cosine_similarity': 1.003955602645874,\n",
       "  'weight_shape': torch.Size([4096, 11008]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_17_gate_proj': {'frobenius_norm': 8.62624740600586,\n",
       "  'frobenius_norm_relative': 0.06701213895445002,\n",
       "  'spectral_norm': 8.62624740600586,\n",
       "  'spectral_norm_relative': 0.06701213895445002,\n",
       "  'mean_abs_difference': 0.0010239921975880861,\n",
       "  'max_abs_difference': 0.0076904296875,\n",
       "  'std_difference': 0.0012861188733950257,\n",
       "  'significant_diff_ratio': 0.973486602306366,\n",
       "  'cosine_similarity': 1.0036693811416626,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_24_up_proj': {'frobenius_norm': 8.566575050354004,\n",
       "  'frobenius_norm_relative': 0.06945619311327554,\n",
       "  'spectral_norm': 8.566575050354004,\n",
       "  'spectral_norm_relative': 0.06945619311327554,\n",
       "  'mean_abs_difference': 0.0010171601315960288,\n",
       "  'max_abs_difference': 0.009765625,\n",
       "  'std_difference': 0.0012771247420459986,\n",
       "  'significant_diff_ratio': 0.9741190671920776,\n",
       "  'cosine_similarity': 1.003485918045044,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_8_o_proj': {'frobenius_norm': 4.410915374755859,\n",
       "  'frobenius_norm_relative': 0.07760407669125106,\n",
       "  'spectral_norm': 4.410915374755859,\n",
       "  'spectral_norm_relative': 0.07760407669125106,\n",
       "  'mean_abs_difference': 0.0008548144833184779,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0010771312518045306,\n",
       "  'significant_diff_ratio': 0.9767356514930725,\n",
       "  'cosine_similarity': 0.998130738735199,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_31_gate_proj': {'frobenius_norm': 8.413201332092285,\n",
       "  'frobenius_norm_relative': 0.059064352598252,\n",
       "  'spectral_norm': 8.413201332092285,\n",
       "  'spectral_norm_relative': 0.059064352598252,\n",
       "  'mean_abs_difference': 0.0009964585769921541,\n",
       "  'max_abs_difference': 0.013671875,\n",
       "  'std_difference': 0.0012546507641673088,\n",
       "  'significant_diff_ratio': 0.9700521230697632,\n",
       "  'cosine_similarity': 1.0054513216018677,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_22_k_proj': {'frobenius_norm': 5.244569778442383,\n",
       "  'frobenius_norm_relative': 0.05616871804830197,\n",
       "  'spectral_norm': 5.244569778442383,\n",
       "  'spectral_norm_relative': 0.05616871804830197,\n",
       "  'mean_abs_difference': 0.0010171675821766257,\n",
       "  'max_abs_difference': 0.007568359375,\n",
       "  'std_difference': 0.0012810220941901207,\n",
       "  'significant_diff_ratio': 0.96922767162323,\n",
       "  'cosine_similarity': 0.9999568462371826,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_10_post_attention_layernorm': {'frobenius_norm': 0.12156364321708679,\n",
       "  'frobenius_norm_relative': 0.00806576181934551,\n",
       "  'spectral_norm': 0.12156364321708679,\n",
       "  'spectral_norm_relative': 0.00806576181934551,\n",
       "  'mean_abs_difference': 0.0016049742698669434,\n",
       "  'max_abs_difference': 0.005859375,\n",
       "  'std_difference': 0.0011078525567427278,\n",
       "  'significant_diff_ratio': 0.86962890625,\n",
       "  'cosine_similarity': 0.9999924898147583,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_14_q_proj': {'frobenius_norm': 5.366689682006836,\n",
       "  'frobenius_norm_relative': 0.054965457429190664,\n",
       "  'spectral_norm': 5.366689682006836,\n",
       "  'spectral_norm_relative': 0.054965457429190664,\n",
       "  'mean_abs_difference': 0.0010410810355097055,\n",
       "  'max_abs_difference': 0.007568359375,\n",
       "  'std_difference': 0.0013108383864164352,\n",
       "  'significant_diff_ratio': 0.968817412853241,\n",
       "  'cosine_similarity': 1.0000410079956055,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_8_q_proj': {'frobenius_norm': 5.1102094650268555,\n",
       "  'frobenius_norm_relative': 0.05012070814943925,\n",
       "  'spectral_norm': 5.1102094650268555,\n",
       "  'spectral_norm_relative': 0.05012070814943925,\n",
       "  'mean_abs_difference': 0.0009908040519803762,\n",
       "  'max_abs_difference': 0.007183074951171875,\n",
       "  'std_difference': 0.001248408225364983,\n",
       "  'significant_diff_ratio': 0.9663439989089966,\n",
       "  'cosine_similarity': 1.0002453327178955,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_21_k_proj': {'frobenius_norm': 5.294196128845215,\n",
       "  'frobenius_norm_relative': 0.05799056292352896,\n",
       "  'spectral_norm': 5.294196128845215,\n",
       "  'spectral_norm_relative': 0.05799056292352896,\n",
       "  'mean_abs_difference': 0.0010271577630192041,\n",
       "  'max_abs_difference': 0.0074462890625,\n",
       "  'std_difference': 0.0012930878438055515,\n",
       "  'significant_diff_ratio': 0.9703312516212463,\n",
       "  'cosine_similarity': 0.9996341466903687,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_0_k_proj': {'frobenius_norm': 2.53912091255188,\n",
       "  'frobenius_norm_relative': 0.04142570042731183,\n",
       "  'spectral_norm': 2.53912091255188,\n",
       "  'spectral_norm_relative': 0.04142570042731183,\n",
       "  'mean_abs_difference': 0.0004406938096508384,\n",
       "  'max_abs_difference': 0.0166015625,\n",
       "  'std_difference': 0.0006204194505698979,\n",
       "  'significant_diff_ratio': 0.9621654748916626,\n",
       "  'cosine_similarity': 1.0013927221298218,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_1_input_layernorm': {'frobenius_norm': 0.07843246310949326,\n",
       "  'frobenius_norm_relative': 0.013330697491077077,\n",
       "  'spectral_norm': 0.07843246310949326,\n",
       "  'spectral_norm_relative': 0.013330697491077077,\n",
       "  'mean_abs_difference': 0.0009403752046637237,\n",
       "  'max_abs_difference': 0.005859375,\n",
       "  'std_difference': 0.0010028055403381586,\n",
       "  'significant_diff_ratio': 0.88134765625,\n",
       "  'cosine_similarity': 0.9999561309814453,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_25_v_proj': {'frobenius_norm': 4.972454071044922,\n",
       "  'frobenius_norm_relative': 0.0631493340706661,\n",
       "  'spectral_norm': 4.972454071044922,\n",
       "  'spectral_norm_relative': 0.0631493340706661,\n",
       "  'mean_abs_difference': 0.0009660234209150076,\n",
       "  'max_abs_difference': 0.006591796875,\n",
       "  'std_difference': 0.001214389456436038,\n",
       "  'significant_diff_ratio': 0.9716722965240479,\n",
       "  'cosine_similarity': 0.999299943447113,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_3_gate_proj': {'frobenius_norm': 7.588742733001709,\n",
       "  'frobenius_norm_relative': 0.06012720269671633,\n",
       "  'spectral_norm': 7.588742733001709,\n",
       "  'spectral_norm_relative': 0.06012720269671633,\n",
       "  'mean_abs_difference': 0.0009015626274049282,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0011314955772832036,\n",
       "  'significant_diff_ratio': 0.9703407883644104,\n",
       "  'cosine_similarity': 1.0038514137268066,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_31_k_proj': {'frobenius_norm': 5.498818397521973,\n",
       "  'frobenius_norm_relative': 0.0600335496510609,\n",
       "  'spectral_norm': 5.498818397521973,\n",
       "  'spectral_norm_relative': 0.0600335496510609,\n",
       "  'mean_abs_difference': 0.0010643609566614032,\n",
       "  'max_abs_difference': 0.015625,\n",
       "  'std_difference': 0.0013429520186036825,\n",
       "  'significant_diff_ratio': 0.9710589647293091,\n",
       "  'cosine_similarity': 0.9993984699249268,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_2_v_proj': {'frobenius_norm': 4.275660991668701,\n",
       "  'frobenius_norm_relative': 0.07336246621539821,\n",
       "  'spectral_norm': 4.275660991668701,\n",
       "  'spectral_norm_relative': 0.07336246621539821,\n",
       "  'mean_abs_difference': 0.0008289013057947159,\n",
       "  'max_abs_difference': 0.006072998046875,\n",
       "  'std_difference': 0.0010442895581945777,\n",
       "  'significant_diff_ratio': 0.9756855964660645,\n",
       "  'cosine_similarity': 0.998313307762146,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_29_gate_proj': {'frobenius_norm': 8.810039520263672,\n",
       "  'frobenius_norm_relative': 0.06599283556659254,\n",
       "  'spectral_norm': 8.810039520263672,\n",
       "  'spectral_norm_relative': 0.06599283556659254,\n",
       "  'mean_abs_difference': 0.0010458707110956311,\n",
       "  'max_abs_difference': 0.009765625,\n",
       "  'std_difference': 0.0013134951004758477,\n",
       "  'significant_diff_ratio': 0.973149836063385,\n",
       "  'cosine_similarity': 1.004440426826477,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_8_up_proj': {'frobenius_norm': 7.824586868286133,\n",
       "  'frobenius_norm_relative': 0.06663277261360907,\n",
       "  'spectral_norm': 7.824586868286133,\n",
       "  'spectral_norm_relative': 0.06663277261360907,\n",
       "  'mean_abs_difference': 0.0009281010716222227,\n",
       "  'max_abs_difference': 0.006988525390625,\n",
       "  'std_difference': 0.0011663895566016436,\n",
       "  'significant_diff_ratio': 0.9730927348136902,\n",
       "  'cosine_similarity': 1.0041662454605103,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_22_up_proj': {'frobenius_norm': 8.469330787658691,\n",
       "  'frobenius_norm_relative': 0.06921897576134509,\n",
       "  'spectral_norm': 8.469330787658691,\n",
       "  'spectral_norm_relative': 0.06921897576134509,\n",
       "  'mean_abs_difference': 0.0010055670281872153,\n",
       "  'max_abs_difference': 0.01025390625,\n",
       "  'std_difference': 0.001262644655071199,\n",
       "  'significant_diff_ratio': 0.9740433096885681,\n",
       "  'cosine_similarity': 1.0035878419876099,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_22_gate_proj': {'frobenius_norm': 8.77707290649414,\n",
       "  'frobenius_norm_relative': 0.06616109090398017,\n",
       "  'spectral_norm': 8.77707290649414,\n",
       "  'spectral_norm_relative': 0.06616109090398017,\n",
       "  'mean_abs_difference': 0.001042781863361597,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0013085457030683756,\n",
       "  'significant_diff_ratio': 0.9730570316314697,\n",
       "  'cosine_similarity': 1.0041927099227905,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_8_v_proj': {'frobenius_norm': 4.413210391998291,\n",
       "  'frobenius_norm_relative': 0.07639175667863583,\n",
       "  'spectral_norm': 4.413210391998291,\n",
       "  'spectral_norm_relative': 0.07639175667863583,\n",
       "  'mean_abs_difference': 0.0008565316675230861,\n",
       "  'max_abs_difference': 0.006103515625,\n",
       "  'std_difference': 0.001077725668437779,\n",
       "  'significant_diff_ratio': 0.9766912460327148,\n",
       "  'cosine_similarity': 0.9982579350471497,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_0_up_proj': {'frobenius_norm': 7.323666095733643,\n",
       "  'frobenius_norm_relative': 0.06839279080594045,\n",
       "  'spectral_norm': 7.323666095733643,\n",
       "  'spectral_norm_relative': 0.06839279080594045,\n",
       "  'mean_abs_difference': 0.0008687714580446482,\n",
       "  'max_abs_difference': 0.00632476806640625,\n",
       "  'std_difference': 0.0010916744358837605,\n",
       "  'significant_diff_ratio': 0.9738690853118896,\n",
       "  'cosine_similarity': 1.0047249794006348,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_11_gate_proj': {'frobenius_norm': 8.468936920166016,\n",
       "  'frobenius_norm_relative': 0.06640998051740638,\n",
       "  'spectral_norm': 8.468936920166016,\n",
       "  'spectral_norm_relative': 0.06640998051740638,\n",
       "  'mean_abs_difference': 0.0010051733115687966,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.001262707868590951,\n",
       "  'significant_diff_ratio': 0.9733859300613403,\n",
       "  'cosine_similarity': 1.003607153892517,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_11_down_proj': {'frobenius_norm': 7.91891622543335,\n",
       "  'frobenius_norm_relative': 0.06647033165094346,\n",
       "  'spectral_norm': 7.91891622543335,\n",
       "  'spectral_norm_relative': 0.06647033165094346,\n",
       "  'mean_abs_difference': 0.0009391416097059846,\n",
       "  'max_abs_difference': 0.01171875,\n",
       "  'std_difference': 0.001180442632175982,\n",
       "  'significant_diff_ratio': 0.9730153679847717,\n",
       "  'cosine_similarity': 1.0040627717971802,\n",
       "  'weight_shape': torch.Size([4096, 11008]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_30_q_proj': {'frobenius_norm': 5.335108757019043,\n",
       "  'frobenius_norm_relative': 0.06108547575074326,\n",
       "  'spectral_norm': 5.335108757019043,\n",
       "  'spectral_norm_relative': 0.06108547575074326,\n",
       "  'mean_abs_difference': 0.0010352940298616886,\n",
       "  'max_abs_difference': 0.009765625,\n",
       "  'std_difference': 0.0013029645197093487,\n",
       "  'significant_diff_ratio': 0.9713774919509888,\n",
       "  'cosine_similarity': 0.9992645382881165,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_30_down_proj': {'frobenius_norm': 8.022528648376465,\n",
       "  'frobenius_norm_relative': 0.06426361420737257,\n",
       "  'spectral_norm': 8.022528648376465,\n",
       "  'spectral_norm_relative': 0.06426361420737257,\n",
       "  'mean_abs_difference': 0.000939988880418241,\n",
       "  'max_abs_difference': 0.015625,\n",
       "  'std_difference': 0.0011962716234847903,\n",
       "  'significant_diff_ratio': 0.9713287949562073,\n",
       "  'cosine_similarity': 1.003781795501709,\n",
       "  'weight_shape': torch.Size([4096, 11008]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_16_down_proj': {'frobenius_norm': 8.063284873962402,\n",
       "  'frobenius_norm_relative': 0.06632824928254634,\n",
       "  'spectral_norm': 8.063284873962402,\n",
       "  'spectral_norm_relative': 0.06632824928254634,\n",
       "  'mean_abs_difference': 0.0009563038474880159,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0012020273134112358,\n",
       "  'significant_diff_ratio': 0.9729512929916382,\n",
       "  'cosine_similarity': 1.003871202468872,\n",
       "  'weight_shape': torch.Size([4096, 11008]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_28_gate_proj': {'frobenius_norm': 8.845413208007812,\n",
       "  'frobenius_norm_relative': 0.0664297459423523,\n",
       "  'spectral_norm': 8.845413208007812,\n",
       "  'spectral_norm_relative': 0.0664297459423523,\n",
       "  'mean_abs_difference': 0.00105039041955024,\n",
       "  'max_abs_difference': 0.009765625,\n",
       "  'std_difference': 0.0013187070144340396,\n",
       "  'significant_diff_ratio': 0.9732878804206848,\n",
       "  'cosine_similarity': 1.004284143447876,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_12_k_proj': {'frobenius_norm': 5.351505279541016,\n",
       "  'frobenius_norm_relative': 0.052515864274516294,\n",
       "  'spectral_norm': 5.351505279541016,\n",
       "  'spectral_norm_relative': 0.052515864274516294,\n",
       "  'mean_abs_difference': 0.0010375857818871737,\n",
       "  'max_abs_difference': 0.0076904296875,\n",
       "  'std_difference': 0.0013072373112663627,\n",
       "  'significant_diff_ratio': 0.9677895903587341,\n",
       "  'cosine_similarity': 1.0001232624053955,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_19_k_proj': {'frobenius_norm': 5.321815013885498,\n",
       "  'frobenius_norm_relative': 0.05690165446426253,\n",
       "  'spectral_norm': 5.321815013885498,\n",
       "  'spectral_norm_relative': 0.05690165446426253,\n",
       "  'mean_abs_difference': 0.0010318574495613575,\n",
       "  'max_abs_difference': 0.00799560546875,\n",
       "  'std_difference': 0.0012998442398384213,\n",
       "  'significant_diff_ratio': 0.9697065949440002,\n",
       "  'cosine_similarity': 0.999660074710846,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_26_down_proj': {'frobenius_norm': 8.623748779296875,\n",
       "  'frobenius_norm_relative': 0.0696823651705728,\n",
       "  'spectral_norm': 8.623748779296875,\n",
       "  'spectral_norm_relative': 0.0696823651705728,\n",
       "  'mean_abs_difference': 0.0010234675137326121,\n",
       "  'max_abs_difference': 0.01171875,\n",
       "  'std_difference': 0.0012856641551479697,\n",
       "  'significant_diff_ratio': 0.9742385745048523,\n",
       "  'cosine_similarity': 1.0034387111663818,\n",
       "  'weight_shape': torch.Size([4096, 11008]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_30_up_proj': {'frobenius_norm': 8.418402671813965,\n",
       "  'frobenius_norm_relative': 0.06504543708575124,\n",
       "  'spectral_norm': 8.418402671813965,\n",
       "  'spectral_norm_relative': 0.06504543708575124,\n",
       "  'mean_abs_difference': 0.0009966341312974691,\n",
       "  'max_abs_difference': 0.0185546875,\n",
       "  'std_difference': 0.0012551960535347462,\n",
       "  'significant_diff_ratio': 0.9726055860519409,\n",
       "  'cosine_similarity': 1.0038806200027466,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_18_v_proj': {'frobenius_norm': 4.628059387207031,\n",
       "  'frobenius_norm_relative': 0.06700849402346584,\n",
       "  'spectral_norm': 4.628059387207031,\n",
       "  'spectral_norm_relative': 0.06700849402346584,\n",
       "  'mean_abs_difference': 0.0008982150466181338,\n",
       "  'max_abs_difference': 0.006591796875,\n",
       "  'std_difference': 0.0011303001083433628,\n",
       "  'significant_diff_ratio': 0.9734213948249817,\n",
       "  'cosine_similarity': 0.9989879131317139,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_10_down_proj': {'frobenius_norm': 7.899662017822266,\n",
       "  'frobenius_norm_relative': 0.0667437159963591,\n",
       "  'spectral_norm': 7.899662017822266,\n",
       "  'spectral_norm_relative': 0.0667437159963591,\n",
       "  'mean_abs_difference': 0.0009368686005473137,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0011775732273235917,\n",
       "  'significant_diff_ratio': 0.9731699228286743,\n",
       "  'cosine_similarity': 1.0041122436523438,\n",
       "  'weight_shape': torch.Size([4096, 11008]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_18_q_proj': {'frobenius_norm': 5.296039581298828,\n",
       "  'frobenius_norm_relative': 0.05700274041800103,\n",
       "  'spectral_norm': 5.296039581298828,\n",
       "  'spectral_norm_relative': 0.05700274041800103,\n",
       "  'mean_abs_difference': 0.0010259905830025673,\n",
       "  'max_abs_difference': 0.0080718994140625,\n",
       "  'std_difference': 0.0012935962295159698,\n",
       "  'significant_diff_ratio': 0.9693618416786194,\n",
       "  'cosine_similarity': 0.9996270537376404,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_13_post_attention_layernorm': {'frobenius_norm': 0.14650492370128632,\n",
       "  'frobenius_norm_relative': 0.008883903714886378,\n",
       "  'spectral_norm': 0.14650492370128632,\n",
       "  'spectral_norm_relative': 0.008883903714886378,\n",
       "  'mean_abs_difference': 0.001902759075164795,\n",
       "  'max_abs_difference': 0.005859375,\n",
       "  'std_difference': 0.0013206887524574995,\n",
       "  'significant_diff_ratio': 0.784912109375,\n",
       "  'cosine_similarity': 0.9999873638153076,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_5_o_proj': {'frobenius_norm': 4.421146869659424,\n",
       "  'frobenius_norm_relative': 0.07551400708216917,\n",
       "  'spectral_norm': 4.421146869659424,\n",
       "  'spectral_norm_relative': 0.07551400708216917,\n",
       "  'mean_abs_difference': 0.0008578454726375639,\n",
       "  'max_abs_difference': 0.0062255859375,\n",
       "  'std_difference': 0.0010796661954373121,\n",
       "  'significant_diff_ratio': 0.9762672781944275,\n",
       "  'cosine_similarity': 0.9982259273529053,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_9_q_proj': {'frobenius_norm': 5.138644218444824,\n",
       "  'frobenius_norm_relative': 0.050340476977520685,\n",
       "  'spectral_norm': 5.138644218444824,\n",
       "  'spectral_norm_relative': 0.050340476977520685,\n",
       "  'mean_abs_difference': 0.0009961165487766266,\n",
       "  'max_abs_difference': 0.00732421875,\n",
       "  'std_difference': 0.0012553904671221972,\n",
       "  'significant_diff_ratio': 0.9659645557403564,\n",
       "  'cosine_similarity': 1.0002446174621582,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_15_down_proj': {'frobenius_norm': 8.011927604675293,\n",
       "  'frobenius_norm_relative': 0.0658570445236215,\n",
       "  'spectral_norm': 8.011927604675293,\n",
       "  'spectral_norm_relative': 0.0658570445236215,\n",
       "  'mean_abs_difference': 0.0009500262094661593,\n",
       "  'max_abs_difference': 0.0089111328125,\n",
       "  'std_difference': 0.0011943490244448185,\n",
       "  'significant_diff_ratio': 0.9727612733840942,\n",
       "  'cosine_similarity': 1.0039008855819702,\n",
       "  'weight_shape': torch.Size([4096, 11008]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_12_post_attention_layernorm': {'frobenius_norm': 0.13483990728855133,\n",
       "  'frobenius_norm_relative': 0.0084103488845568,\n",
       "  'spectral_norm': 0.13483990728855133,\n",
       "  'spectral_norm_relative': 0.0084103488845568,\n",
       "  'mean_abs_difference': 0.001734018325805664,\n",
       "  'max_abs_difference': 0.005859375,\n",
       "  'std_difference': 0.0012826650636270642,\n",
       "  'significant_diff_ratio': 0.789306640625,\n",
       "  'cosine_similarity': 0.999988317489624,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_30_o_proj': {'frobenius_norm': 5.0764875411987305,\n",
       "  'frobenius_norm_relative': 0.059449914044043824,\n",
       "  'spectral_norm': 5.0764875411987305,\n",
       "  'spectral_norm_relative': 0.059449914044043824,\n",
       "  'mean_abs_difference': 0.0009837074903771281,\n",
       "  'max_abs_difference': 0.008056640625,\n",
       "  'std_difference': 0.001239931327290833,\n",
       "  'significant_diff_ratio': 0.9698613286018372,\n",
       "  'cosine_similarity': 0.9993709325790405,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_17_input_layernorm': {'frobenius_norm': 0.29475247859954834,\n",
       "  'frobenius_norm_relative': 0.011123795088578284,\n",
       "  'spectral_norm': 0.29475247859954834,\n",
       "  'spectral_norm_relative': 0.011123795088578284,\n",
       "  'mean_abs_difference': 0.004403494764119387,\n",
       "  'max_abs_difference': 0.009765625,\n",
       "  'std_difference': 0.00134936289396137,\n",
       "  'significant_diff_ratio': 0.996337890625,\n",
       "  'cosine_similarity': 0.9999978542327881,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_18_o_proj': {'frobenius_norm': 4.791514873504639,\n",
       "  'frobenius_norm_relative': 0.07035233975716941,\n",
       "  'spectral_norm': 4.791514873504639,\n",
       "  'spectral_norm_relative': 0.07035233975716941,\n",
       "  'mean_abs_difference': 0.0009299881057813764,\n",
       "  'max_abs_difference': 0.00665283203125,\n",
       "  'std_difference': 0.0011700257891789079,\n",
       "  'significant_diff_ratio': 0.9744356870651245,\n",
       "  'cosine_similarity': 0.9986652731895447,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_18_up_proj': {'frobenius_norm': 8.304411888122559,\n",
       "  'frobenius_norm_relative': 0.06767455229969958,\n",
       "  'spectral_norm': 8.304411888122559,\n",
       "  'spectral_norm_relative': 0.06767455229969958,\n",
       "  'mean_abs_difference': 0.000985409365966916,\n",
       "  'max_abs_difference': 0.008056640625,\n",
       "  'std_difference': 0.001238072058185935,\n",
       "  'significant_diff_ratio': 0.9734672904014587,\n",
       "  'cosine_similarity': 1.0036559104919434,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_19_gate_proj': {'frobenius_norm': 8.692544937133789,\n",
       "  'frobenius_norm_relative': 0.06669191773144695,\n",
       "  'spectral_norm': 8.692544937133789,\n",
       "  'spectral_norm_relative': 0.06669191773144695,\n",
       "  'mean_abs_difference': 0.0010321848094463348,\n",
       "  'max_abs_difference': 0.0087890625,\n",
       "  'std_difference': 0.0012959794839844108,\n",
       "  'significant_diff_ratio': 0.9733238220214844,\n",
       "  'cosine_similarity': 1.00387704372406,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_24_input_layernorm': {'frobenius_norm': 0.3438488245010376,\n",
       "  'frobenius_norm_relative': 0.010801459111587135,\n",
       "  'spectral_norm': 0.3438488245010376,\n",
       "  'spectral_norm_relative': 0.010801459111587135,\n",
       "  'mean_abs_difference': 0.005087739787995815,\n",
       "  'max_abs_difference': 0.01171875,\n",
       "  'std_difference': 0.0017265193164348602,\n",
       "  'significant_diff_ratio': 0.99267578125,\n",
       "  'cosine_similarity': 0.9999963045120239,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_1_up_proj': {'frobenius_norm': 7.350497245788574,\n",
       "  'frobenius_norm_relative': 0.06453092850927723,\n",
       "  'spectral_norm': 7.350497245788574,\n",
       "  'spectral_norm_relative': 0.06453092850927723,\n",
       "  'mean_abs_difference': 0.0008728926768526435,\n",
       "  'max_abs_difference': 0.006195068359375,\n",
       "  'std_difference': 0.001095786690711975,\n",
       "  'significant_diff_ratio': 0.9723474383354187,\n",
       "  'cosine_similarity': 1.0045617818832397,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_18_input_layernorm': {'frobenius_norm': 0.27791568636894226,\n",
       "  'frobenius_norm_relative': 0.009929976612621252,\n",
       "  'spectral_norm': 0.27791568636894226,\n",
       "  'spectral_norm_relative': 0.009929976612621252,\n",
       "  'mean_abs_difference': 0.004119643941521645,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0013732697116211057,\n",
       "  'significant_diff_ratio': 0.993896484375,\n",
       "  'cosine_similarity': 0.9999984502792358,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_20_input_layernorm': {'frobenius_norm': 0.2916102111339569,\n",
       "  'frobenius_norm_relative': 0.010376312634160978,\n",
       "  'spectral_norm': 0.2916102111339569,\n",
       "  'spectral_norm_relative': 0.010376312634160978,\n",
       "  'mean_abs_difference': 0.004353410564363003,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0013465802185237408,\n",
       "  'significant_diff_ratio': 0.998291015625,\n",
       "  'cosine_similarity': 0.9999988079071045,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_3_k_proj': {'frobenius_norm': 4.5795769691467285,\n",
       "  'frobenius_norm_relative': 0.04306733296842791,\n",
       "  'spectral_norm': 4.5795769691467285,\n",
       "  'spectral_norm_relative': 0.04306733296842791,\n",
       "  'mean_abs_difference': 0.0008852165192365646,\n",
       "  'max_abs_difference': 0.0068359375,\n",
       "  'std_difference': 0.0011191990924999118,\n",
       "  'significant_diff_ratio': 0.9619109034538269,\n",
       "  'cosine_similarity': 1.0005735158920288,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_15_post_attention_layernorm': {'frobenius_norm': 0.14409592747688293,\n",
       "  'frobenius_norm_relative': 0.008050540092084484,\n",
       "  'spectral_norm': 0.14409592747688293,\n",
       "  'spectral_norm_relative': 0.008050540092084484,\n",
       "  'mean_abs_difference': 0.001860499382019043,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0013425260549411178,\n",
       "  'significant_diff_ratio': 0.771240234375,\n",
       "  'cosine_similarity': 0.9999885559082031,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_19_down_proj': {'frobenius_norm': 8.304408073425293,\n",
       "  'frobenius_norm_relative': 0.06829426156478931,\n",
       "  'spectral_norm': 8.304408073425293,\n",
       "  'spectral_norm_relative': 0.06829426156478931,\n",
       "  'mean_abs_difference': 0.0009856141405180097,\n",
       "  'max_abs_difference': 0.01171875,\n",
       "  'std_difference': 0.0012380407424643636,\n",
       "  'significant_diff_ratio': 0.9736819863319397,\n",
       "  'cosine_similarity': 1.0037070512771606,\n",
       "  'weight_shape': torch.Size([4096, 11008]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_13_q_proj': {'frobenius_norm': 5.308023452758789,\n",
       "  'frobenius_norm_relative': 0.05422569609304404,\n",
       "  'spectral_norm': 5.308023452758789,\n",
       "  'spectral_norm_relative': 0.05422569609304404,\n",
       "  'mean_abs_difference': 0.0010291552171111107,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0012965487549081445,\n",
       "  'significant_diff_ratio': 0.9683996438980103,\n",
       "  'cosine_similarity': 1.000087022781372,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_20_v_proj': {'frobenius_norm': 4.7370758056640625,\n",
       "  'frobenius_norm_relative': 0.0669852639495586,\n",
       "  'spectral_norm': 4.7370758056640625,\n",
       "  'spectral_norm_relative': 0.0669852639495586,\n",
       "  'mean_abs_difference': 0.0009192785946652293,\n",
       "  'max_abs_difference': 0.00701904296875,\n",
       "  'std_difference': 0.0011569190537557006,\n",
       "  'significant_diff_ratio': 0.9733082056045532,\n",
       "  'cosine_similarity': 0.9989830255508423,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_28_q_proj': {'frobenius_norm': 5.270599365234375,\n",
       "  'frobenius_norm_relative': 0.05916211243336499,\n",
       "  'spectral_norm': 5.270599365234375,\n",
       "  'spectral_norm_relative': 0.05916211243336499,\n",
       "  'mean_abs_difference': 0.0010230974294245243,\n",
       "  'max_abs_difference': 0.015625,\n",
       "  'std_difference': 0.0012872683582827449,\n",
       "  'significant_diff_ratio': 0.9705202579498291,\n",
       "  'cosine_similarity': 0.9993804693222046,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'final_norm': {'frobenius_norm': 0.6897059679031372,\n",
       "  'frobenius_norm_relative': 0.006069600970057737,\n",
       "  'spectral_norm': 0.6897059679031372,\n",
       "  'spectral_norm_relative': 0.006069600970057737,\n",
       "  'mean_abs_difference': 0.010121654719114304,\n",
       "  'max_abs_difference': 0.03125,\n",
       "  'std_difference': 0.0037019301671534777,\n",
       "  'significant_diff_ratio': 0.993896484375,\n",
       "  'cosine_similarity': 1.0000014305114746,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_8_down_proj': {'frobenius_norm': 7.804276466369629,\n",
       "  'frobenius_norm_relative': 0.06696756519796132,\n",
       "  'spectral_norm': 7.804276466369629,\n",
       "  'spectral_norm_relative': 0.06696756519796132,\n",
       "  'mean_abs_difference': 0.0009260291699320078,\n",
       "  'max_abs_difference': 0.01171875,\n",
       "  'std_difference': 0.0011633418034762144,\n",
       "  'significant_diff_ratio': 0.9732714891433716,\n",
       "  'cosine_similarity': 1.00424325466156,\n",
       "  'weight_shape': torch.Size([4096, 11008]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_14_o_proj': {'frobenius_norm': 4.389196872711182,\n",
       "  'frobenius_norm_relative': 0.07264744408065253,\n",
       "  'spectral_norm': 4.389196872711182,\n",
       "  'spectral_norm_relative': 0.07264744408065253,\n",
       "  'mean_abs_difference': 0.0008508003666065633,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0010719376150518656,\n",
       "  'significant_diff_ratio': 0.9751731157302856,\n",
       "  'cosine_similarity': 0.9983190298080444,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_8_input_layernorm': {'frobenius_norm': 0.22120004892349243,\n",
       "  'frobenius_norm_relative': 0.010286498154862364,\n",
       "  'spectral_norm': 0.22120004892349243,\n",
       "  'spectral_norm_relative': 0.010286498154862364,\n",
       "  'mean_abs_difference': 0.003193755866959691,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0013236732920631766,\n",
       "  'significant_diff_ratio': 0.96826171875,\n",
       "  'cosine_similarity': 0.9999924898147583,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_19_input_layernorm': {'frobenius_norm': 0.27105724811553955,\n",
       "  'frobenius_norm_relative': 0.009723762029276797,\n",
       "  'spectral_norm': 0.27105724811553955,\n",
       "  'spectral_norm_relative': 0.009723762029276797,\n",
       "  'mean_abs_difference': 0.004009516444057226,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0013658577809110284,\n",
       "  'significant_diff_ratio': 0.994140625,\n",
       "  'cosine_similarity': 0.9999986290931702,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_26_gate_proj': {'frobenius_norm': 8.811647415161133,\n",
       "  'frobenius_norm_relative': 0.0659610090199579,\n",
       "  'spectral_norm': 8.811647415161133,\n",
       "  'spectral_norm_relative': 0.0659610090199579,\n",
       "  'mean_abs_difference': 0.001046657213009894,\n",
       "  'max_abs_difference': 0.008056640625,\n",
       "  'std_difference': 0.001313704182393849,\n",
       "  'significant_diff_ratio': 0.9729995131492615,\n",
       "  'cosine_similarity': 1.0043638944625854,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_23_k_proj': {'frobenius_norm': 5.261019229888916,\n",
       "  'frobenius_norm_relative': 0.05702244021514039,\n",
       "  'spectral_norm': 5.261019229888916,\n",
       "  'spectral_norm_relative': 0.05702244021514039,\n",
       "  'mean_abs_difference': 0.001020663185045123,\n",
       "  'max_abs_difference': 0.0077056884765625,\n",
       "  'std_difference': 0.0012850731145590544,\n",
       "  'significant_diff_ratio': 0.9694520235061646,\n",
       "  'cosine_similarity': 0.9997819066047668,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_2_k_proj': {'frobenius_norm': 4.2913689613342285,\n",
       "  'frobenius_norm_relative': 0.037777744289514595,\n",
       "  'spectral_norm': 4.2913689613342285,\n",
       "  'spectral_norm_relative': 0.037777744289514595,\n",
       "  'mean_abs_difference': 0.0008263499476015568,\n",
       "  'max_abs_difference': 0.0062255859375,\n",
       "  'std_difference': 0.001048975856974721,\n",
       "  'significant_diff_ratio': 0.9589781165122986,\n",
       "  'cosine_similarity': 1.0011476278305054,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_28_down_proj': {'frobenius_norm': 8.511173248291016,\n",
       "  'frobenius_norm_relative': 0.06799768622570507,\n",
       "  'spectral_norm': 8.511173248291016,\n",
       "  'spectral_norm_relative': 0.06799768622570507,\n",
       "  'mean_abs_difference': 0.0010072553995996714,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0012689855648204684,\n",
       "  'significant_diff_ratio': 0.9734148383140564,\n",
       "  'cosine_similarity': 1.003453254699707,\n",
       "  'weight_shape': torch.Size([4096, 11008]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_6_down_proj': {'frobenius_norm': 7.707906723022461,\n",
       "  'frobenius_norm_relative': 0.06695023417548984,\n",
       "  'spectral_norm': 7.707906723022461,\n",
       "  'spectral_norm_relative': 0.06695023417548984,\n",
       "  'mean_abs_difference': 0.0009147930541075766,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0011489731259644032,\n",
       "  'significant_diff_ratio': 0.973260760307312,\n",
       "  'cosine_similarity': 1.0043410062789917,\n",
       "  'weight_shape': torch.Size([4096, 11008]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_5_up_proj': {'frobenius_norm': 7.61134147644043,\n",
       "  'frobenius_norm_relative': 0.06563303938054005,\n",
       "  'spectral_norm': 7.61134147644043,\n",
       "  'spectral_norm_relative': 0.06563303938054005,\n",
       "  'mean_abs_difference': 0.0009033440728671849,\n",
       "  'max_abs_difference': 0.00647735595703125,\n",
       "  'std_difference': 0.001134641352109611,\n",
       "  'significant_diff_ratio': 0.9727516174316406,\n",
       "  'cosine_similarity': 1.004326581954956,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_21_input_layernorm': {'frobenius_norm': 0.29096195101737976,\n",
       "  'frobenius_norm_relative': 0.00973043599180845,\n",
       "  'spectral_norm': 0.29096195101737976,\n",
       "  'spectral_norm_relative': 0.00973043599180845,\n",
       "  'mean_abs_difference': 0.004318595863878727,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.001420882879756391,\n",
       "  'significant_diff_ratio': 0.993896484375,\n",
       "  'cosine_similarity': 0.9999991059303284,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_23_down_proj': {'frobenius_norm': 8.527939796447754,\n",
       "  'frobenius_norm_relative': 0.06970808704240139,\n",
       "  'spectral_norm': 8.527939796447754,\n",
       "  'spectral_norm_relative': 0.06970808704240139,\n",
       "  'mean_abs_difference': 0.0010130091104656458,\n",
       "  'max_abs_difference': 0.00978851318359375,\n",
       "  'std_difference': 0.001271359040401876,\n",
       "  'significant_diff_ratio': 0.9743133187294006,\n",
       "  'cosine_similarity': 1.0035464763641357,\n",
       "  'weight_shape': torch.Size([4096, 11008]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_0_v_proj': {'frobenius_norm': 3.7908897399902344,\n",
       "  'frobenius_norm_relative': 0.0842658171334102,\n",
       "  'spectral_norm': 3.7908897399902344,\n",
       "  'spectral_norm_relative': 0.0842658171334102,\n",
       "  'mean_abs_difference': 0.0007258172845467925,\n",
       "  'max_abs_difference': 0.00634765625,\n",
       "  'std_difference': 0.0009258080390281975,\n",
       "  'significant_diff_ratio': 0.9805576801300049,\n",
       "  'cosine_similarity': 0.9975800514221191,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_22_o_proj': {'frobenius_norm': 5.014476776123047,\n",
       "  'frobenius_norm_relative': 0.06962438114711783,\n",
       "  'spectral_norm': 5.014476776123047,\n",
       "  'spectral_norm_relative': 0.06962438114711783,\n",
       "  'mean_abs_difference': 0.0009726113057695329,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0012244214303791523,\n",
       "  'significant_diff_ratio': 0.9743739366531372,\n",
       "  'cosine_similarity': 0.9988519549369812,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_27_input_layernorm': {'frobenius_norm': 0.3605331480503082,\n",
       "  'frobenius_norm_relative': 0.010371656038457562,\n",
       "  'spectral_norm': 0.3605331480503082,\n",
       "  'spectral_norm_relative': 0.010371656038457562,\n",
       "  'mean_abs_difference': 0.0052474793046712875,\n",
       "  'max_abs_difference': 0.01171875,\n",
       "  'std_difference': 0.0020492717158049345,\n",
       "  'significant_diff_ratio': 0.97705078125,\n",
       "  'cosine_similarity': 0.999993085861206,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_31_input_layernorm': {'frobenius_norm': 0.24491284787654877,\n",
       "  'frobenius_norm_relative': 0.008319830353624989,\n",
       "  'spectral_norm': 0.24491284787654877,\n",
       "  'spectral_norm_relative': 0.008319830353624989,\n",
       "  'mean_abs_difference': 0.0035457611083984375,\n",
       "  'max_abs_difference': 0.01171875,\n",
       "  'std_difference': 0.0014395138714462519,\n",
       "  'significant_diff_ratio': 0.9765625,\n",
       "  'cosine_similarity': 0.999998927116394,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_25_input_layernorm': {'frobenius_norm': 0.3556511402130127,\n",
       "  'frobenius_norm_relative': 0.010235893340148896,\n",
       "  'spectral_norm': 0.3556511402130127,\n",
       "  'spectral_norm_relative': 0.010235893340148896,\n",
       "  'mean_abs_difference': 0.005169880576431751,\n",
       "  'max_abs_difference': 0.01171875,\n",
       "  'std_difference': 0.002038171049207449,\n",
       "  'significant_diff_ratio': 0.974609375,\n",
       "  'cosine_similarity': 0.999993085861206,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_27_post_attention_layernorm': {'frobenius_norm': 0.2920877933502197,\n",
       "  'frobenius_norm_relative': 0.010211433745091822,\n",
       "  'spectral_norm': 0.2920877933502197,\n",
       "  'spectral_norm_relative': 0.010211433745091822,\n",
       "  'mean_abs_difference': 0.004338681697845459,\n",
       "  'max_abs_difference': 0.009765625,\n",
       "  'std_difference': 0.001433488680049777,\n",
       "  'significant_diff_ratio': 0.994873046875,\n",
       "  'cosine_similarity': 0.9999985098838806,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_20_up_proj': {'frobenius_norm': 8.37235164642334,\n",
       "  'frobenius_norm_relative': 0.06827905321541552,\n",
       "  'spectral_norm': 8.37235164642334,\n",
       "  'spectral_norm_relative': 0.06827905321541552,\n",
       "  'mean_abs_difference': 0.0009936142014339566,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0012481885496526957,\n",
       "  'significant_diff_ratio': 0.9736646413803101,\n",
       "  'cosine_similarity': 1.0036191940307617,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_29_input_layernorm': {'frobenius_norm': 0.33343014121055603,\n",
       "  'frobenius_norm_relative': 0.009740164936833044,\n",
       "  'spectral_norm': 0.33343014121055603,\n",
       "  'spectral_norm_relative': 0.009740164936833044,\n",
       "  'mean_abs_difference': 0.004798755049705505,\n",
       "  'max_abs_difference': 0.015625,\n",
       "  'std_difference': 0.0020286571234464645,\n",
       "  'significant_diff_ratio': 0.954833984375,\n",
       "  'cosine_similarity': 0.999993085861206,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_19_q_proj': {'frobenius_norm': 5.330599308013916,\n",
       "  'frobenius_norm_relative': 0.05808797848278871,\n",
       "  'spectral_norm': 5.330599308013916,\n",
       "  'spectral_norm_relative': 0.05808797848278871,\n",
       "  'mean_abs_difference': 0.0010338914580643177,\n",
       "  'max_abs_difference': 0.01171875,\n",
       "  'std_difference': 0.0013019619509577751,\n",
       "  'significant_diff_ratio': 0.9701717495918274,\n",
       "  'cosine_similarity': 0.9995095729827881,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_9_up_proj': {'frobenius_norm': 7.905624866485596,\n",
       "  'frobenius_norm_relative': 0.06686323705602694,\n",
       "  'spectral_norm': 7.905624866485596,\n",
       "  'spectral_norm_relative': 0.06686323705602694,\n",
       "  'mean_abs_difference': 0.0009376067901030183,\n",
       "  'max_abs_difference': 0.0072021484375,\n",
       "  'std_difference': 0.001178458333015442,\n",
       "  'significant_diff_ratio': 0.9731096625328064,\n",
       "  'cosine_similarity': 1.0040717124938965,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_13_v_proj': {'frobenius_norm': 4.355114459991455,\n",
       "  'frobenius_norm_relative': 0.06997270667139532,\n",
       "  'spectral_norm': 4.355114459991455,\n",
       "  'spectral_norm_relative': 0.06997270667139532,\n",
       "  'mean_abs_difference': 0.0008440351230092347,\n",
       "  'max_abs_difference': 0.006561279296875,\n",
       "  'std_difference': 0.001063698553480208,\n",
       "  'significant_diff_ratio': 0.9743920564651489,\n",
       "  'cosine_similarity': 0.9984908699989319,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_26_q_proj': {'frobenius_norm': 5.321805953979492,\n",
       "  'frobenius_norm_relative': 0.060231475149193754,\n",
       "  'spectral_norm': 5.321805953979492,\n",
       "  'spectral_norm_relative': 0.060231475149193754,\n",
       "  'mean_abs_difference': 0.0010325564071536064,\n",
       "  'max_abs_difference': 0.008056640625,\n",
       "  'std_difference': 0.0012996926670894027,\n",
       "  'significant_diff_ratio': 0.9711175560951233,\n",
       "  'cosine_similarity': 0.9995778203010559,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_5_down_proj': {'frobenius_norm': 7.689506530761719,\n",
       "  'frobenius_norm_relative': 0.0664140302248536,\n",
       "  'spectral_norm': 7.689506530761719,\n",
       "  'spectral_norm_relative': 0.0664140302248536,\n",
       "  'mean_abs_difference': 0.0009127834346145391,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0011462501715868711,\n",
       "  'significant_diff_ratio': 0.9730415940284729,\n",
       "  'cosine_similarity': 1.0043097734451294,\n",
       "  'weight_shape': torch.Size([4096, 11008]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_15_gate_proj': {'frobenius_norm': 8.518645286560059,\n",
       "  'frobenius_norm_relative': 0.06724467948953927,\n",
       "  'spectral_norm': 8.518645286560059,\n",
       "  'spectral_norm_relative': 0.06724467948953927,\n",
       "  'mean_abs_difference': 0.0010109497234225273,\n",
       "  'max_abs_difference': 0.00927734375,\n",
       "  'std_difference': 0.0012700851075351238,\n",
       "  'significant_diff_ratio': 0.973667323589325,\n",
       "  'cosine_similarity': 1.003510594367981,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_13_k_proj': {'frobenius_norm': 5.359001636505127,\n",
       "  'frobenius_norm_relative': 0.05377419231726087,\n",
       "  'spectral_norm': 5.359001636505127,\n",
       "  'spectral_norm_relative': 0.05377419231726087,\n",
       "  'mean_abs_difference': 0.0010387334041297436,\n",
       "  'max_abs_difference': 0.008056640625,\n",
       "  'std_difference': 0.001308994134888053,\n",
       "  'significant_diff_ratio': 0.9684460163116455,\n",
       "  'cosine_similarity': 1.0001786947250366,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_9_k_proj': {'frobenius_norm': 5.166133403778076,\n",
       "  'frobenius_norm_relative': 0.049375397631209864,\n",
       "  'spectral_norm': 5.166133403778076,\n",
       "  'spectral_norm_relative': 0.049375397631209864,\n",
       "  'mean_abs_difference': 0.00100141076836735,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0012621300993487239,\n",
       "  'significant_diff_ratio': 0.9657313227653503,\n",
       "  'cosine_similarity': 1.0002989768981934,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_4_input_layernorm': {'frobenius_norm': 0.17321208119392395,\n",
       "  'frobenius_norm_relative': 0.010367540051380061,\n",
       "  'spectral_norm': 0.17321208119392395,\n",
       "  'spectral_norm_relative': 0.010367540051380061,\n",
       "  'mean_abs_difference': 0.0024109738878905773,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0012447371846064925,\n",
       "  'significant_diff_ratio': 0.907958984375,\n",
       "  'cosine_similarity': 0.9999895691871643,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_20_down_proj': {'frobenius_norm': 8.306987762451172,\n",
       "  'frobenius_norm_relative': 0.0683307950171292,\n",
       "  'spectral_norm': 8.306987762451172,\n",
       "  'spectral_norm_relative': 0.0683307950171292,\n",
       "  'mean_abs_difference': 0.0009861363796517253,\n",
       "  'max_abs_difference': 0.015625,\n",
       "  'std_difference': 0.0012384267756715417,\n",
       "  'significant_diff_ratio': 0.9737603664398193,\n",
       "  'cosine_similarity': 1.0037128925323486,\n",
       "  'weight_shape': torch.Size([4096, 11008]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_10_o_proj': {'frobenius_norm': 4.326534271240234,\n",
       "  'frobenius_norm_relative': 0.07477565490452871,\n",
       "  'spectral_norm': 4.326534271240234,\n",
       "  'spectral_norm_relative': 0.07477565490452871,\n",
       "  'mean_abs_difference': 0.0008384695975109935,\n",
       "  'max_abs_difference': 0.0062255859375,\n",
       "  'std_difference': 0.0010566383134573698,\n",
       "  'significant_diff_ratio': 0.9758691787719727,\n",
       "  'cosine_similarity': 0.9982815980911255,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_4_o_proj': {'frobenius_norm': 4.485839366912842,\n",
       "  'frobenius_norm_relative': 0.07858351865384036,\n",
       "  'spectral_norm': 4.485839366912842,\n",
       "  'spectral_norm_relative': 0.07858351865384036,\n",
       "  'mean_abs_difference': 0.0008706428925506771,\n",
       "  'max_abs_difference': 0.006072998046875,\n",
       "  'std_difference': 0.001095354906283319,\n",
       "  'significant_diff_ratio': 0.9771450161933899,\n",
       "  'cosine_similarity': 0.9980177283287048,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_4_up_proj': {'frobenius_norm': 7.59793758392334,\n",
       "  'frobenius_norm_relative': 0.06552167259082625,\n",
       "  'spectral_norm': 7.59793758392334,\n",
       "  'spectral_norm_relative': 0.06552167259082625,\n",
       "  'mean_abs_difference': 0.000902065890841186,\n",
       "  'max_abs_difference': 0.0064697265625,\n",
       "  'std_difference': 0.0011326324893161654,\n",
       "  'significant_diff_ratio': 0.9726582765579224,\n",
       "  'cosine_similarity': 1.0043582916259766,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_6_post_attention_layernorm': {'frobenius_norm': 0.13462889194488525,\n",
       "  'frobenius_norm_relative': 0.010031056644220257,\n",
       "  'spectral_norm': 0.13462889194488525,\n",
       "  'spectral_norm_relative': 0.010031056644220257,\n",
       "  'mean_abs_difference': 0.0018409490585327148,\n",
       "  'max_abs_difference': 0.005859375,\n",
       "  'std_difference': 0.0010605325223878026,\n",
       "  'significant_diff_ratio': 0.913818359375,\n",
       "  'cosine_similarity': 0.9999902248382568,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_18_down_proj': {'frobenius_norm': 8.232666015625,\n",
       "  'frobenius_norm_relative': 0.06786050421198273,\n",
       "  'spectral_norm': 8.232666015625,\n",
       "  'spectral_norm_relative': 0.06786050421198273,\n",
       "  'mean_abs_difference': 0.0009769403841346502,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0012273507891222835,\n",
       "  'significant_diff_ratio': 0.9735403656959534,\n",
       "  'cosine_similarity': 1.0037798881530762,\n",
       "  'weight_shape': torch.Size([4096, 11008]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_20_k_proj': {'frobenius_norm': 5.356678009033203,\n",
       "  'frobenius_norm_relative': 0.05695513987905455,\n",
       "  'spectral_norm': 5.356678009033203,\n",
       "  'spectral_norm_relative': 0.05695513987905455,\n",
       "  'mean_abs_difference': 0.001038905931636691,\n",
       "  'max_abs_difference': 0.00732421875,\n",
       "  'std_difference': 0.0013084152014926076,\n",
       "  'significant_diff_ratio': 0.9698905348777771,\n",
       "  'cosine_similarity': 0.9998575448989868,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_9_post_attention_layernorm': {'frobenius_norm': 0.12474033236503601,\n",
       "  'frobenius_norm_relative': 0.008401572841351181,\n",
       "  'spectral_norm': 0.12474033236503601,\n",
       "  'spectral_norm_relative': 0.008401572841351181,\n",
       "  'mean_abs_difference': 0.0016632080078125,\n",
       "  'max_abs_difference': 0.005859375,\n",
       "  'std_difference': 0.0010933420853689313,\n",
       "  'significant_diff_ratio': 0.88232421875,\n",
       "  'cosine_similarity': 0.9999925494194031,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_21_gate_proj': {'frobenius_norm': 8.752872467041016,\n",
       "  'frobenius_norm_relative': 0.0663544808930907,\n",
       "  'spectral_norm': 8.752872467041016,\n",
       "  'spectral_norm_relative': 0.0663544808930907,\n",
       "  'mean_abs_difference': 0.0010396541329100728,\n",
       "  'max_abs_difference': 0.0126953125,\n",
       "  'std_difference': 0.001304931123740971,\n",
       "  'significant_diff_ratio': 0.973150908946991,\n",
       "  'cosine_similarity': 1.0041214227676392,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_30_gate_proj': {'frobenius_norm': 8.715604782104492,\n",
       "  'frobenius_norm_relative': 0.0638357060529505,\n",
       "  'spectral_norm': 8.715604782104492,\n",
       "  'spectral_norm_relative': 0.0638357060529505,\n",
       "  'mean_abs_difference': 0.0010338424472138286,\n",
       "  'max_abs_difference': 0.01025390625,\n",
       "  'std_difference': 0.001299481256864965,\n",
       "  'significant_diff_ratio': 0.9725122451782227,\n",
       "  'cosine_similarity': 1.0050028562545776,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_3_up_proj': {'frobenius_norm': 7.513704776763916,\n",
       "  'frobenius_norm_relative': 0.06433475996233581,\n",
       "  'spectral_norm': 7.513704776763916,\n",
       "  'spectral_norm_relative': 0.06433475996233581,\n",
       "  'mean_abs_difference': 0.000892461568582803,\n",
       "  'max_abs_difference': 0.006591796875,\n",
       "  'std_difference': 0.0011201227316632867,\n",
       "  'significant_diff_ratio': 0.9722139239311218,\n",
       "  'cosine_similarity': 1.0043563842773438,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_14_up_proj': {'frobenius_norm': 8.155856132507324,\n",
       "  'frobenius_norm_relative': 0.06632002394950386,\n",
       "  'spectral_norm': 8.155856132507324,\n",
       "  'spectral_norm_relative': 0.06632002394950386,\n",
       "  'mean_abs_difference': 0.0009674763423390687,\n",
       "  'max_abs_difference': 0.00732421875,\n",
       "  'std_difference': 0.001215900294482708,\n",
       "  'significant_diff_ratio': 0.9729374647140503,\n",
       "  'cosine_similarity': 1.0037484169006348,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_8_post_attention_layernorm': {'frobenius_norm': 0.13295693695545197,\n",
       "  'frobenius_norm_relative': 0.009164523625714686,\n",
       "  'spectral_norm': 0.13295693695545197,\n",
       "  'spectral_norm_relative': 0.009164523625714686,\n",
       "  'mean_abs_difference': 0.001799464225769043,\n",
       "  'max_abs_difference': 0.005859375,\n",
       "  'std_difference': 0.0010933233425021172,\n",
       "  'significant_diff_ratio': 0.902587890625,\n",
       "  'cosine_similarity': 0.9999920129776001,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_7_v_proj': {'frobenius_norm': 4.458589553833008,\n",
       "  'frobenius_norm_relative': 0.0792613309233287,\n",
       "  'spectral_norm': 4.458589553833008,\n",
       "  'spectral_norm_relative': 0.0792613309233287,\n",
       "  'mean_abs_difference': 0.0008649220690131187,\n",
       "  'max_abs_difference': 0.006103515625,\n",
       "  'std_difference': 0.0010887174867093563,\n",
       "  'significant_diff_ratio': 0.9774587154388428,\n",
       "  'cosine_similarity': 0.9978500604629517,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_21_o_proj': {'frobenius_norm': 5.080254077911377,\n",
       "  'frobenius_norm_relative': 0.07070374819150534,\n",
       "  'spectral_norm': 5.080254077911377,\n",
       "  'spectral_norm_relative': 0.07070374819150534,\n",
       "  'mean_abs_difference': 0.0009865862084552646,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0012404401786625385,\n",
       "  'significant_diff_ratio': 0.9746689796447754,\n",
       "  'cosine_similarity': 0.9987472891807556,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_29_post_attention_layernorm': {'frobenius_norm': 0.3236376643180847,\n",
       "  'frobenius_norm_relative': 0.010769506954186917,\n",
       "  'spectral_norm': 0.3236376643180847,\n",
       "  'spectral_norm_relative': 0.010769506954186917,\n",
       "  'mean_abs_difference': 0.004855513572692871,\n",
       "  'max_abs_difference': 0.009765625,\n",
       "  'std_difference': 0.0014218149008229375,\n",
       "  'significant_diff_ratio': 0.998046875,\n",
       "  'cosine_similarity': 0.9999992251396179,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_22_q_proj': {'frobenius_norm': 5.24020528793335,\n",
       "  'frobenius_norm_relative': 0.056827845664843446,\n",
       "  'spectral_norm': 5.24020528793335,\n",
       "  'spectral_norm_relative': 0.056827845664843446,\n",
       "  'mean_abs_difference': 0.0010160889942198992,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0012799483956769109,\n",
       "  'significant_diff_ratio': 0.9693853855133057,\n",
       "  'cosine_similarity': 0.9998118281364441,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_22_post_attention_layernorm': {'frobenius_norm': 0.2164146602153778,\n",
       "  'frobenius_norm_relative': 0.008812626756083942,\n",
       "  'spectral_norm': 0.2164146602153778,\n",
       "  'spectral_norm_relative': 0.008812626756083942,\n",
       "  'mean_abs_difference': 0.0030868053436279297,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0013908453984186053,\n",
       "  'significant_diff_ratio': 0.9541015625,\n",
       "  'cosine_similarity': 0.9999954700469971,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_3_v_proj': {'frobenius_norm': 4.3687310218811035,\n",
       "  'frobenius_norm_relative': 0.07902701786041318,\n",
       "  'spectral_norm': 4.3687310218811035,\n",
       "  'spectral_norm_relative': 0.07902701786041318,\n",
       "  'mean_abs_difference': 0.0008480151882395148,\n",
       "  'max_abs_difference': 0.00604248046875,\n",
       "  'std_difference': 0.0010669007897377014,\n",
       "  'significant_diff_ratio': 0.9774608016014099,\n",
       "  'cosine_similarity': 0.9982520341873169,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_6_v_proj': {'frobenius_norm': 4.4694013595581055,\n",
       "  'frobenius_norm_relative': 0.08047265616534091,\n",
       "  'spectral_norm': 4.4694013595581055,\n",
       "  'spectral_norm_relative': 0.08047265616534091,\n",
       "  'mean_abs_difference': 0.0008670289535075426,\n",
       "  'max_abs_difference': 0.00592041015625,\n",
       "  'std_difference': 0.001091293292120099,\n",
       "  'significant_diff_ratio': 0.9778518676757812,\n",
       "  'cosine_similarity': 0.9979537129402161,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_2_post_attention_layernorm': {'frobenius_norm': 0.09992062300443649,\n",
       "  'frobenius_norm_relative': 0.01159489469484401,\n",
       "  'spectral_norm': 0.09992062300443649,\n",
       "  'spectral_norm_relative': 0.01159489469484401,\n",
       "  'mean_abs_difference': 0.0012758595403283834,\n",
       "  'max_abs_difference': 0.0048828125,\n",
       "  'std_difference': 0.001034511486068368,\n",
       "  'significant_diff_ratio': 0.81591796875,\n",
       "  'cosine_similarity': 0.9999710917472839,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_21_v_proj': {'frobenius_norm': 4.879202842712402,\n",
       "  'frobenius_norm_relative': 0.06693156375590155,\n",
       "  'spectral_norm': 4.879202842712402,\n",
       "  'spectral_norm_relative': 0.06693156375590155,\n",
       "  'mean_abs_difference': 0.0009478670544922352,\n",
       "  'max_abs_difference': 0.007080078125,\n",
       "  'std_difference': 0.0011915052309632301,\n",
       "  'significant_diff_ratio': 0.9733640551567078,\n",
       "  'cosine_similarity': 0.9991537928581238,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_23_up_proj': {'frobenius_norm': 8.530685424804688,\n",
       "  'frobenius_norm_relative': 0.0693740222567811,\n",
       "  'spectral_norm': 8.530685424804688,\n",
       "  'spectral_norm_relative': 0.0693740222567811,\n",
       "  'mean_abs_difference': 0.0010129038710147142,\n",
       "  'max_abs_difference': 0.00732421875,\n",
       "  'std_difference': 0.0012717884965240955,\n",
       "  'significant_diff_ratio': 0.9741165041923523,\n",
       "  'cosine_similarity': 1.0035099983215332,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_3_input_layernorm': {'frobenius_norm': 0.20425836741924286,\n",
       "  'frobenius_norm_relative': 0.0111941802228016,\n",
       "  'spectral_norm': 0.20425836741924286,\n",
       "  'spectral_norm_relative': 0.0111941802228016,\n",
       "  'mean_abs_difference': 0.002934732474386692,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0012544470373541117,\n",
       "  'significant_diff_ratio': 0.958251953125,\n",
       "  'cosine_similarity': 0.9999905824661255,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_7_gate_proj': {'frobenius_norm': 8.042999267578125,\n",
       "  'frobenius_norm_relative': 0.06108231626230074,\n",
       "  'spectral_norm': 8.042999267578125,\n",
       "  'spectral_norm_relative': 0.06108231626230074,\n",
       "  'mean_abs_difference': 0.0009548623347654939,\n",
       "  'max_abs_difference': 0.0069580078125,\n",
       "  'std_difference': 0.001199236256070435,\n",
       "  'significant_diff_ratio': 0.9710086584091187,\n",
       "  'cosine_similarity': 1.0044723749160767,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_12_o_proj': {'frobenius_norm': 4.416309833526611,\n",
       "  'frobenius_norm_relative': 0.07417611945010123,\n",
       "  'spectral_norm': 4.416309833526611,\n",
       "  'spectral_norm_relative': 0.07417611945010123,\n",
       "  'mean_abs_difference': 0.0008556984830647707,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0010785047197714448,\n",
       "  'significant_diff_ratio': 0.9757921695709229,\n",
       "  'cosine_similarity': 0.998267650604248,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_12_q_proj': {'frobenius_norm': 5.287425994873047,\n",
       "  'frobenius_norm_relative': 0.05349028461161979,\n",
       "  'spectral_norm': 5.287425994873047,\n",
       "  'spectral_norm_relative': 0.05349028461161979,\n",
       "  'mean_abs_difference': 0.00102561479434371,\n",
       "  'max_abs_difference': 0.0072021484375,\n",
       "  'std_difference': 0.0012915695551782846,\n",
       "  'significant_diff_ratio': 0.9679966568946838,\n",
       "  'cosine_similarity': 1.0000038146972656,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_23_o_proj': {'frobenius_norm': 5.061686992645264,\n",
       "  'frobenius_norm_relative': 0.06690395447485839,\n",
       "  'spectral_norm': 5.061686992645264,\n",
       "  'spectral_norm_relative': 0.06690395447485839,\n",
       "  'mean_abs_difference': 0.0009831442730501294,\n",
       "  'max_abs_difference': 0.00677490234375,\n",
       "  'std_difference': 0.0012360257096588612,\n",
       "  'significant_diff_ratio': 0.973329484462738,\n",
       "  'cosine_similarity': 0.9990488290786743,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_6_q_proj': {'frobenius_norm': 4.9914374351501465,\n",
       "  'frobenius_norm_relative': 0.04934326684589363,\n",
       "  'spectral_norm': 4.9914374351501465,\n",
       "  'spectral_norm_relative': 0.04934326684589363,\n",
       "  'mean_abs_difference': 0.0009676441550254822,\n",
       "  'max_abs_difference': 0.00701904296875,\n",
       "  'std_difference': 0.001219457946717739,\n",
       "  'significant_diff_ratio': 0.9659169316291809,\n",
       "  'cosine_similarity': 1.0002386569976807,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_2_gate_proj': {'frobenius_norm': 7.436445236206055,\n",
       "  'frobenius_norm_relative': 0.05978342937917467,\n",
       "  'spectral_norm': 7.436445236206055,\n",
       "  'spectral_norm_relative': 0.05978342937917467,\n",
       "  'mean_abs_difference': 0.0008836511406116188,\n",
       "  'max_abs_difference': 0.006378173828125,\n",
       "  'std_difference': 0.0011087949387729168,\n",
       "  'significant_diff_ratio': 0.9701905250549316,\n",
       "  'cosine_similarity': 1.0040292739868164,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_13_up_proj': {'frobenius_norm': 8.09200668334961,\n",
       "  'frobenius_norm_relative': 0.06587627512996319,\n",
       "  'spectral_norm': 8.09200668334961,\n",
       "  'spectral_norm_relative': 0.06587627512996319,\n",
       "  'mean_abs_difference': 0.0009594949660822749,\n",
       "  'max_abs_difference': 0.0074462890625,\n",
       "  'std_difference': 0.0012063729809597135,\n",
       "  'significant_diff_ratio': 0.972716748714447,\n",
       "  'cosine_similarity': 1.0037996768951416,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_10_q_proj': {'frobenius_norm': 5.228488445281982,\n",
       "  'frobenius_norm_relative': 0.05129184073090293,\n",
       "  'spectral_norm': 5.228488445281982,\n",
       "  'spectral_norm_relative': 0.05129184073090293,\n",
       "  'mean_abs_difference': 0.0010138285579159856,\n",
       "  'max_abs_difference': 0.0076904296875,\n",
       "  'std_difference': 0.0012773124035447836,\n",
       "  'significant_diff_ratio': 0.9667128324508667,\n",
       "  'cosine_similarity': 1.0000810623168945,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_1_k_proj': {'frobenius_norm': 3.8670010566711426,\n",
       "  'frobenius_norm_relative': 0.036090438541867366,\n",
       "  'spectral_norm': 3.8670010566711426,\n",
       "  'spectral_norm_relative': 0.036090438541867366,\n",
       "  'mean_abs_difference': 0.000740374147426337,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0009452746598981321,\n",
       "  'significant_diff_ratio': 0.9603727459907532,\n",
       "  'cosine_similarity': 1.0010056495666504,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_6_gate_proj': {'frobenius_norm': 7.926518440246582,\n",
       "  'frobenius_norm_relative': 0.06016749490658261,\n",
       "  'spectral_norm': 7.926518440246582,\n",
       "  'spectral_norm_relative': 0.06016749490658261,\n",
       "  'mean_abs_difference': 0.000941154663451016,\n",
       "  'max_abs_difference': 0.0068359375,\n",
       "  'std_difference': 0.0011818904895335436,\n",
       "  'significant_diff_ratio': 0.9705483913421631,\n",
       "  'cosine_similarity': 1.0045710802078247,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_5_k_proj': {'frobenius_norm': 4.747844696044922,\n",
       "  'frobenius_norm_relative': 0.042555051637605416,\n",
       "  'spectral_norm': 4.747844696044922,\n",
       "  'spectral_norm_relative': 0.042555051637605416,\n",
       "  'mean_abs_difference': 0.0009188769618049264,\n",
       "  'max_abs_difference': 0.00732421875,\n",
       "  'std_difference': 0.0011603632010519505,\n",
       "  'significant_diff_ratio': 0.960820734500885,\n",
       "  'cosine_similarity': 1.000489354133606,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_9_down_proj': {'frobenius_norm': 7.857545852661133,\n",
       "  'frobenius_norm_relative': 0.06695896321238916,\n",
       "  'spectral_norm': 7.857545852661133,\n",
       "  'spectral_norm_relative': 0.06695896321238916,\n",
       "  'mean_abs_difference': 0.000932212162297219,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0011712838895618916,\n",
       "  'significant_diff_ratio': 0.9732651710510254,\n",
       "  'cosine_similarity': 1.004188060760498,\n",
       "  'weight_shape': torch.Size([4096, 11008]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_11_o_proj': {'frobenius_norm': 4.428964138031006,\n",
       "  'frobenius_norm_relative': 0.07354737572315648,\n",
       "  'spectral_norm': 4.428964138031006,\n",
       "  'spectral_norm_relative': 0.07354737572315648,\n",
       "  'mean_abs_difference': 0.0008569533238187432,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0010816043941304088,\n",
       "  'significant_diff_ratio': 0.9754784107208252,\n",
       "  'cosine_similarity': 0.9982882738113403,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_11_up_proj': {'frobenius_norm': 8.037643432617188,\n",
       "  'frobenius_norm_relative': 0.06670278987859041,\n",
       "  'spectral_norm': 8.037643432617188,\n",
       "  'spectral_norm_relative': 0.06670278987859041,\n",
       "  'mean_abs_difference': 0.0009530227980576456,\n",
       "  'max_abs_difference': 0.0075531005859375,\n",
       "  'std_difference': 0.0011981845600530505,\n",
       "  'significant_diff_ratio': 0.9730544090270996,\n",
       "  'cosine_similarity': 1.0039528608322144,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_0_down_proj': {'frobenius_norm': 7.330352783203125,\n",
       "  'frobenius_norm_relative': 0.06567294598936273,\n",
       "  'spectral_norm': 7.330352783203125,\n",
       "  'spectral_norm_relative': 0.06567294598936273,\n",
       "  'mean_abs_difference': 0.0008695260039530694,\n",
       "  'max_abs_difference': 0.006561279296875,\n",
       "  'std_difference': 0.001092742197215557,\n",
       "  'significant_diff_ratio': 0.9728407859802246,\n",
       "  'cosine_similarity': 1.0047093629837036,\n",
       "  'weight_shape': torch.Size([4096, 11008]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_6_o_proj': {'frobenius_norm': 4.4882025718688965,\n",
       "  'frobenius_norm_relative': 0.0819626140585392,\n",
       "  'spectral_norm': 4.4882025718688965,\n",
       "  'spectral_norm_relative': 0.0819626140585392,\n",
       "  'mean_abs_difference': 0.0008702456834726036,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0010958786588162184,\n",
       "  'significant_diff_ratio': 0.9779826402664185,\n",
       "  'cosine_similarity': 0.9978708028793335,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_20_q_proj': {'frobenius_norm': 5.3425421714782715,\n",
       "  'frobenius_norm_relative': 0.057834771485102326,\n",
       "  'spectral_norm': 5.3425421714782715,\n",
       "  'spectral_norm_relative': 0.057834771485102326,\n",
       "  'mean_abs_difference': 0.0010362555040046573,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0013049375265836716,\n",
       "  'significant_diff_ratio': 0.9701040387153625,\n",
       "  'cosine_similarity': 0.9996099472045898,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_24_down_proj': {'frobenius_norm': 8.58829402923584,\n",
       "  'frobenius_norm_relative': 0.06995147047641545,\n",
       "  'spectral_norm': 8.58829402923584,\n",
       "  'spectral_norm_relative': 0.06995147047641545,\n",
       "  'mean_abs_difference': 0.0010201759869232774,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0012803543359041214,\n",
       "  'significant_diff_ratio': 0.9743436574935913,\n",
       "  'cosine_similarity': 1.0034899711608887,\n",
       "  'weight_shape': torch.Size([4096, 11008]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_27_down_proj': {'frobenius_norm': 8.59900188446045,\n",
       "  'frobenius_norm_relative': 0.06911742890809366,\n",
       "  'spectral_norm': 8.59900188446045,\n",
       "  'spectral_norm_relative': 0.06911742890809366,\n",
       "  'mean_abs_difference': 0.0010194431524723768,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0012820209376513958,\n",
       "  'significant_diff_ratio': 0.9739643335342407,\n",
       "  'cosine_similarity': 1.0034382343292236,\n",
       "  'weight_shape': torch.Size([4096, 11008]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_1_gate_proj': {'frobenius_norm': 7.311300754547119,\n",
       "  'frobenius_norm_relative': 0.06043434170450168,\n",
       "  'spectral_norm': 7.311300754547119,\n",
       "  'spectral_norm_relative': 0.06043434170450168,\n",
       "  'mean_abs_difference': 0.0008682101033627987,\n",
       "  'max_abs_difference': 0.01171875,\n",
       "  'std_difference': 0.0010900963097810745,\n",
       "  'significant_diff_ratio': 0.9706401228904724,\n",
       "  'cosine_similarity': 1.0042961835861206,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_0_q_proj': {'frobenius_norm': 2.478200674057007,\n",
       "  'frobenius_norm_relative': 0.04529428611679507,\n",
       "  'spectral_norm': 2.478200674057007,\n",
       "  'spectral_norm_relative': 0.04529428611679507,\n",
       "  'mean_abs_difference': 0.00043242849642410874,\n",
       "  'max_abs_difference': 0.01171875,\n",
       "  'std_difference': 0.0006054203258827329,\n",
       "  'significant_diff_ratio': 0.9693558216094971,\n",
       "  'cosine_similarity': 1.0020240545272827,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_13_o_proj': {'frobenius_norm': 4.409083366394043,\n",
       "  'frobenius_norm_relative': 0.07154843116362551,\n",
       "  'spectral_norm': 4.409083366394043,\n",
       "  'spectral_norm_relative': 0.07154843116362551,\n",
       "  'mean_abs_difference': 0.0008541274000890553,\n",
       "  'max_abs_difference': 0.0074462890625,\n",
       "  'std_difference': 0.0010768012143671513,\n",
       "  'significant_diff_ratio': 0.9749303460121155,\n",
       "  'cosine_similarity': 0.9983645081520081,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_26_k_proj': {'frobenius_norm': 5.332941055297852,\n",
       "  'frobenius_norm_relative': 0.05988596209850361,\n",
       "  'spectral_norm': 5.332941055297852,\n",
       "  'spectral_norm_relative': 0.05988596209850361,\n",
       "  'mean_abs_difference': 0.0010347208008170128,\n",
       "  'max_abs_difference': 0.009765625,\n",
       "  'std_difference': 0.001302430871874094,\n",
       "  'significant_diff_ratio': 0.9711325764656067,\n",
       "  'cosine_similarity': 0.9996031522750854,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_4_q_proj': {'frobenius_norm': 4.761052131652832,\n",
       "  'frobenius_norm_relative': 0.04465244023290847,\n",
       "  'spectral_norm': 4.761052131652832,\n",
       "  'spectral_norm_relative': 0.04465244023290847,\n",
       "  'mean_abs_difference': 0.0009224803070537746,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0011635096743702888,\n",
       "  'significant_diff_ratio': 0.9624634385108948,\n",
       "  'cosine_similarity': 1.0003162622451782,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_5_gate_proj': {'frobenius_norm': 7.8381195068359375,\n",
       "  'frobenius_norm_relative': 0.06049924458235837,\n",
       "  'spectral_norm': 7.8381195068359375,\n",
       "  'spectral_norm_relative': 0.06049924458235837,\n",
       "  'mean_abs_difference': 0.0009307627333328128,\n",
       "  'max_abs_difference': 0.00688934326171875,\n",
       "  'std_difference': 0.001168685033917427,\n",
       "  'significant_diff_ratio': 0.9706669449806213,\n",
       "  'cosine_similarity': 1.004170298576355,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_28_o_proj': {'frobenius_norm': 5.024733543395996,\n",
       "  'frobenius_norm_relative': 0.06104172467120421,\n",
       "  'spectral_norm': 5.024733543395996,\n",
       "  'spectral_norm_relative': 0.06104172467120421,\n",
       "  'mean_abs_difference': 0.0009754984639585018,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0012272208696231246,\n",
       "  'significant_diff_ratio': 0.9707567691802979,\n",
       "  'cosine_similarity': 0.9993414878845215,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_29_q_proj': {'frobenius_norm': 5.417647838592529,\n",
       "  'frobenius_norm_relative': 0.06242447503051387,\n",
       "  'spectral_norm': 5.417647838592529,\n",
       "  'spectral_norm_relative': 0.06242447503051387,\n",
       "  'mean_abs_difference': 0.0010521032381802797,\n",
       "  'max_abs_difference': 0.01318359375,\n",
       "  'std_difference': 0.0013230439508333802,\n",
       "  'significant_diff_ratio': 0.9722159504890442,\n",
       "  'cosine_similarity': 0.9993367195129395,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_26_up_proj': {'frobenius_norm': 8.555595397949219,\n",
       "  'frobenius_norm_relative': 0.06882573699419178,\n",
       "  'spectral_norm': 8.555595397949219,\n",
       "  'spectral_norm_relative': 0.06882573699419178,\n",
       "  'mean_abs_difference': 0.001015764893963933,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.001275516115128994,\n",
       "  'significant_diff_ratio': 0.9739344120025635,\n",
       "  'cosine_similarity': 1.0034339427947998,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_27_gate_proj': {'frobenius_norm': 8.808809280395508,\n",
       "  'frobenius_norm_relative': 0.06587299436480276,\n",
       "  'spectral_norm': 8.808809280395508,\n",
       "  'spectral_norm_relative': 0.06587299436480276,\n",
       "  'mean_abs_difference': 0.0010461758356541395,\n",
       "  'max_abs_difference': 0.013671875,\n",
       "  'std_difference': 0.001313277636654675,\n",
       "  'significant_diff_ratio': 0.9730355739593506,\n",
       "  'cosine_similarity': 1.0044147968292236,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_24_gate_proj': {'frobenius_norm': 8.827963829040527,\n",
       "  'frobenius_norm_relative': 0.06640276596391705,\n",
       "  'spectral_norm': 8.827963829040527,\n",
       "  'spectral_norm_relative': 0.06640276596391705,\n",
       "  'mean_abs_difference': 0.001048817066475749,\n",
       "  'max_abs_difference': 0.00927734375,\n",
       "  'std_difference': 0.0013160945381969213,\n",
       "  'significant_diff_ratio': 0.9731314182281494,\n",
       "  'cosine_similarity': 1.0041980743408203,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_29_o_proj': {'frobenius_norm': 5.178296089172363,\n",
       "  'frobenius_norm_relative': 0.062164558900403497,\n",
       "  'spectral_norm': 5.178296089172363,\n",
       "  'spectral_norm_relative': 0.062164558900403497,\n",
       "  'mean_abs_difference': 0.0010051049757748842,\n",
       "  'max_abs_difference': 0.01953125,\n",
       "  'std_difference': 0.0012646657414734364,\n",
       "  'significant_diff_ratio': 0.9712818264961243,\n",
       "  'cosine_similarity': 0.9992499947547913,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_3_o_proj': {'frobenius_norm': 4.515036582946777,\n",
       "  'frobenius_norm_relative': 0.08347618774559598,\n",
       "  'spectral_norm': 4.515036582946777,\n",
       "  'spectral_norm_relative': 0.08347618774559598,\n",
       "  'mean_abs_difference': 0.0008771020220592618,\n",
       "  'max_abs_difference': 0.00595855712890625,\n",
       "  'std_difference': 0.0011023984989151359,\n",
       "  'significant_diff_ratio': 0.9786558151245117,\n",
       "  'cosine_similarity': 0.9977954030036926,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_16_k_proj': {'frobenius_norm': 5.430888652801514,\n",
       "  'frobenius_norm_relative': 0.055255770578686095,\n",
       "  'spectral_norm': 5.430888652801514,\n",
       "  'spectral_norm_relative': 0.055255770578686095,\n",
       "  'mean_abs_difference': 0.0010526154655963182,\n",
       "  'max_abs_difference': 0.00789642333984375,\n",
       "  'std_difference': 0.001326500321738422,\n",
       "  'significant_diff_ratio': 0.9693059325218201,\n",
       "  'cosine_similarity': 1.0000600814819336,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_28_up_proj': {'frobenius_norm': 8.573166847229004,\n",
       "  'frobenius_norm_relative': 0.0679579144936056,\n",
       "  'spectral_norm': 8.573166847229004,\n",
       "  'spectral_norm_relative': 0.0679579144936056,\n",
       "  'mean_abs_difference': 0.0010173505870625377,\n",
       "  'max_abs_difference': 0.01025390625,\n",
       "  'std_difference': 0.0012781458208337426,\n",
       "  'significant_diff_ratio': 0.973608136177063,\n",
       "  'cosine_similarity': 1.0033454895019531,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_9_gate_proj': {'frobenius_norm': 8.3024263381958,\n",
       "  'frobenius_norm_relative': 0.06466808343709984,\n",
       "  'spectral_norm': 8.3024263381958,\n",
       "  'spectral_norm_relative': 0.06466808343709984,\n",
       "  'mean_abs_difference': 0.0009854938834905624,\n",
       "  'max_abs_difference': 0.007568359375,\n",
       "  'std_difference': 0.0012378881219774485,\n",
       "  'significant_diff_ratio': 0.9726201295852661,\n",
       "  'cosine_similarity': 1.0037719011306763,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_20_gate_proj': {'frobenius_norm': 8.702253341674805,\n",
       "  'frobenius_norm_relative': 0.06641569879728096,\n",
       "  'spectral_norm': 8.702253341674805,\n",
       "  'spectral_norm_relative': 0.06641569879728096,\n",
       "  'mean_abs_difference': 0.0010334814433008432,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0012974096462130547,\n",
       "  'significant_diff_ratio': 0.973181962966919,\n",
       "  'cosine_similarity': 1.0040143728256226,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_28_post_attention_layernorm': {'frobenius_norm': 0.3042753338813782,\n",
       "  'frobenius_norm_relative': 0.01041382498939895,\n",
       "  'spectral_norm': 0.3042753338813782,\n",
       "  'spectral_norm_relative': 0.01041382498939895,\n",
       "  'mean_abs_difference': 0.004540622234344482,\n",
       "  'max_abs_difference': 0.009765625,\n",
       "  'std_difference': 0.0014182807644829154,\n",
       "  'significant_diff_ratio': 0.997802734375,\n",
       "  'cosine_similarity': 0.9999989867210388,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_14_gate_proj': {'frobenius_norm': 8.529053688049316,\n",
       "  'frobenius_norm_relative': 0.06768678330811323,\n",
       "  'spectral_norm': 8.529053688049316,\n",
       "  'spectral_norm_relative': 0.06768678330811323,\n",
       "  'mean_abs_difference': 0.001012250897474587,\n",
       "  'max_abs_difference': 0.00726318359375,\n",
       "  'std_difference': 0.0012716222554445267,\n",
       "  'significant_diff_ratio': 0.9738004803657532,\n",
       "  'cosine_similarity': 1.0035146474838257,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_17_q_proj': {'frobenius_norm': 5.312648773193359,\n",
       "  'frobenius_norm_relative': 0.055923305813844096,\n",
       "  'spectral_norm': 5.312648773193359,\n",
       "  'spectral_norm_relative': 0.055923305813844096,\n",
       "  'mean_abs_difference': 0.0010297062108293176,\n",
       "  'max_abs_difference': 0.00830078125,\n",
       "  'std_difference': 0.0012976530706509948,\n",
       "  'significant_diff_ratio': 0.9690259695053101,\n",
       "  'cosine_similarity': 0.9998317956924438,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_25_down_proj': {'frobenius_norm': 8.635223388671875,\n",
       "  'frobenius_norm_relative': 0.06998791791448096,\n",
       "  'spectral_norm': 8.635223388671875,\n",
       "  'spectral_norm_relative': 0.06998791791448096,\n",
       "  'mean_abs_difference': 0.0010255755623802543,\n",
       "  'max_abs_difference': 0.015625,\n",
       "  'std_difference': 0.0012873420491814613,\n",
       "  'significant_diff_ratio': 0.9743975400924683,\n",
       "  'cosine_similarity': 1.003444790840149,\n",
       "  'weight_shape': torch.Size([4096, 11008]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_15_input_layernorm': {'frobenius_norm': 0.2879183292388916,\n",
       "  'frobenius_norm_relative': 0.011583339670334331,\n",
       "  'spectral_norm': 0.2879183292388916,\n",
       "  'spectral_norm_relative': 0.011583339670334331,\n",
       "  'mean_abs_difference': 0.004293352365493774,\n",
       "  'max_abs_difference': 0.01171875,\n",
       "  'std_difference': 0.0013439058093354106,\n",
       "  'significant_diff_ratio': 0.9970703125,\n",
       "  'cosine_similarity': 0.9999963045120239,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_1_v_proj': {'frobenius_norm': 3.9947245121002197,\n",
       "  'frobenius_norm_relative': 0.0976634902037834,\n",
       "  'spectral_norm': 3.9947245121002197,\n",
       "  'spectral_norm_relative': 0.0976634902037834,\n",
       "  'mean_abs_difference': 0.000772513507399708,\n",
       "  'max_abs_difference': 0.006591796875,\n",
       "  'std_difference': 0.0009754625498317182,\n",
       "  'significant_diff_ratio': 0.9829115867614746,\n",
       "  'cosine_similarity': 0.9964909553527832,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_7_post_attention_layernorm': {'frobenius_norm': 0.13879646360874176,\n",
       "  'frobenius_norm_relative': 0.009768544323890186,\n",
       "  'spectral_norm': 0.13879646360874176,\n",
       "  'spectral_norm_relative': 0.009768544323890186,\n",
       "  'mean_abs_difference': 0.0019086599349975586,\n",
       "  'max_abs_difference': 0.0048828125,\n",
       "  'std_difference': 0.0010703192092478275,\n",
       "  'significant_diff_ratio': 0.92724609375,\n",
       "  'cosine_similarity': 0.9999918341636658,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_27_v_proj': {'frobenius_norm': 4.876420021057129,\n",
       "  'frobenius_norm_relative': 0.06083455883710045,\n",
       "  'spectral_norm': 4.876420021057129,\n",
       "  'spectral_norm_relative': 0.06083455883710045,\n",
       "  'mean_abs_difference': 0.0009474256657995284,\n",
       "  'max_abs_difference': 0.006500244140625,\n",
       "  'std_difference': 0.0011910725152119994,\n",
       "  'significant_diff_ratio': 0.9706944823265076,\n",
       "  'cosine_similarity': 0.9992872476577759,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_0_post_attention_layernorm': {'frobenius_norm': 0.06904205679893494,\n",
       "  'frobenius_norm_relative': 0.019547686401994592,\n",
       "  'spectral_norm': 0.06904205679893494,\n",
       "  'spectral_norm_relative': 0.019547686401994592,\n",
       "  'mean_abs_difference': 0.0008602877496741712,\n",
       "  'max_abs_difference': 0.00439453125,\n",
       "  'std_difference': 0.0009471190278418362,\n",
       "  'significant_diff_ratio': 0.898681640625,\n",
       "  'cosine_similarity': 0.999854564666748,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_14_v_proj': {'frobenius_norm': 4.285957336425781,\n",
       "  'frobenius_norm_relative': 0.07013915424705598,\n",
       "  'spectral_norm': 4.285957336425781,\n",
       "  'spectral_norm_relative': 0.07013915424705598,\n",
       "  'mean_abs_difference': 0.0008306287927553058,\n",
       "  'max_abs_difference': 0.005889892578125,\n",
       "  'std_difference': 0.0010468580294400454,\n",
       "  'significant_diff_ratio': 0.9744570851325989,\n",
       "  'cosine_similarity': 0.9984819889068604,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_7_o_proj': {'frobenius_norm': 4.47178840637207,\n",
       "  'frobenius_norm_relative': 0.08103328782975419,\n",
       "  'spectral_norm': 4.47178840637207,\n",
       "  'spectral_norm_relative': 0.08103328782975419,\n",
       "  'mean_abs_difference': 0.0008664465858601034,\n",
       "  'max_abs_difference': 0.006103515625,\n",
       "  'std_difference': 0.0010918958578258753,\n",
       "  'significant_diff_ratio': 0.9777671098709106,\n",
       "  'cosine_similarity': 0.9979553818702698,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_29_v_proj': {'frobenius_norm': 4.954885482788086,\n",
       "  'frobenius_norm_relative': 0.05968456670407185,\n",
       "  'spectral_norm': 4.954885482788086,\n",
       "  'spectral_norm_relative': 0.05968456670407185,\n",
       "  'mean_abs_difference': 0.0009610253619030118,\n",
       "  'max_abs_difference': 0.007568359375,\n",
       "  'std_difference': 0.0012102395994588733,\n",
       "  'significant_diff_ratio': 0.9701020121574402,\n",
       "  'cosine_similarity': 0.9994542598724365,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_27_k_proj': {'frobenius_norm': 5.2194647789001465,\n",
       "  'frobenius_norm_relative': 0.056404255125249325,\n",
       "  'spectral_norm': 5.2194647789001465,\n",
       "  'spectral_norm_relative': 0.056404255125249325,\n",
       "  'mean_abs_difference': 0.00101311388425529,\n",
       "  'max_abs_difference': 0.0086212158203125,\n",
       "  'std_difference': 0.0012749333400279284,\n",
       "  'significant_diff_ratio': 0.9690226316452026,\n",
       "  'cosine_similarity': 0.9996358752250671,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_8_k_proj': {'frobenius_norm': 5.111876487731934,\n",
       "  'frobenius_norm_relative': 0.049849899489023006,\n",
       "  'spectral_norm': 5.111876487731934,\n",
       "  'spectral_norm_relative': 0.049849899489023006,\n",
       "  'mean_abs_difference': 0.0009909870568662882,\n",
       "  'max_abs_difference': 0.00732421875,\n",
       "  'std_difference': 0.0012488323263823986,\n",
       "  'significant_diff_ratio': 0.966234564781189,\n",
       "  'cosine_similarity': 1.000373125076294,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_10_gate_proj': {'frobenius_norm': 8.398812294006348,\n",
       "  'frobenius_norm_relative': 0.06561947191194913,\n",
       "  'spectral_norm': 8.398812294006348,\n",
       "  'spectral_norm_relative': 0.06561947191194913,\n",
       "  'mean_abs_difference': 0.0009967676596716046,\n",
       "  'max_abs_difference': 0.0074710845947265625,\n",
       "  'std_difference': 0.0012522529577836394,\n",
       "  'significant_diff_ratio': 0.9731269478797913,\n",
       "  'cosine_similarity': 1.0037037134170532,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_4_v_proj': {'frobenius_norm': 4.426141262054443,\n",
       "  'frobenius_norm_relative': 0.075854659575615,\n",
       "  'spectral_norm': 4.426141262054443,\n",
       "  'spectral_norm_relative': 0.075854659575615,\n",
       "  'mean_abs_difference': 0.0008593168458901346,\n",
       "  'max_abs_difference': 0.0062255859375,\n",
       "  'std_difference': 0.0010808942606672645,\n",
       "  'significant_diff_ratio': 0.9765108227729797,\n",
       "  'cosine_similarity': 0.9981894493103027,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_19_post_attention_layernorm': {'frobenius_norm': 0.17723466455936432,\n",
       "  'frobenius_norm_relative': 0.008027783878626463,\n",
       "  'spectral_norm': 0.17723466455936432,\n",
       "  'spectral_norm_relative': 0.008027783878626463,\n",
       "  'mean_abs_difference': 0.0024023056030273438,\n",
       "  'max_abs_difference': 0.005859375,\n",
       "  'std_difference': 0.0014057089574635029,\n",
       "  'significant_diff_ratio': 0.86328125,\n",
       "  'cosine_similarity': 0.999991774559021,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_16_o_proj': {'frobenius_norm': 4.5589375495910645,\n",
       "  'frobenius_norm_relative': 0.06956888442199269,\n",
       "  'spectral_norm': 4.5589375495910645,\n",
       "  'spectral_norm_relative': 0.06956888442199269,\n",
       "  'mean_abs_difference': 0.0008826879784464836,\n",
       "  'max_abs_difference': 0.0068359375,\n",
       "  'std_difference': 0.0011133627267554402,\n",
       "  'significant_diff_ratio': 0.9740788340568542,\n",
       "  'cosine_similarity': 0.9985560774803162,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_15_q_proj': {'frobenius_norm': 5.316622257232666,\n",
       "  'frobenius_norm_relative': 0.05511993738158327,\n",
       "  'spectral_norm': 5.316622257232666,\n",
       "  'spectral_norm_relative': 0.05511993738158327,\n",
       "  'mean_abs_difference': 0.0010304109891876578,\n",
       "  'max_abs_difference': 0.00830078125,\n",
       "  'std_difference': 0.001298686140216887,\n",
       "  'significant_diff_ratio': 0.9687067270278931,\n",
       "  'cosine_similarity': 0.9998506307601929,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_24_v_proj': {'frobenius_norm': 4.964504718780518,\n",
       "  'frobenius_norm_relative': 0.06541390451019384,\n",
       "  'spectral_norm': 4.964504718780518,\n",
       "  'spectral_norm_relative': 0.06541390451019384,\n",
       "  'mean_abs_difference': 0.0009646923863328993,\n",
       "  'max_abs_difference': 0.0068359375,\n",
       "  'std_difference': 0.001212327741086483,\n",
       "  'significant_diff_ratio': 0.9727107286453247,\n",
       "  'cosine_similarity': 0.9992544651031494,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_25_up_proj': {'frobenius_norm': 8.57492733001709,\n",
       "  'frobenius_norm_relative': 0.06922494869558873,\n",
       "  'spectral_norm': 8.57492733001709,\n",
       "  'spectral_norm_relative': 0.06922494869558873,\n",
       "  'mean_abs_difference': 0.0010182401165366173,\n",
       "  'max_abs_difference': 0.0076751708984375,\n",
       "  'std_difference': 0.0012783874990418553,\n",
       "  'significant_diff_ratio': 0.9740738868713379,\n",
       "  'cosine_similarity': 1.0034421682357788,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_31_v_proj': {'frobenius_norm': 4.631101131439209,\n",
       "  'frobenius_norm_relative': 0.059758749746542106,\n",
       "  'spectral_norm': 4.631101131439209,\n",
       "  'spectral_norm_relative': 0.059758749746542106,\n",
       "  'mean_abs_difference': 0.0008942301501519978,\n",
       "  'max_abs_difference': 0.00789642333984375,\n",
       "  'std_difference': 0.0011312909191474319,\n",
       "  'significant_diff_ratio': 0.9698655009269714,\n",
       "  'cosine_similarity': 0.9993642568588257,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_4_post_attention_layernorm': {'frobenius_norm': 0.13436259329319,\n",
       "  'frobenius_norm_relative': 0.011349244036208662,\n",
       "  'spectral_norm': 0.13436259329319,\n",
       "  'spectral_norm_relative': 0.011349244036208662,\n",
       "  'mean_abs_difference': 0.0018381178379058838,\n",
       "  'max_abs_difference': 0.0048828125,\n",
       "  'std_difference': 0.001046623452566564,\n",
       "  'significant_diff_ratio': 0.914794921875,\n",
       "  'cosine_similarity': 0.99998539686203,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_17_v_proj': {'frobenius_norm': 4.448672294616699,\n",
       "  'frobenius_norm_relative': 0.06742298688338008,\n",
       "  'spectral_norm': 4.448672294616699,\n",
       "  'spectral_norm_relative': 0.06742298688338008,\n",
       "  'mean_abs_difference': 0.0008623851463198662,\n",
       "  'max_abs_difference': 0.00628662109375,\n",
       "  'std_difference': 0.0010865428484976292,\n",
       "  'significant_diff_ratio': 0.9734436273574829,\n",
       "  'cosine_similarity': 0.9987910389900208,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_24_post_attention_layernorm': {'frobenius_norm': 0.23234659433364868,\n",
       "  'frobenius_norm_relative': 0.008882295892381476,\n",
       "  'spectral_norm': 0.23234659433364868,\n",
       "  'spectral_norm_relative': 0.008882295892381476,\n",
       "  'mean_abs_difference': 0.003332793712615967,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0014449930749833584,\n",
       "  'significant_diff_ratio': 0.965576171875,\n",
       "  'cosine_similarity': 0.9999967813491821,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_2_input_layernorm': {'frobenius_norm': 0.12187516689300537,\n",
       "  'frobenius_norm_relative': 0.010813652281523412,\n",
       "  'spectral_norm': 0.12187516689300537,\n",
       "  'spectral_norm_relative': 0.010813652281523412,\n",
       "  'mean_abs_difference': 0.0016280289273709059,\n",
       "  'max_abs_difference': 0.0048828125,\n",
       "  'std_difference': 0.0010340465232729912,\n",
       "  'significant_diff_ratio': 0.88330078125,\n",
       "  'cosine_similarity': 0.9999831914901733,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_14_post_attention_layernorm': {'frobenius_norm': 0.14219404757022858,\n",
       "  'frobenius_norm_relative': 0.00826507629826469,\n",
       "  'spectral_norm': 0.14219404757022858,\n",
       "  'spectral_norm_relative': 0.00826507629826469,\n",
       "  'mean_abs_difference': 0.0018096566200256348,\n",
       "  'max_abs_difference': 0.005859375,\n",
       "  'std_difference': 0.001361010828986764,\n",
       "  'significant_diff_ratio': 0.7490234375,\n",
       "  'cosine_similarity': 0.9999873638153076,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_14_k_proj': {'frobenius_norm': 5.416794300079346,\n",
       "  'frobenius_norm_relative': 0.05443003538149676,\n",
       "  'spectral_norm': 5.416794300079346,\n",
       "  'spectral_norm_relative': 0.05443003538149676,\n",
       "  'mean_abs_difference': 0.0010505927493795753,\n",
       "  'max_abs_difference': 0.00732421875,\n",
       "  'std_difference': 0.0013230841141194105,\n",
       "  'significant_diff_ratio': 0.9687387347221375,\n",
       "  'cosine_similarity': 1.0001171827316284,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_12_input_layernorm': {'frobenius_norm': 0.2822675406932831,\n",
       "  'frobenius_norm_relative': 0.011543106690251234,\n",
       "  'spectral_norm': 0.2822675406932831,\n",
       "  'spectral_norm_relative': 0.011543106690251234,\n",
       "  'mean_abs_difference': 0.004198194481432438,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0013518526684492826,\n",
       "  'significant_diff_ratio': 0.997802734375,\n",
       "  'cosine_similarity': 0.9999958276748657,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_21_up_proj': {'frobenius_norm': 8.44128704071045,\n",
       "  'frobenius_norm_relative': 0.0689499395574666,\n",
       "  'spectral_norm': 8.44128704071045,\n",
       "  'spectral_norm_relative': 0.0689499395574666,\n",
       "  'mean_abs_difference': 0.0010019522160291672,\n",
       "  'max_abs_difference': 0.009765625,\n",
       "  'std_difference': 0.0012584526557475328,\n",
       "  'significant_diff_ratio': 0.9739883542060852,\n",
       "  'cosine_similarity': 1.0035779476165771,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_26_v_proj': {'frobenius_norm': 4.972836494445801,\n",
       "  'frobenius_norm_relative': 0.06215874893946559,\n",
       "  'spectral_norm': 4.972836494445801,\n",
       "  'spectral_norm_relative': 0.06215874893946559,\n",
       "  'mean_abs_difference': 0.0009658053750172257,\n",
       "  'max_abs_difference': 0.0068359375,\n",
       "  'std_difference': 0.0012144986540079117,\n",
       "  'significant_diff_ratio': 0.9713771343231201,\n",
       "  'cosine_similarity': 0.9994522333145142,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_20_post_attention_layernorm': {'frobenius_norm': 0.1962137371301651,\n",
       "  'frobenius_norm_relative': 0.008578339949652805,\n",
       "  'spectral_norm': 0.1962137371301651,\n",
       "  'spectral_norm_relative': 0.008578339949652805,\n",
       "  'mean_abs_difference': 0.0027413368225097656,\n",
       "  'max_abs_difference': 0.0068359375,\n",
       "  'std_difference': 0.0013899286277592182,\n",
       "  'significant_diff_ratio': 0.92041015625,\n",
       "  'cosine_similarity': 0.999992847442627,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_14_input_layernorm': {'frobenius_norm': 0.30667370557785034,\n",
       "  'frobenius_norm_relative': 0.012040701862372771,\n",
       "  'spectral_norm': 0.30667370557785034,\n",
       "  'spectral_norm_relative': 0.012040701862372771,\n",
       "  'mean_abs_difference': 0.0045935227535665035,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.001364230178296566,\n",
       "  'significant_diff_ratio': 0.9990234375,\n",
       "  'cosine_similarity': 0.9999969601631165,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_7_down_proj': {'frobenius_norm': 7.75740909576416,\n",
       "  'frobenius_norm_relative': 0.0672678551427072,\n",
       "  'spectral_norm': 7.75740909576416,\n",
       "  'spectral_norm_relative': 0.0672678551427072,\n",
       "  'mean_abs_difference': 0.000920667196623981,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0011563372099772096,\n",
       "  'significant_diff_ratio': 0.973381757736206,\n",
       "  'cosine_similarity': 1.004298448562622,\n",
       "  'weight_shape': torch.Size([4096, 11008]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_26_post_attention_layernorm': {'frobenius_norm': 0.27005064487457275,\n",
       "  'frobenius_norm_relative': 0.009713736340225,\n",
       "  'spectral_norm': 0.27005064487457275,\n",
       "  'spectral_norm_relative': 0.009713736340225,\n",
       "  'mean_abs_difference': 0.003977775573730469,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.001420350163243711,\n",
       "  'significant_diff_ratio': 0.991455078125,\n",
       "  'cosine_similarity': 0.9999982714653015,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_25_o_proj': {'frobenius_norm': 5.151584625244141,\n",
       "  'frobenius_norm_relative': 0.06635441510434069,\n",
       "  'spectral_norm': 5.151584625244141,\n",
       "  'spectral_norm_relative': 0.06635441510434069,\n",
       "  'mean_abs_difference': 0.0010011799167841673,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0012579842004925013,\n",
       "  'significant_diff_ratio': 0.973063051700592,\n",
       "  'cosine_similarity': 0.9990505576133728,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_16_gate_proj': {'frobenius_norm': 8.571089744567871,\n",
       "  'frobenius_norm_relative': 0.06719484905420826,\n",
       "  'spectral_norm': 8.571089744567871,\n",
       "  'spectral_norm_relative': 0.06719484905420826,\n",
       "  'mean_abs_difference': 0.0010173255577683449,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0012779023963958025,\n",
       "  'significant_diff_ratio': 0.9736019372940063,\n",
       "  'cosine_similarity': 1.003460168838501,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_7_k_proj': {'frobenius_norm': 5.020422458648682,\n",
       "  'frobenius_norm_relative': 0.04948270623522803,\n",
       "  'spectral_norm': 5.020422458648682,\n",
       "  'spectral_norm_relative': 0.04948270623522803,\n",
       "  'mean_abs_difference': 0.0009731253958307207,\n",
       "  'max_abs_difference': 0.00714111328125,\n",
       "  'std_difference': 0.0012265217956155539,\n",
       "  'significant_diff_ratio': 0.9661608934402466,\n",
       "  'cosine_similarity': 1.0002515316009521,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_30_input_layernorm': {'frobenius_norm': 0.3503587543964386,\n",
       "  'frobenius_norm_relative': 0.009797934352349177,\n",
       "  'spectral_norm': 0.3503587543964386,\n",
       "  'spectral_norm_relative': 0.009797934352349177,\n",
       "  'mean_abs_difference': 0.0050654723308980465,\n",
       "  'max_abs_difference': 0.01171875,\n",
       "  'std_difference': 0.0020763527136296034,\n",
       "  'significant_diff_ratio': 0.96484375,\n",
       "  'cosine_similarity': 0.9999931454658508,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_4_down_proj': {'frobenius_norm': 7.636986255645752,\n",
       "  'frobenius_norm_relative': 0.0660218432795391,\n",
       "  'spectral_norm': 7.636986255645752,\n",
       "  'spectral_norm_relative': 0.0660218432795391,\n",
       "  'mean_abs_difference': 0.0009064741898328066,\n",
       "  'max_abs_difference': 0.01171875,\n",
       "  'std_difference': 0.001138423103839159,\n",
       "  'significant_diff_ratio': 0.9728655219078064,\n",
       "  'cosine_similarity': 1.0043408870697021,\n",
       "  'weight_shape': torch.Size([4096, 11008]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_16_v_proj': {'frobenius_norm': 4.459264755249023,\n",
       "  'frobenius_norm_relative': 0.06735369807368205,\n",
       "  'spectral_norm': 4.459264755249023,\n",
       "  'spectral_norm_relative': 0.06735369807368205,\n",
       "  'mean_abs_difference': 0.0008642047178000212,\n",
       "  'max_abs_difference': 0.00689697265625,\n",
       "  'std_difference': 0.0010891611455008388,\n",
       "  'significant_diff_ratio': 0.9734370708465576,\n",
       "  'cosine_similarity': 0.99866783618927,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_29_k_proj': {'frobenius_norm': 5.408339023590088,\n",
       "  'frobenius_norm_relative': 0.061780537637739774,\n",
       "  'spectral_norm': 5.408339023590088,\n",
       "  'spectral_norm_relative': 0.061780537637739774,\n",
       "  'mean_abs_difference': 0.0010503368685021996,\n",
       "  'max_abs_difference': 0.00954437255859375,\n",
       "  'std_difference': 0.001320775132626295,\n",
       "  'significant_diff_ratio': 0.9719750285148621,\n",
       "  'cosine_similarity': 0.9993736147880554,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_5_v_proj': {'frobenius_norm': 4.350338459014893,\n",
       "  'frobenius_norm_relative': 0.07268901985842877,\n",
       "  'spectral_norm': 4.350338459014893,\n",
       "  'spectral_norm_relative': 0.07268901985842877,\n",
       "  'mean_abs_difference': 0.0008440296514891088,\n",
       "  'max_abs_difference': 0.006092071533203125,\n",
       "  'std_difference': 0.0010624458082020283,\n",
       "  'significant_diff_ratio': 0.9755011796951294,\n",
       "  'cosine_similarity': 0.9983773827552795,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_17_k_proj': {'frobenius_norm': 5.334137439727783,\n",
       "  'frobenius_norm_relative': 0.054930137429767975,\n",
       "  'spectral_norm': 5.334137439727783,\n",
       "  'spectral_norm_relative': 0.054930137429767975,\n",
       "  'mean_abs_difference': 0.0010334529215469956,\n",
       "  'max_abs_difference': 0.00762939453125,\n",
       "  'std_difference': 0.0013029290130361915,\n",
       "  'significant_diff_ratio': 0.9687454700469971,\n",
       "  'cosine_similarity': 0.9999521970748901,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_18_k_proj': {'frobenius_norm': 5.291459560394287,\n",
       "  'frobenius_norm_relative': 0.055730429645839324,\n",
       "  'spectral_norm': 5.291459560394287,\n",
       "  'spectral_norm_relative': 0.055730429645839324,\n",
       "  'mean_abs_difference': 0.0010249504121020436,\n",
       "  'max_abs_difference': 0.007659912109375,\n",
       "  'std_difference': 0.0012925165938213468,\n",
       "  'significant_diff_ratio': 0.9688745141029358,\n",
       "  'cosine_similarity': 0.9998249411582947,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_6_up_proj': {'frobenius_norm': 7.640622615814209,\n",
       "  'frobenius_norm_relative': 0.06611216395878375,\n",
       "  'spectral_norm': 7.640622615814209,\n",
       "  'spectral_norm_relative': 0.06611216395878375,\n",
       "  'mean_abs_difference': 0.0009067205828614533,\n",
       "  'max_abs_difference': 0.0068359375,\n",
       "  'std_difference': 0.0011389772407710552,\n",
       "  'significant_diff_ratio': 0.9729219675064087,\n",
       "  'cosine_similarity': 1.0043625831604004,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_3_down_proj': {'frobenius_norm': 7.577291488647461,\n",
       "  'frobenius_norm_relative': 0.0647440627417357,\n",
       "  'spectral_norm': 7.577291488647461,\n",
       "  'spectral_norm_relative': 0.0647440627417357,\n",
       "  'mean_abs_difference': 0.0008998644771054387,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0011295769363641739,\n",
       "  'significant_diff_ratio': 0.9723770022392273,\n",
       "  'cosine_similarity': 1.0043100118637085,\n",
       "  'weight_shape': torch.Size([4096, 11008]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_5_input_layernorm': {'frobenius_norm': 0.1740250289440155,\n",
       "  'frobenius_norm_relative': 0.010161847419590505,\n",
       "  'spectral_norm': 0.1740250289440155,\n",
       "  'spectral_norm_relative': 0.010161847419590505,\n",
       "  'mean_abs_difference': 0.002398568904027343,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0012908573262393475,\n",
       "  'significant_diff_ratio': 0.891357421875,\n",
       "  'cosine_similarity': 0.9999886751174927,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_7_q_proj': {'frobenius_norm': 5.00929069519043,\n",
       "  'frobenius_norm_relative': 0.049606009466210355,\n",
       "  'spectral_norm': 5.00929069519043,\n",
       "  'spectral_norm_relative': 0.049606009466210355,\n",
       "  'mean_abs_difference': 0.0009708308498375118,\n",
       "  'max_abs_difference': 0.007080078125,\n",
       "  'std_difference': 0.0012238039635121822,\n",
       "  'significant_diff_ratio': 0.9661020040512085,\n",
       "  'cosine_similarity': 1.0001767873764038,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_14_down_proj': {'frobenius_norm': 8.035466194152832,\n",
       "  'frobenius_norm_relative': 0.06628801968347714,\n",
       "  'spectral_norm': 8.035466194152832,\n",
       "  'spectral_norm_relative': 0.06628801968347714,\n",
       "  'mean_abs_difference': 0.0009529832168482244,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0011978618567809463,\n",
       "  'significant_diff_ratio': 0.9729366898536682,\n",
       "  'cosine_similarity': 1.0039082765579224,\n",
       "  'weight_shape': torch.Size([4096, 11008]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_28_v_proj': {'frobenius_norm': 4.937997817993164,\n",
       "  'frobenius_norm_relative': 0.05972126862045856,\n",
       "  'spectral_norm': 4.937997817993164,\n",
       "  'spectral_norm_relative': 0.05972126862045856,\n",
       "  'mean_abs_difference': 0.0009586571832187474,\n",
       "  'max_abs_difference': 0.0069732666015625,\n",
       "  'std_difference': 0.001206120359711349,\n",
       "  'significant_diff_ratio': 0.9701546430587769,\n",
       "  'cosine_similarity': 0.9993994235992432,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_17_up_proj': {'frobenius_norm': 8.256293296813965,\n",
       "  'frobenius_norm_relative': 0.06708535831406712,\n",
       "  'spectral_norm': 8.256293296813965,\n",
       "  'spectral_norm_relative': 0.06708535831406712,\n",
       "  'mean_abs_difference': 0.0009796591475605965,\n",
       "  'max_abs_difference': 0.00927734375,\n",
       "  'std_difference': 0.0012308789882808924,\n",
       "  'significant_diff_ratio': 0.973271906375885,\n",
       "  'cosine_similarity': 1.0036778450012207,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_7_input_layernorm': {'frobenius_norm': 0.20509512722492218,\n",
       "  'frobenius_norm_relative': 0.009349520847390927,\n",
       "  'spectral_norm': 0.20509512722492218,\n",
       "  'spectral_norm_relative': 0.009349520847390927,\n",
       "  'mean_abs_difference': 0.0029208073392510414,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0013231367338448763,\n",
       "  'significant_diff_ratio': 0.9501953125,\n",
       "  'cosine_similarity': 0.9999926686286926,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_28_input_layernorm': {'frobenius_norm': 0.3702763319015503,\n",
       "  'frobenius_norm_relative': 0.010518919637492987,\n",
       "  'spectral_norm': 0.3702763319015503,\n",
       "  'spectral_norm_relative': 0.010518919637492987,\n",
       "  'mean_abs_difference': 0.005391884595155716,\n",
       "  'max_abs_difference': 0.01171875,\n",
       "  'std_difference': 0.0020979628898203373,\n",
       "  'significant_diff_ratio': 0.977294921875,\n",
       "  'cosine_similarity': 0.9999929666519165,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_6_k_proj': {'frobenius_norm': 4.999655723571777,\n",
       "  'frobenius_norm_relative': 0.04846495563293143,\n",
       "  'spectral_norm': 4.999655723571777,\n",
       "  'spectral_norm_relative': 0.04846495563293143,\n",
       "  'mean_abs_difference': 0.0009691443992778659,\n",
       "  'max_abs_difference': 0.00738525390625,\n",
       "  'std_difference': 0.0012214655289426446,\n",
       "  'significant_diff_ratio': 0.9655052423477173,\n",
       "  'cosine_similarity': 1.0003392696380615,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_10_up_proj': {'frobenius_norm': 7.992169380187988,\n",
       "  'frobenius_norm_relative': 0.06689061402796855,\n",
       "  'spectral_norm': 7.992169380187988,\n",
       "  'spectral_norm_relative': 0.06689061402796855,\n",
       "  'mean_abs_difference': 0.0009477834100835025,\n",
       "  'max_abs_difference': 0.00665283203125,\n",
       "  'std_difference': 0.0011913508642464876,\n",
       "  'significant_diff_ratio': 0.9731636643409729,\n",
       "  'cosine_similarity': 1.0040122270584106,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_24_q_proj': {'frobenius_norm': 5.339209079742432,\n",
       "  'frobenius_norm_relative': 0.0599680470560544,\n",
       "  'spectral_norm': 5.339209079742432,\n",
       "  'spectral_norm_relative': 0.0599680470560544,\n",
       "  'mean_abs_difference': 0.0010366415372118354,\n",
       "  'max_abs_difference': 0.0087890625,\n",
       "  'std_difference': 0.0013040164485573769,\n",
       "  'significant_diff_ratio': 0.9713507294654846,\n",
       "  'cosine_similarity': 0.9992771148681641,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_30_k_proj': {'frobenius_norm': 5.332021713256836,\n",
       "  'frobenius_norm_relative': 0.060236614097658896,\n",
       "  'spectral_norm': 5.332021713256836,\n",
       "  'spectral_norm_relative': 0.060236614097658896,\n",
       "  'mean_abs_difference': 0.001034602290019393,\n",
       "  'max_abs_difference': 0.0107421875,\n",
       "  'std_difference': 0.0013022474013268948,\n",
       "  'significant_diff_ratio': 0.9710012674331665,\n",
       "  'cosine_similarity': 0.9992855787277222,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_23_post_attention_layernorm': {'frobenius_norm': 0.21492044627666473,\n",
       "  'frobenius_norm_relative': 0.008484464336143124,\n",
       "  'spectral_norm': 0.21492044627666473,\n",
       "  'spectral_norm_relative': 0.008484464336143124,\n",
       "  'mean_abs_difference': 0.003056943416595459,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0013998475624248385,\n",
       "  'significant_diff_ratio': 0.947509765625,\n",
       "  'cosine_similarity': 0.9999964833259583,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_24_o_proj': {'frobenius_norm': 5.148160457611084,\n",
       "  'frobenius_norm_relative': 0.06902081871906426,\n",
       "  'spectral_norm': 5.148160457611084,\n",
       "  'spectral_norm_relative': 0.06902081871906426,\n",
       "  'mean_abs_difference': 0.0009990442777052522,\n",
       "  'max_abs_difference': 0.01171875,\n",
       "  'std_difference': 0.0012570625403895974,\n",
       "  'significant_diff_ratio': 0.9740133285522461,\n",
       "  'cosine_similarity': 0.9989004731178284,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_22_input_layernorm': {'frobenius_norm': 0.3001353144645691,\n",
       "  'frobenius_norm_relative': 0.00990765150755374,\n",
       "  'spectral_norm': 0.3001353144645691,\n",
       "  'spectral_norm_relative': 0.00990765150755374,\n",
       "  'mean_abs_difference': 0.00447250297293067,\n",
       "  'max_abs_difference': 0.01171875,\n",
       "  'std_difference': 0.0014106185408309102,\n",
       "  'significant_diff_ratio': 0.997314453125,\n",
       "  'cosine_similarity': 0.9999991059303284,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'embed_tokens': {'frobenius_norm': 10.981884956359863,\n",
       "  'frobenius_norm_relative': 0.057915589966475875,\n",
       "  'spectral_norm': 10.981884956359863,\n",
       "  'spectral_norm_relative': 0.057915589966475875,\n",
       "  'mean_abs_difference': 0.0007397193694487214,\n",
       "  'max_abs_difference': 0.006591796875,\n",
       "  'std_difference': 0.0009699251968413591,\n",
       "  'significant_diff_ratio': 0.961707592010498,\n",
       "  'cosine_similarity': 1.0292723178863525,\n",
       "  'weight_shape': torch.Size([32000, 4096]),\n",
       "  'total_parameters': 131072000},\n",
       " 'layer_9_o_proj': {'frobenius_norm': 4.373073101043701,\n",
       "  'frobenius_norm_relative': 0.07525901380026284,\n",
       "  'spectral_norm': 4.373073101043701,\n",
       "  'spectral_norm_relative': 0.07525901380026284,\n",
       "  'mean_abs_difference': 0.0008473646594211459,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0010679556289687753,\n",
       "  'significant_diff_ratio': 0.9760482907295227,\n",
       "  'cosine_similarity': 0.9982420802116394,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_15_up_proj': {'frobenius_norm': 8.154401779174805,\n",
       "  'frobenius_norm_relative': 0.06595016815590297,\n",
       "  'spectral_norm': 8.154401779174805,\n",
       "  'spectral_norm_relative': 0.06595016815590297,\n",
       "  'mean_abs_difference': 0.0009672019514255226,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.00121569714974612,\n",
       "  'significant_diff_ratio': 0.9728357195854187,\n",
       "  'cosine_similarity': 1.0037199258804321,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'lm_head': {'frobenius_norm': 23.344120025634766,\n",
       "  'frobenius_norm_relative': 0.12358925555308098,\n",
       "  'spectral_norm': 23.344120025634766,\n",
       "  'spectral_norm_relative': 0.12358925555308098,\n",
       "  'mean_abs_difference': 0.001527020474895835,\n",
       "  'max_abs_difference': 0.04156494140625,\n",
       "  'std_difference': 0.002061779610812664,\n",
       "  'significant_diff_ratio': 0.9831951856613159,\n",
       "  'cosine_similarity': 1.0257526636123657,\n",
       "  'weight_shape': torch.Size([32000, 4096]),\n",
       "  'total_parameters': 131072000},\n",
       " 'layer_21_post_attention_layernorm': {'frobenius_norm': 0.20223864912986755,\n",
       "  'frobenius_norm_relative': 0.008538323896268433,\n",
       "  'spectral_norm': 0.20223864912986755,\n",
       "  'spectral_norm_relative': 0.008538323896268433,\n",
       "  'mean_abs_difference': 0.0028361082077026367,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0014009667793288827,\n",
       "  'significant_diff_ratio': 0.92919921875,\n",
       "  'cosine_similarity': 0.9999940395355225,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_12_down_proj': {'frobenius_norm': 7.951237201690674,\n",
       "  'frobenius_norm_relative': 0.06618498389577321,\n",
       "  'spectral_norm': 7.951237201690674,\n",
       "  'spectral_norm_relative': 0.06618498389577321,\n",
       "  'mean_abs_difference': 0.0009430106147192419,\n",
       "  'max_abs_difference': 0.007568359375,\n",
       "  'std_difference': 0.0011852747993543744,\n",
       "  'significant_diff_ratio': 0.9729090929031372,\n",
       "  'cosine_similarity': 1.0039992332458496,\n",
       "  'weight_shape': torch.Size([4096, 11008]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_25_gate_proj': {'frobenius_norm': 8.83199691772461,\n",
       "  'frobenius_norm_relative': 0.0663101967262198,\n",
       "  'spectral_norm': 8.83199691772461,\n",
       "  'spectral_norm_relative': 0.0663101967262198,\n",
       "  'mean_abs_difference': 0.0010491920402273536,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.001316705485805869,\n",
       "  'significant_diff_ratio': 0.9731427431106567,\n",
       "  'cosine_similarity': 1.0042798519134521,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_27_q_proj': {'frobenius_norm': 5.1969733238220215,\n",
       "  'frobenius_norm_relative': 0.05670091953052372,\n",
       "  'spectral_norm': 5.1969733238220215,\n",
       "  'spectral_norm_relative': 0.05670091953052372,\n",
       "  'mean_abs_difference': 0.0010088107082992792,\n",
       "  'max_abs_difference': 0.00927734375,\n",
       "  'std_difference': 0.0012694415636360645,\n",
       "  'significant_diff_ratio': 0.9690819978713989,\n",
       "  'cosine_similarity': 0.9995550513267517,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_31_up_proj': {'frobenius_norm': 8.08641529083252,\n",
       "  'frobenius_norm_relative': 0.06018220445178067,\n",
       "  'spectral_norm': 8.08641529083252,\n",
       "  'spectral_norm_relative': 0.06018220445178067,\n",
       "  'mean_abs_difference': 0.0009542134357616305,\n",
       "  'max_abs_difference': 0.014190673828125,\n",
       "  'std_difference': 0.0012058718129992485,\n",
       "  'significant_diff_ratio': 0.9701734185218811,\n",
       "  'cosine_similarity': 1.0048518180847168,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_31_o_proj': {'frobenius_norm': 4.912502288818359,\n",
       "  'frobenius_norm_relative': 0.06277010804898855,\n",
       "  'spectral_norm': 4.912502288818359,\n",
       "  'spectral_norm_relative': 0.06277010804898855,\n",
       "  'mean_abs_difference': 0.0009500551968812943,\n",
       "  'max_abs_difference': 0.015625,\n",
       "  'std_difference': 0.0011997476685792208,\n",
       "  'significant_diff_ratio': 0.9717007875442505,\n",
       "  'cosine_similarity': 0.9993681907653809,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_22_v_proj': {'frobenius_norm': 4.870222091674805,\n",
       "  'frobenius_norm_relative': 0.066567108817231,\n",
       "  'spectral_norm': 4.870222091674805,\n",
       "  'spectral_norm_relative': 0.066567108817231,\n",
       "  'mean_abs_difference': 0.0009458037093281746,\n",
       "  'max_abs_difference': 0.00665283203125,\n",
       "  'std_difference': 0.0011893679620698094,\n",
       "  'significant_diff_ratio': 0.9731869697570801,\n",
       "  'cosine_similarity': 0.9990348815917969,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_28_k_proj': {'frobenius_norm': 5.324228763580322,\n",
       "  'frobenius_norm_relative': 0.05906270767149764,\n",
       "  'spectral_norm': 5.324228763580322,\n",
       "  'spectral_norm_relative': 0.05906270767149764,\n",
       "  'mean_abs_difference': 0.0010338198626413941,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.001300386036746204,\n",
       "  'significant_diff_ratio': 0.9705520868301392,\n",
       "  'cosine_similarity': 0.999366044998169,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_15_v_proj': {'frobenius_norm': 4.346723556518555,\n",
       "  'frobenius_norm_relative': 0.06840682709389885,\n",
       "  'spectral_norm': 4.346723556518555,\n",
       "  'spectral_norm_relative': 0.06840682709389885,\n",
       "  'mean_abs_difference': 0.0008423661347478628,\n",
       "  'max_abs_difference': 0.0058441162109375,\n",
       "  'std_difference': 0.0010617045918479562,\n",
       "  'significant_diff_ratio': 0.9737943410873413,\n",
       "  'cosine_similarity': 0.9984422922134399,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_15_o_proj': {'frobenius_norm': 4.4074859619140625,\n",
       "  'frobenius_norm_relative': 0.070286654839327,\n",
       "  'spectral_norm': 4.4074859619140625,\n",
       "  'spectral_norm_relative': 0.070286654839327,\n",
       "  'mean_abs_difference': 0.0008548776968382299,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.001076446264050901,\n",
       "  'significant_diff_ratio': 0.9745209813117981,\n",
       "  'cosine_similarity': 0.9983840584754944,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_16_q_proj': {'frobenius_norm': 5.3896074295043945,\n",
       "  'frobenius_norm_relative': 0.05629930536251091,\n",
       "  'spectral_norm': 5.3896074295043945,\n",
       "  'spectral_norm_relative': 0.05629930536251091,\n",
       "  'mean_abs_difference': 0.0010451240232214332,\n",
       "  'max_abs_difference': 0.0080413818359375,\n",
       "  'std_difference': 0.001316390698775649,\n",
       "  'significant_diff_ratio': 0.969587504863739,\n",
       "  'cosine_similarity': 0.9998968839645386,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_21_q_proj': {'frobenius_norm': 5.3157196044921875,\n",
       "  'frobenius_norm_relative': 0.05886154094391835,\n",
       "  'spectral_norm': 5.3157196044921875,\n",
       "  'spectral_norm_relative': 0.05886154094391835,\n",
       "  'mean_abs_difference': 0.0010316901607438922,\n",
       "  'max_abs_difference': 0.013671875,\n",
       "  'std_difference': 0.0012983259512111545,\n",
       "  'significant_diff_ratio': 0.9706134796142578,\n",
       "  'cosine_similarity': 0.9994752407073975,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_10_k_proj': {'frobenius_norm': 5.259161472320557,\n",
       "  'frobenius_norm_relative': 0.05006448356241329,\n",
       "  'spectral_norm': 5.259161472320557,\n",
       "  'spectral_norm_relative': 0.05006448356241329,\n",
       "  'mean_abs_difference': 0.001019523711875081,\n",
       "  'max_abs_difference': 0.0072021484375,\n",
       "  'std_difference': 0.001284815720282495,\n",
       "  'significant_diff_ratio': 0.9664433598518372,\n",
       "  'cosine_similarity': 1.000173807144165,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_16_post_attention_layernorm': {'frobenius_norm': 0.15516746044158936,\n",
       "  'frobenius_norm_relative': 0.008151916048659981,\n",
       "  'spectral_norm': 0.15516746044158936,\n",
       "  'spectral_norm_relative': 0.008151916048659981,\n",
       "  'mean_abs_difference': 0.0020401477813720703,\n",
       "  'max_abs_difference': 0.005859375,\n",
       "  'std_difference': 0.0013472323771566153,\n",
       "  'significant_diff_ratio': 0.805908203125,\n",
       "  'cosine_similarity': 0.999989926815033,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_2_up_proj': {'frobenius_norm': 7.382428169250488,\n",
       "  'frobenius_norm_relative': 0.06366556946267243,\n",
       "  'spectral_norm': 7.382428169250488,\n",
       "  'spectral_norm_relative': 0.06366556946267243,\n",
       "  'mean_abs_difference': 0.0008769890409894288,\n",
       "  'max_abs_difference': 0.0062255859375,\n",
       "  'std_difference': 0.001100563327781856,\n",
       "  'significant_diff_ratio': 0.9719051122665405,\n",
       "  'cosine_similarity': 1.0044782161712646,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_6_input_layernorm': {'frobenius_norm': 0.21336494386196136,\n",
       "  'frobenius_norm_relative': 0.01007518330977558,\n",
       "  'spectral_norm': 0.21336494386196136,\n",
       "  'spectral_norm_relative': 0.01007518330977558,\n",
       "  'mean_abs_difference': 0.0030593944247812033,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0013291712384670973,\n",
       "  'significant_diff_ratio': 0.95947265625,\n",
       "  'cosine_similarity': 0.9999922513961792,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_11_input_layernorm': {'frobenius_norm': 0.2766379415988922,\n",
       "  'frobenius_norm_relative': 0.011361792669613562,\n",
       "  'spectral_norm': 0.2766379415988922,\n",
       "  'spectral_norm_relative': 0.011361792669613562,\n",
       "  'mean_abs_difference': 0.004114169627428055,\n",
       "  'max_abs_difference': 0.009765625,\n",
       "  'std_difference': 0.001325808698311448,\n",
       "  'significant_diff_ratio': 0.99755859375,\n",
       "  'cosine_similarity': 0.9999959468841553,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_31_q_proj': {'frobenius_norm': 5.514733791351318,\n",
       "  'frobenius_norm_relative': 0.06240377657276829,\n",
       "  'spectral_norm': 5.514733791351318,\n",
       "  'spectral_norm_relative': 0.06240377657276829,\n",
       "  'mean_abs_difference': 0.0010688756592571735,\n",
       "  'max_abs_difference': 0.01171875,\n",
       "  'std_difference': 0.0013467506505548954,\n",
       "  'significant_diff_ratio': 0.971977710723877,\n",
       "  'cosine_similarity': 0.999191403388977,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_29_down_proj': {'frobenius_norm': 8.359954833984375,\n",
       "  'frobenius_norm_relative': 0.06656437202723305,\n",
       "  'spectral_norm': 8.359954833984375,\n",
       "  'spectral_norm_relative': 0.06656437202723305,\n",
       "  'mean_abs_difference': 0.0009864402236416936,\n",
       "  'max_abs_difference': 0.01171875,\n",
       "  'std_difference': 0.0012465473264455795,\n",
       "  'significant_diff_ratio': 0.9727200269699097,\n",
       "  'cosine_similarity': 1.003531575202942,\n",
       "  'weight_shape': torch.Size([4096, 11008]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_31_post_attention_layernorm': {'frobenius_norm': 0.2960869073867798,\n",
       "  'frobenius_norm_relative': 0.010916757888783442,\n",
       "  'spectral_norm': 0.2960869073867798,\n",
       "  'spectral_norm_relative': 0.010916757888783442,\n",
       "  'mean_abs_difference': 0.0044422149658203125,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0012973180273547769,\n",
       "  'significant_diff_ratio': 0.9990234375,\n",
       "  'cosine_similarity': 0.999998927116394,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_12_gate_proj': {'frobenius_norm': 8.49283218383789,\n",
       "  'frobenius_norm_relative': 0.06702222679800034,\n",
       "  'spectral_norm': 8.49283218383789,\n",
       "  'spectral_norm_relative': 0.06702222679800034,\n",
       "  'mean_abs_difference': 0.0010079869534820318,\n",
       "  'max_abs_difference': 0.00726318359375,\n",
       "  'std_difference': 0.0012662364169955254,\n",
       "  'significant_diff_ratio': 0.9735888242721558,\n",
       "  'cosine_similarity': 1.0035734176635742,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_26_o_proj': {'frobenius_norm': 5.141714572906494,\n",
       "  'frobenius_norm_relative': 0.064698671590432,\n",
       "  'spectral_norm': 5.141714572906494,\n",
       "  'spectral_norm_relative': 0.064698671590432,\n",
       "  'mean_abs_difference': 0.0009970241226255894,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0012556178262457252,\n",
       "  'significant_diff_ratio': 0.9725108742713928,\n",
       "  'cosine_similarity': 0.9991836547851562,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_20_o_proj': {'frobenius_norm': 4.88060998916626,\n",
       "  'frobenius_norm_relative': 0.06944919333632711,\n",
       "  'spectral_norm': 4.88060998916626,\n",
       "  'spectral_norm_relative': 0.06944919333632711,\n",
       "  'mean_abs_difference': 0.0009455127292312682,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0011917821830138564,\n",
       "  'significant_diff_ratio': 0.9740130305290222,\n",
       "  'cosine_similarity': 0.9988136887550354,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_26_input_layernorm': {'frobenius_norm': 0.36475661396980286,\n",
       "  'frobenius_norm_relative': 0.010862169839446961,\n",
       "  'spectral_norm': 0.36475661396980286,\n",
       "  'spectral_norm_relative': 0.010862169839446961,\n",
       "  'mean_abs_difference': 0.005320478230714798,\n",
       "  'max_abs_difference': 0.015625,\n",
       "  'std_difference': 0.0020434781908988953,\n",
       "  'significant_diff_ratio': 0.980224609375,\n",
       "  'cosine_similarity': 0.9999926090240479,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_23_gate_proj': {'frobenius_norm': 8.81283187866211,\n",
       "  'frobenius_norm_relative': 0.06644594820329307,\n",
       "  'spectral_norm': 8.81283187866211,\n",
       "  'spectral_norm_relative': 0.06644594820329307,\n",
       "  'mean_abs_difference': 0.0010470787528902292,\n",
       "  'max_abs_difference': 0.008571624755859375,\n",
       "  'std_difference': 0.0013138602953404188,\n",
       "  'significant_diff_ratio': 0.9731373190879822,\n",
       "  'cosine_similarity': 1.0041559934616089,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_27_o_proj': {'frobenius_norm': 5.042657375335693,\n",
       "  'frobenius_norm_relative': 0.06302448395957762,\n",
       "  'spectral_norm': 5.042657375335693,\n",
       "  'spectral_norm_relative': 0.06302448395957762,\n",
       "  'mean_abs_difference': 0.000979797332547605,\n",
       "  'max_abs_difference': 0.01171875,\n",
       "  'std_difference': 0.0012315205531194806,\n",
       "  'significant_diff_ratio': 0.9716857671737671,\n",
       "  'cosine_similarity': 0.9992443323135376,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_7_up_proj': {'frobenius_norm': 7.707221508026123,\n",
       "  'frobenius_norm_relative': 0.06645271422110637,\n",
       "  'spectral_norm': 7.707221508026123,\n",
       "  'spectral_norm_relative': 0.06645271422110637,\n",
       "  'mean_abs_difference': 0.0009143789066001773,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0011488894233480096,\n",
       "  'significant_diff_ratio': 0.973019003868103,\n",
       "  'cosine_similarity': 1.0042904615402222,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_15_k_proj': {'frobenius_norm': 5.4003987312316895,\n",
       "  'frobenius_norm_relative': 0.05412723877470095,\n",
       "  'spectral_norm': 5.4003987312316895,\n",
       "  'spectral_norm_relative': 0.05412723877470095,\n",
       "  'mean_abs_difference': 0.001046835328452289,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.001319154049269855,\n",
       "  'significant_diff_ratio': 0.9685633778572083,\n",
       "  'cosine_similarity': 0.9999767541885376,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_25_post_attention_layernorm': {'frobenius_norm': 0.25286558270454407,\n",
       "  'frobenius_norm_relative': 0.0094085192235228,\n",
       "  'spectral_norm': 0.25286558270454407,\n",
       "  'spectral_norm_relative': 0.0094085192235228,\n",
       "  'mean_abs_difference': 0.003692150115966797,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0014186420012265444,\n",
       "  'significant_diff_ratio': 0.98193359375,\n",
       "  'cosine_similarity': 0.9999974370002747,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_17_o_proj': {'frobenius_norm': 4.663743019104004,\n",
       "  'frobenius_norm_relative': 0.07111127765763556,\n",
       "  'spectral_norm': 4.663743019104004,\n",
       "  'spectral_norm_relative': 0.07111127765763556,\n",
       "  'mean_abs_difference': 0.0009037334239110351,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0011388639686629176,\n",
       "  'significant_diff_ratio': 0.9747008085250854,\n",
       "  'cosine_similarity': 0.9984526634216309,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_23_q_proj': {'frobenius_norm': 5.275055408477783,\n",
       "  'frobenius_norm_relative': 0.05761193948695812,\n",
       "  'spectral_norm': 5.275055408477783,\n",
       "  'spectral_norm_relative': 0.05761193948695812,\n",
       "  'mean_abs_difference': 0.0010234846267849207,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0012884729076176882,\n",
       "  'significant_diff_ratio': 0.9697124361991882,\n",
       "  'cosine_similarity': 0.9996523857116699,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_17_down_proj': {'frobenius_norm': 8.140670776367188,\n",
       "  'frobenius_norm_relative': 0.06703736734450512,\n",
       "  'spectral_norm': 8.140670776367188,\n",
       "  'spectral_norm_relative': 0.06703736734450512,\n",
       "  'mean_abs_difference': 0.0009659070638008416,\n",
       "  'max_abs_difference': 0.015625,\n",
       "  'std_difference': 0.001213588286191225,\n",
       "  'significant_diff_ratio': 0.9732857346534729,\n",
       "  'cosine_similarity': 1.0038080215454102,\n",
       "  'weight_shape': torch.Size([4096, 11008]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_5_post_attention_layernorm': {'frobenius_norm': 0.13624075055122375,\n",
       "  'frobenius_norm_relative': 0.010863133032225511,\n",
       "  'spectral_norm': 0.13624075055122375,\n",
       "  'spectral_norm_relative': 0.010863133032225511,\n",
       "  'mean_abs_difference': 0.001877129077911377,\n",
       "  'max_abs_difference': 0.005859375,\n",
       "  'std_difference': 0.001049379468895495,\n",
       "  'significant_diff_ratio': 0.92724609375,\n",
       "  'cosine_similarity': 0.9999881982803345,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_0_gate_proj': {'frobenius_norm': 7.291021823883057,\n",
       "  'frobenius_norm_relative': 0.06694593561687379,\n",
       "  'spectral_norm': 7.291021823883057,\n",
       "  'spectral_norm_relative': 0.06694593561687379,\n",
       "  'mean_abs_difference': 0.0008643656619824469,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.001086859731003642,\n",
       "  'significant_diff_ratio': 0.9733356833457947,\n",
       "  'cosine_similarity': 1.0047595500946045,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_3_q_proj': {'frobenius_norm': 4.599825859069824,\n",
       "  'frobenius_norm_relative': 0.04500039260034916,\n",
       "  'spectral_norm': 4.599825859069824,\n",
       "  'spectral_norm_relative': 0.04500039260034916,\n",
       "  'mean_abs_difference': 0.0008899049134925008,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0011240741005167365,\n",
       "  'significant_diff_ratio': 0.9630357623100281,\n",
       "  'cosine_similarity': 1.0005450248718262,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_2_down_proj': {'frobenius_norm': 7.458620071411133,\n",
       "  'frobenius_norm_relative': 0.06392231488560686,\n",
       "  'spectral_norm': 7.458620071411133,\n",
       "  'spectral_norm_relative': 0.06392231488560686,\n",
       "  'mean_abs_difference': 0.0008861838141456246,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0011119086993858218,\n",
       "  'significant_diff_ratio': 0.9720113277435303,\n",
       "  'cosine_similarity': 1.0043858289718628,\n",
       "  'weight_shape': torch.Size([4096, 11008]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_11_post_attention_layernorm': {'frobenius_norm': 0.1279473900794983,\n",
       "  'frobenius_norm_relative': 0.008222338833141364,\n",
       "  'spectral_norm': 0.1279473900794983,\n",
       "  'spectral_norm_relative': 0.008222338833141364,\n",
       "  'mean_abs_difference': 0.0016783475875854492,\n",
       "  'max_abs_difference': 0.0048828125,\n",
       "  'std_difference': 0.0011687172809615731,\n",
       "  'significant_diff_ratio': 0.8505859375,\n",
       "  'cosine_similarity': 0.9999914169311523,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_0_input_layernorm': {'frobenius_norm': 0.06370905786752701,\n",
       "  'frobenius_norm_relative': 0.026121502108325926,\n",
       "  'spectral_norm': 0.06370905786752701,\n",
       "  'spectral_norm_relative': 0.026121502108325926,\n",
       "  'mean_abs_difference': 0.0007808239897713065,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0009826107416301966,\n",
       "  'significant_diff_ratio': 0.964599609375,\n",
       "  'cosine_similarity': 0.9996707439422607,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_25_q_proj': {'frobenius_norm': 5.274148464202881,\n",
       "  'frobenius_norm_relative': 0.05874496535043081,\n",
       "  'spectral_norm': 5.274148464202881,\n",
       "  'spectral_norm_relative': 0.05874496535043081,\n",
       "  'mean_abs_difference': 0.0010241870768368244,\n",
       "  'max_abs_difference': 0.0133056640625,\n",
       "  'std_difference': 0.0012881677830591798,\n",
       "  'significant_diff_ratio': 0.9704422354698181,\n",
       "  'cosine_similarity': 0.9994786381721497,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_25_k_proj': {'frobenius_norm': 5.252642631530762,\n",
       "  'frobenius_norm_relative': 0.058312784052417195,\n",
       "  'spectral_norm': 5.252642631530762,\n",
       "  'spectral_norm_relative': 0.058312784052417195,\n",
       "  'mean_abs_difference': 0.001020154682919383,\n",
       "  'max_abs_difference': 0.0076904296875,\n",
       "  'std_difference': 0.0012829340994358063,\n",
       "  'significant_diff_ratio': 0.9702855348587036,\n",
       "  'cosine_similarity': 0.9995087385177612,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_9_v_proj': {'frobenius_norm': 4.381922721862793,\n",
       "  'frobenius_norm_relative': 0.07438238926223756,\n",
       "  'spectral_norm': 4.381922721862793,\n",
       "  'spectral_norm_relative': 0.07438238926223756,\n",
       "  'mean_abs_difference': 0.0008501394768245518,\n",
       "  'max_abs_difference': 0.00640869140625,\n",
       "  'std_difference': 0.0010701532009989023,\n",
       "  'significant_diff_ratio': 0.9759843945503235,\n",
       "  'cosine_similarity': 0.9983159303665161,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216}}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9908bfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0009561135084368289\n",
      "0.0009058689465746284\n",
      "0.0010020476765930653\n",
      "0.0010336291743442416\n",
      "0.0008379047503694892\n",
      "0.0009911861270666122\n",
      "0.0009741014800965786\n",
      "0.004400855395942926\n",
      "0.004629045724868774\n",
      "0.003312712535262108\n",
      "0.0009484292240813375\n",
      "0.0008355370373465121\n",
      "0.0009050709777511656\n",
      "0.004676243755966425\n",
      "0.0010285121388733387\n",
      "0.0010895447339862585\n",
      "0.00092760642291978\n",
      "0.0024721622467041016\n",
      "0.0008579519344493747\n",
      "0.0007826546207070351\n",
      "0.0010299839777871966\n",
      "0.0008387219277210534\n",
      "0.003567485371604562\n",
      "0.0009204488596878946\n",
      "0.001008506747893989\n",
      "0.0008401564555242658\n",
      "0.0009431222570128739\n",
      "0.000997869879938662\n",
      "0.0009723561233840883\n",
      "0.0010217539966106415\n",
      "0.0008733381982892752\n",
      "0.0016355335246771574\n",
      "0.0009452314116060734\n",
      "0.0010106184054166079\n",
      "0.0008362624794244766\n",
      "0.0049591064453125\n",
      "0.0023543834686279297\n",
      "0.0010138978250324726\n",
      "0.0008328189142048359\n",
      "0.0007840266334824264\n",
      "0.000944170169532299\n",
      "0.0010239921975880861\n",
      "0.0010171601315960288\n",
      "0.0008548144833184779\n",
      "0.0009964585769921541\n",
      "0.0010171675821766257\n",
      "0.0016049742698669434\n",
      "0.0010410810355097055\n",
      "0.0009908040519803762\n",
      "0.0010271577630192041\n",
      "0.0004406938096508384\n",
      "0.0009403752046637237\n",
      "0.0009660234209150076\n",
      "0.0009015626274049282\n",
      "0.0010643609566614032\n",
      "0.0008289013057947159\n",
      "0.0010458707110956311\n",
      "0.0009281010716222227\n",
      "0.0010055670281872153\n",
      "0.001042781863361597\n",
      "0.0008565316675230861\n",
      "0.0008687714580446482\n",
      "0.0010051733115687966\n",
      "0.0009391416097059846\n",
      "0.0010352940298616886\n",
      "0.000939988880418241\n",
      "0.0009563038474880159\n",
      "0.00105039041955024\n",
      "0.0010375857818871737\n",
      "0.0010318574495613575\n",
      "0.0010234675137326121\n",
      "0.0009966341312974691\n",
      "0.0008982150466181338\n",
      "0.0009368686005473137\n",
      "0.0010259905830025673\n",
      "0.001902759075164795\n",
      "0.0008578454726375639\n",
      "0.0009961165487766266\n",
      "0.0009500262094661593\n",
      "0.001734018325805664\n",
      "0.0009837074903771281\n",
      "0.004403494764119387\n",
      "0.0009299881057813764\n",
      "0.000985409365966916\n",
      "0.0010321848094463348\n",
      "0.005087739787995815\n",
      "0.0008728926768526435\n",
      "0.004119643941521645\n",
      "0.004353410564363003\n",
      "0.0008852165192365646\n",
      "0.001860499382019043\n",
      "0.0009856141405180097\n",
      "0.0010291552171111107\n",
      "0.0009192785946652293\n",
      "0.0010230974294245243\n",
      "0.010121654719114304\n",
      "0.0009260291699320078\n",
      "0.0008508003666065633\n",
      "0.003193755866959691\n",
      "0.004009516444057226\n",
      "0.001046657213009894\n",
      "0.001020663185045123\n",
      "0.0008263499476015568\n",
      "0.0010072553995996714\n",
      "0.0009147930541075766\n",
      "0.0009033440728671849\n",
      "0.004318595863878727\n",
      "0.0010130091104656458\n",
      "0.0007258172845467925\n",
      "0.0009726113057695329\n",
      "0.0052474793046712875\n",
      "0.0035457611083984375\n",
      "0.005169880576431751\n",
      "0.004338681697845459\n",
      "0.0009936142014339566\n",
      "0.004798755049705505\n",
      "0.0010338914580643177\n",
      "0.0009376067901030183\n",
      "0.0008440351230092347\n",
      "0.0010325564071536064\n",
      "0.0009127834346145391\n",
      "0.0010109497234225273\n",
      "0.0010387334041297436\n",
      "0.00100141076836735\n",
      "0.0024109738878905773\n",
      "0.0009861363796517253\n",
      "0.0008384695975109935\n",
      "0.0008706428925506771\n",
      "0.000902065890841186\n",
      "0.0018409490585327148\n",
      "0.0009769403841346502\n",
      "0.001038905931636691\n",
      "0.0016632080078125\n",
      "0.0010396541329100728\n",
      "0.0010338424472138286\n",
      "0.000892461568582803\n",
      "0.0009674763423390687\n",
      "0.001799464225769043\n",
      "0.0008649220690131187\n",
      "0.0009865862084552646\n",
      "0.004855513572692871\n",
      "0.0010160889942198992\n",
      "0.0030868053436279297\n",
      "0.0008480151882395148\n",
      "0.0008670289535075426\n",
      "0.0012758595403283834\n",
      "0.0009478670544922352\n",
      "0.0010129038710147142\n",
      "0.002934732474386692\n",
      "0.0009548623347654939\n",
      "0.0008556984830647707\n",
      "0.00102561479434371\n",
      "0.0009831442730501294\n",
      "0.0009676441550254822\n",
      "0.0008836511406116188\n",
      "0.0009594949660822749\n",
      "0.0010138285579159856\n",
      "0.000740374147426337\n",
      "0.000941154663451016\n",
      "0.0009188769618049264\n",
      "0.000932212162297219\n",
      "0.0008569533238187432\n",
      "0.0009530227980576456\n",
      "0.0008695260039530694\n",
      "0.0008702456834726036\n",
      "0.0010362555040046573\n",
      "0.0010201759869232774\n",
      "0.0010194431524723768\n",
      "0.0008682101033627987\n",
      "0.00043242849642410874\n",
      "0.0008541274000890553\n",
      "0.0010347208008170128\n",
      "0.0009224803070537746\n",
      "0.0009307627333328128\n",
      "0.0009754984639585018\n",
      "0.0010521032381802797\n",
      "0.001015764893963933\n",
      "0.0010461758356541395\n",
      "0.001048817066475749\n",
      "0.0010051049757748842\n",
      "0.0008771020220592618\n",
      "0.0010526154655963182\n",
      "0.0010173505870625377\n",
      "0.0009854938834905624\n",
      "0.0010334814433008432\n",
      "0.004540622234344482\n",
      "0.001012250897474587\n",
      "0.0010297062108293176\n",
      "0.0010255755623802543\n",
      "0.004293352365493774\n",
      "0.000772513507399708\n",
      "0.0019086599349975586\n",
      "0.0009474256657995284\n",
      "0.0008602877496741712\n",
      "0.0008306287927553058\n",
      "0.0008664465858601034\n",
      "0.0009610253619030118\n",
      "0.00101311388425529\n",
      "0.0009909870568662882\n",
      "0.0009967676596716046\n",
      "0.0008593168458901346\n",
      "0.0024023056030273438\n",
      "0.0008826879784464836\n",
      "0.0010304109891876578\n",
      "0.0009646923863328993\n",
      "0.0010182401165366173\n",
      "0.0008942301501519978\n",
      "0.0018381178379058838\n",
      "0.0008623851463198662\n",
      "0.003332793712615967\n",
      "0.0016280289273709059\n",
      "0.0018096566200256348\n",
      "0.0010505927493795753\n",
      "0.004198194481432438\n",
      "0.0010019522160291672\n",
      "0.0009658053750172257\n",
      "0.0027413368225097656\n",
      "0.0045935227535665035\n",
      "0.000920667196623981\n",
      "0.003977775573730469\n",
      "0.0010011799167841673\n",
      "0.0010173255577683449\n",
      "0.0009731253958307207\n",
      "0.0050654723308980465\n",
      "0.0009064741898328066\n",
      "0.0008642047178000212\n",
      "0.0010503368685021996\n",
      "0.0008440296514891088\n",
      "0.0010334529215469956\n",
      "0.0010249504121020436\n",
      "0.0009067205828614533\n",
      "0.0008998644771054387\n",
      "0.002398568904027343\n",
      "0.0009708308498375118\n",
      "0.0009529832168482244\n",
      "0.0009586571832187474\n",
      "0.0009796591475605965\n",
      "0.0029208073392510414\n",
      "0.005391884595155716\n",
      "0.0009691443992778659\n",
      "0.0009477834100835025\n",
      "0.0010366415372118354\n",
      "0.001034602290019393\n",
      "0.003056943416595459\n",
      "0.0009990442777052522\n",
      "0.00447250297293067\n",
      "0.0007397193694487214\n",
      "0.0008473646594211459\n",
      "0.0009672019514255226\n",
      "0.001527020474895835\n",
      "0.0028361082077026367\n",
      "0.0009430106147192419\n",
      "0.0010491920402273536\n",
      "0.0010088107082992792\n",
      "0.0009542134357616305\n",
      "0.0009500551968812943\n",
      "0.0009458037093281746\n",
      "0.0010338198626413941\n",
      "0.0008423661347478628\n",
      "0.0008548776968382299\n",
      "0.0010451240232214332\n",
      "0.0010316901607438922\n",
      "0.001019523711875081\n",
      "0.0020401477813720703\n",
      "0.0008769890409894288\n",
      "0.0030593944247812033\n",
      "0.004114169627428055\n",
      "0.0010688756592571735\n",
      "0.0009864402236416936\n",
      "0.0044422149658203125\n",
      "0.0010079869534820318\n",
      "0.0009970241226255894\n",
      "0.0009455127292312682\n",
      "0.005320478230714798\n",
      "0.0010470787528902292\n",
      "0.000979797332547605\n",
      "0.0009143789066001773\n",
      "0.001046835328452289\n",
      "0.003692150115966797\n",
      "0.0009037334239110351\n",
      "0.0010234846267849207\n",
      "0.0009659070638008416\n",
      "0.001877129077911377\n",
      "0.0008643656619824469\n",
      "0.0008899049134925008\n",
      "0.0008861838141456246\n",
      "0.0016783475875854492\n",
      "0.0007808239897713065\n",
      "0.0010241870768368244\n",
      "0.001020154682919383\n",
      "0.0008501394768245518\n"
     ]
    }
   ],
   "source": [
    "for i in weight_differences:\n",
    "    print(weight_differences[i]['mean_abs_difference'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3bf55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "activations_model_1 = {}\n",
    "activations_model_2 = {}\n",
    "current_hooks = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854f9fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_activations():\n",
    "    global activations_model_1, activations_model_2\n",
    "    activations_model_1.clear()\n",
    "    activations_model_2.clear()\n",
    "\n",
    "def remove_all_hooks():\n",
    "    global current_hooks\n",
    "    for hook in current_hooks:\n",
    "        hook.remove()\n",
    "    current_hooks.clear()\n",
    "\n",
    "def get_activation_hook(name, model_name):\n",
    "    def hook(module, input, output):\n",
    "        try:\n",
    "            # Handle output\n",
    "            if isinstance(output, tuple):\n",
    "                activation = output[0] if output[0] is not None else None\n",
    "            else:\n",
    "                activation = output\n",
    "            \n",
    "            # Handle input - check for None values\n",
    "            input_tensor = None\n",
    "            if input is not None:\n",
    "                if isinstance(input, tuple) and len(input) > 0:\n",
    "                    input_tensor = input[0] if input[0] is not None else None\n",
    "                else:\n",
    "                    input_tensor = input if input is not None else None\n",
    "            \n",
    "            # Create activation data with None checks\n",
    "            activation_data = {\n",
    "                'output': activation.detach().cpu() if activation is not None else None,\n",
    "                'input': input_tensor.detach().cpu() if input_tensor is not None else None,\n",
    "                'weight': module.weight.detach().cpu() if hasattr(module, 'weight') and module.weight is not None else None,\n",
    "                'bias': module.bias.detach().cpu() if hasattr(module, 'bias') and module.bias is not None else None\n",
    "            }\n",
    "            \n",
    "            if model_name == \"Model_1\":\n",
    "                activations_model_1[name] = activation_data\n",
    "            else:\n",
    "                activations_model_2[name] = activation_data\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error in hook {name}: {e}\")\n",
    "            # Store None data to prevent missing keys\n",
    "            activation_data = {\n",
    "                'output': None,\n",
    "                'input': None, \n",
    "                'weight': None,\n",
    "                'bias': None\n",
    "            }\n",
    "            \n",
    "            if model_name == \"Model_1\":\n",
    "                activations_model_1[name] = activation_data\n",
    "            else:\n",
    "                activations_model_2[name] = activation_data\n",
    "            \n",
    "    return hook\n",
    "\n",
    "def register_llama_hooks(model, model_name, layer_range=None):\n",
    "    global current_hooks\n",
    "    hooks = []\n",
    "    layer_info = {}\n",
    "    \n",
    "    # Determine layer range\n",
    "    if layer_range is None:\n",
    "        layer_range = range(len(model.model.layers))\n",
    "    \n",
    "    for i in layer_range:\n",
    "        if i >= len(model.model.layers):\n",
    "            continue\n",
    "            \n",
    "        layer = model.model.layers[i]\n",
    "        layer_prefix = f\"layer_{i}\"\n",
    "        \n",
    "        # 1. Self-Attention Components\n",
    "        # Query, Key, Value projections\n",
    "        hooks.append(layer.self_attn.q_proj.register_forward_hook(\n",
    "            get_activation_hook(f\"{layer_prefix}_attention_q\", model_name)\n",
    "        ))\n",
    "        hooks.append(layer.self_attn.k_proj.register_forward_hook(\n",
    "            get_activation_hook(f\"{layer_prefix}_attention_k\", model_name)\n",
    "        ))\n",
    "        hooks.append(layer.self_attn.v_proj.register_forward_hook(\n",
    "            get_activation_hook(f\"{layer_prefix}_attention_v\", model_name)\n",
    "        ))\n",
    "        \n",
    "        # Output projection\n",
    "        hooks.append(layer.self_attn.o_proj.register_forward_hook(\n",
    "            get_activation_hook(f\"{layer_prefix}_attention_output\", model_name)\n",
    "        ))\n",
    "        \n",
    "        # 2. MLP Components  \n",
    "        hooks.append(layer.mlp.gate_proj.register_forward_hook(\n",
    "            get_activation_hook(f\"{layer_prefix}_mlp_gate\", model_name)\n",
    "        ))\n",
    "        hooks.append(layer.mlp.up_proj.register_forward_hook(\n",
    "            get_activation_hook(f\"{layer_prefix}_mlp_up\", model_name)\n",
    "        ))\n",
    "        hooks.append(layer.mlp.down_proj.register_forward_hook(\n",
    "            get_activation_hook(f\"{layer_prefix}_mlp_down\", model_name)\n",
    "        ))\n",
    "        \n",
    "        # 3. Layer Norms\n",
    "        hooks.append(layer.input_layernorm.register_forward_hook(\n",
    "            get_activation_hook(f\"{layer_prefix}_input_norm\", model_name)\n",
    "        ))\n",
    "        hooks.append(layer.post_attention_layernorm.register_forward_hook(\n",
    "            get_activation_hook(f\"{layer_prefix}_post_attn_norm\", model_name)\n",
    "        ))\n",
    "        \n",
    "        # Store layer info\n",
    "        layer_info[layer_prefix] = {\n",
    "            'layer_idx': i,\n",
    "            'components': ['attention_q', 'attention_k', 'attention_v', \n",
    "                         'attention_output', 'mlp_gate', 'mlp_up', 'mlp_down',\n",
    "                         'input_norm', 'post_attn_norm']\n",
    "        }\n",
    "    \n",
    "    # Final layer norm and LM head (optional)\n",
    "    hooks.append(model.model.norm.register_forward_hook(\n",
    "        get_activation_hook(\"final_norm\", model_name)\n",
    "    ))\n",
    "    hooks.append(model.lm_head.register_forward_hook(\n",
    "        get_activation_hook(\"lm_head\", model_name)\n",
    "    ))\n",
    "    \n",
    "    current_hooks.extend(hooks)\n",
    "    return hooks, layer_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf4d8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_random_neurons_llama(activations, n_neurons=5, n_tokens=3):\n",
    "    selected_indices = {}\n",
    "    \n",
    "    for layer_name, layer_data in activations.items():\n",
    "        if not isinstance(layer_data, dict):\n",
    "            continue\n",
    "            \n",
    "        activation = layer_data.get('output')\n",
    "        \n",
    "        # Skip if no activation or activation is None\n",
    "        if activation is None:\n",
    "            print(f\"Skipping {layer_name}: No activation data\")\n",
    "            continue\n",
    "        \n",
    "        # Handle different activation shapes\n",
    "        if len(activation.shape) == 3:  # [batch, seq_len, hidden_size]\n",
    "            batch_size, seq_len, hidden_size = activation.shape\n",
    "            \n",
    "            # Skip if dimensions are invalid\n",
    "            if hidden_size == 0 or seq_len == 0:\n",
    "                continue\n",
    "            \n",
    "            # Flatten to [batch * seq_len, hidden_size]\n",
    "            flattened = activation.view(-1, hidden_size)\n",
    "            \n",
    "            # Find significant activations\n",
    "            try:\n",
    "                if 'mlp' in layer_name and ('gate' in layer_name or 'up' in layer_name):\n",
    "                    # Apply SiLU activation for MLP gate/up projections\n",
    "                    activated = F.silu(flattened)\n",
    "                    significant_mask = torch.abs(activated) > 0.01\n",
    "                else:\n",
    "                    # For other components, just check magnitude\n",
    "                    significant_mask = torch.abs(flattened) > 0.01\n",
    "                \n",
    "                # Get indices of significant activations\n",
    "                activated_positions = torch.nonzero(significant_mask, as_tuple=True)\n",
    "                \n",
    "                if len(activated_positions[0]) == 0:\n",
    "                    print(f\"No significant activations in {layer_name}\")\n",
    "                    continue\n",
    "                \n",
    "                # Randomly select positions and neurons\n",
    "                n_selections = min(n_neurons * n_tokens, len(activated_positions[0]))\n",
    "                if n_selections == 0:\n",
    "                    continue\n",
    "                    \n",
    "                selected_idx = torch.randint(0, len(activated_positions[0]), (n_selections,))\n",
    "                \n",
    "                selected_positions = activated_positions[0][selected_idx].numpy()\n",
    "                selected_neurons = activated_positions[1][selected_idx].numpy()\n",
    "                \n",
    "                # Convert back to token and neuron indices\n",
    "                token_indices = selected_positions // hidden_size\n",
    "                neuron_indices = selected_neurons\n",
    "                \n",
    "                selected_indices[layer_name] = {\n",
    "                    'token_indices': token_indices,\n",
    "                    'neuron_indices': neuron_indices,\n",
    "                    'flat_positions': selected_positions,\n",
    "                    'activation_shape': activation.shape,\n",
    "                    'n_selected': len(selected_positions)\n",
    "                }\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {layer_name}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    return selected_indices\n",
    "\n",
    "def get_component_type(layer_name):\n",
    "    if 'attention' in layer_name:\n",
    "        return 'attention'\n",
    "    elif 'mlp' in layer_name:\n",
    "        return 'mlp'\n",
    "    elif 'norm' in layer_name:\n",
    "        return 'normalization'\n",
    "    elif 'lm_head' in layer_name:\n",
    "        return 'output'\n",
    "    else:\n",
    "        return 'other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9d201c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_expected_activation(input_tensor, weights, bias, token_idx, neuron_idx, layer_name):\n",
    "    try:\n",
    "        # Check for None inputs\n",
    "        if input_tensor is None or weights is None:\n",
    "            return None\n",
    "            \n",
    "        if len(input_tensor.shape) == 3:  # [batch, seq_len, hidden_size]\n",
    "            if token_idx >= input_tensor.shape[1]:\n",
    "                return None\n",
    "            \n",
    "            # Get input for specific token\n",
    "            token_input = input_tensor[0, token_idx, :]  # [hidden_size]\n",
    "            \n",
    "            if neuron_idx >= weights.shape[0]:\n",
    "                return None\n",
    "            \n",
    "            # Calculate: input · weight + bias\n",
    "            neuron_weights = weights[neuron_idx, :]\n",
    "            expected = torch.dot(token_input.flatten(), neuron_weights.flatten())\n",
    "            \n",
    "            if bias is not None and neuron_idx < bias.shape[0]:\n",
    "                expected += bias[neuron_idx]\n",
    "            \n",
    "            # Apply activation function based on component type\n",
    "            if 'mlp_gate' in layer_name or 'mlp_up' in layer_name:\n",
    "                expected = F.silu(expected)\n",
    "            \n",
    "            return expected.item()\n",
    "        \n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating expected activation for {layer_name}: {e}\")\n",
    "        return None\n",
    "\n",
    "def compare_selected_activations(activations_1, activations_2, selected_1):\n",
    "    comparison_results = {}\n",
    "    \n",
    "    for layer_name in selected_1:\n",
    "        if layer_name not in activations_2:\n",
    "            print(f\"Layer {layer_name} not found in model 2 activations\")\n",
    "            continue\n",
    "            \n",
    "        layer_1_data = activations_1[layer_name]\n",
    "        layer_2_data = activations_2[layer_name]\n",
    "        selected_neurons = selected_1[layer_name]\n",
    "        \n",
    "        layer_results = {\n",
    "            'neuron_comparisons': [],\n",
    "            'mean_difference': float('nan'),\n",
    "            'max_difference': float('nan'),\n",
    "            'min_difference': float('nan'),\n",
    "            'component_type': get_component_type(layer_name)\n",
    "        }\n",
    "        \n",
    "        # Get activations and weights - check for None values\n",
    "        activation_1 = layer_1_data.get('output')\n",
    "        input_1 = layer_1_data.get('input')\n",
    "        weights_2 = layer_2_data.get('weight')\n",
    "        bias_2 = layer_2_data.get('bias')\n",
    "        \n",
    "        # Skip if essential data is missing\n",
    "        if activation_1 is None or input_1 is None or weights_2 is None:\n",
    "            print(f\"Skipping {layer_name}: Missing essential data\")\n",
    "            comparison_results[layer_name] = layer_results\n",
    "            continue\n",
    "        \n",
    "        # Compare each selected neuron\n",
    "        token_indices = selected_neurons['token_indices']\n",
    "        neuron_indices = selected_neurons['neuron_indices']\n",
    "        \n",
    "        differences = []\n",
    "        \n",
    "        for i, (token_idx, neuron_idx) in enumerate(zip(token_indices, neuron_indices)):\n",
    "            try:\n",
    "                if token_idx >= activation_1.shape[1] or neuron_idx >= activation_1.shape[2]:\n",
    "                    continue\n",
    "                \n",
    "                # Get actual activation from Model_1\n",
    "                actual_activation = activation_1[0, token_idx, neuron_idx].item()\n",
    "                \n",
    "                # Calculate expected activation using Model_2's weights\n",
    "                expected_activation = calculate_expected_activation(\n",
    "                    input_1, weights_2, bias_2, token_idx, neuron_idx, layer_name\n",
    "                )\n",
    "                \n",
    "                if expected_activation is None:\n",
    "                    continue\n",
    "                \n",
    "                # Calculate differences\n",
    "                difference = abs(actual_activation - expected_activation)\n",
    "                percent_diff = (difference / (abs(actual_activation) + 1e-10)) * 100\n",
    "                \n",
    "                neuron_comparison = {\n",
    "                    'token_idx': int(token_idx),\n",
    "                    'neuron_idx': int(neuron_idx),\n",
    "                    'actual_activation': actual_activation,\n",
    "                    'expected_activation': expected_activation,\n",
    "                    'difference': difference,\n",
    "                    'percent_difference': percent_diff\n",
    "                }\n",
    "                \n",
    "                layer_results['neuron_comparisons'].append(neuron_comparison)\n",
    "                differences.append(difference)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing neuron {neuron_idx} in {layer_name}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Calculate layer statistics\n",
    "        if differences:\n",
    "            layer_results['mean_difference'] = np.mean(differences)\n",
    "            layer_results['max_difference'] = np.max(differences)\n",
    "            layer_results['min_difference'] = np.min(differences)\n",
    "            print(f\"✅ {layer_name}: {len(differences)} neurons compared\")\n",
    "        else:\n",
    "            print(f\"⚠️  {layer_name}: No valid comparisons\")\n",
    "        \n",
    "        comparison_results[layer_name] = layer_results\n",
    "    \n",
    "    return comparison_results\n",
    "\n",
    "def run_comparison(text_input, n_neurons=5, n_tokens=3, layer_range=None):\n",
    "    print(f\"Processing: {text_input[:50]}...\")\n",
    "    \n",
    "    # Clear previous data\n",
    "    clear_activations()\n",
    "    remove_all_hooks()\n",
    "    \n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(text_input, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n",
    "    \n",
    "    print(f\"Input tokens: {inputs['input_ids'].shape[1]}\")\n",
    "    \n",
    "    try:\n",
    "        # Register hooks\n",
    "        print(\"Registering hooks...\")\n",
    "        hooks_1, layers_1 = register_llama_hooks(model_1, \"Model_1\", layer_range)\n",
    "        hooks_2, layers_2 = register_llama_hooks(model_2, \"Model_2\", layer_range)\n",
    "        \n",
    "        # Run both models\n",
    "        print(\"Running models...\")\n",
    "        with torch.no_grad():\n",
    "            outputs_1 = model_1(**inputs)\n",
    "            outputs_2 = model_2(**inputs)\n",
    "        \n",
    "        print(f\"Captured {len(activations_model_1)} activations from Model 1\")\n",
    "        print(f\"Captured {len(activations_model_2)} activations from Model 2\")\n",
    "        \n",
    "        # Check if we got any activations\n",
    "        valid_activations_1 = sum(1 for v in activations_model_1.values() \n",
    "                                if v.get('output') is not None)\n",
    "        valid_activations_2 = sum(1 for v in activations_model_2.values() \n",
    "                                if v.get('output') is not None)\n",
    "        \n",
    "        print(f\"Valid activations - Model 1: {valid_activations_1}, Model 2: {valid_activations_2}\")\n",
    "        \n",
    "        if valid_activations_1 == 0 or valid_activations_2 == 0:\n",
    "            print(\"❌ No valid activations captured!\")\n",
    "            return {\n",
    "                'input_text': text_input,\n",
    "                'error': 'No valid activations captured',\n",
    "                'layer_comparisons': {},\n",
    "                'selected_neurons': {'model_1': {}, 'model_2': {}}\n",
    "            }\n",
    "        \n",
    "        # Select random neurons\n",
    "        print(\"Selecting neurons...\")\n",
    "        selected_1 = select_random_neurons_llama(activations_model_1, n_neurons, n_tokens)\n",
    "        selected_2 = select_random_neurons_llama(activations_model_2, n_neurons, n_tokens)\n",
    "        \n",
    "        print(f\"Selected neurons from {len(selected_1)} layers\")\n",
    "        \n",
    "        if len(selected_1) == 0:\n",
    "            print(\"❌ No neurons selected!\")\n",
    "            return {\n",
    "                'input_text': text_input,\n",
    "                'error': 'No neurons selected',\n",
    "                'layer_comparisons': {},\n",
    "                'selected_neurons': {'model_1': selected_1, 'model_2': selected_2}\n",
    "            }\n",
    "        \n",
    "        # Compare activations\n",
    "        print(\"Comparing activations...\")\n",
    "        comparison_results = compare_selected_activations(\n",
    "            activations_model_1,\n",
    "            activations_model_2,\n",
    "            selected_1\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_text': text_input,\n",
    "            'tokenized_input': inputs,\n",
    "            'model_1_output': outputs_1.logits,\n",
    "            'model_2_output': outputs_2.logits,\n",
    "            'layer_comparisons': comparison_results,\n",
    "            'selected_neurons': {'model_1': selected_1, 'model_2': selected_2}\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in run_comparison: {e}\")\n",
    "        return {\n",
    "            'input_text': text_input,\n",
    "            'error': str(e),\n",
    "            'layer_comparisons': {},\n",
    "            'selected_neurons': {'model_1': {}, 'model_2': {}}\n",
    "        }\n",
    "    \n",
    "    finally:\n",
    "        # Always cleanup hooks\n",
    "        remove_all_hooks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e23253",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results_to_csv(comparison_results, input_id=0, filename=\"llama_comparison.csv\"):\n",
    "    rows = []\n",
    "    \n",
    "    layer_comparisons = comparison_results.get('layer_comparisons', {})\n",
    "    input_text = comparison_results.get('input_text', 'Unknown')\n",
    "    \n",
    "    for layer_name, layer_data in layer_comparisons.items():\n",
    "        for neuron_data in layer_data.get('neuron_comparisons', []):\n",
    "            row = {\n",
    "                'input_id': input_id,\n",
    "                'input_text': input_text[:100],  # Truncate for CSV\n",
    "                'layer_name': layer_name,\n",
    "                'component_type': layer_data['component_type'],\n",
    "                'token_idx': neuron_data['token_idx'],\n",
    "                'neuron_idx': neuron_data['neuron_idx'],\n",
    "                'actual_activation': neuron_data['actual_activation'],\n",
    "                'expected_activation': neuron_data['expected_activation'],\n",
    "                'difference': neuron_data['difference'],\n",
    "                'percent_difference': neuron_data['percent_difference'],\n",
    "                'layer_mean_difference': layer_data['mean_difference'],\n",
    "                'layer_max_difference': layer_data['max_difference'],\n",
    "                'layer_min_difference': layer_data['min_difference']\n",
    "            }\n",
    "            rows.append(row)\n",
    "    \n",
    "    df = pd.DataFrame(rows)\n",
    "    \n",
    "    # Save to CSV\n",
    "    file_exists = os.path.isfile(filename)\n",
    "    if file_exists:\n",
    "        df.to_csv(filename, mode='a', header=False, index=False)\n",
    "        print(f\"Results appended to {filename}\")\n",
    "    else:\n",
    "        df.to_csv(filename, index=False)\n",
    "        print(f\"Results saved to {filename}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_visualizations(comparison_results, save_path=\"llama_comparison.png\"):\n",
    "    layer_comparisons = comparison_results.get('layer_comparisons', {})\n",
    "    \n",
    "    if not layer_comparisons:\n",
    "        print(\"No layer comparisons found!\")\n",
    "        return\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    fig.suptitle('LLaMA Model Comparison Analysis', fontsize=16)\n",
    "    \n",
    "    # Prepare data\n",
    "    layer_names = []\n",
    "    component_types = []\n",
    "    mean_diffs = []\n",
    "    max_diffs = []\n",
    "    \n",
    "    for layer_name, results in layer_comparisons.items():\n",
    "        if results.get('neuron_comparisons'):\n",
    "            layer_names.append(layer_name)\n",
    "            component_types.append(results['component_type'])\n",
    "            mean_diffs.append(results['mean_difference'])\n",
    "            max_diffs.append(results['max_difference'])\n",
    "    \n",
    "    if not layer_names:\n",
    "        print(\"No valid comparisons found!\")\n",
    "        return\n",
    "    \n",
    "    # 1. Mean differences by layer\n",
    "    axes[0, 0].bar(range(len(layer_names)), mean_diffs)\n",
    "    axes[0, 0].set_title('Mean Activation Differences by Layer')\n",
    "    axes[0, 0].set_xlabel('Layer Index')\n",
    "    axes[0, 0].set_ylabel('Mean Difference')\n",
    "    axes[0, 0].set_xticks(range(len(layer_names)))\n",
    "    axes[0, 0].set_xticklabels([name.replace('layer_', 'L') for name in layer_names], rotation=45)\n",
    "    \n",
    "    # 2. Differences by component type\n",
    "    if component_types and mean_diffs:\n",
    "        component_df = pd.DataFrame({\n",
    "            'component_type': component_types,\n",
    "            'mean_difference': mean_diffs\n",
    "        })\n",
    "        sns.boxplot(data=component_df, x='component_type', y='mean_difference', ax=axes[0, 1])\n",
    "        axes[0, 1].set_title('Differences by Component Type')\n",
    "        axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 3. Max vs Mean differences\n",
    "    axes[0, 2].scatter(mean_diffs, max_diffs, alpha=0.6)\n",
    "    axes[0, 2].set_xlabel('Mean Difference')\n",
    "    axes[0, 2].set_ylabel('Max Difference')\n",
    "    axes[0, 2].set_title('Max vs Mean Differences')\n",
    "    \n",
    "    # 4. Token-level analysis\n",
    "    all_token_diffs = []\n",
    "    for layer_name, results in layer_comparisons.items():\n",
    "        for neuron_comp in results.get('neuron_comparisons', []):\n",
    "            all_token_diffs.append({\n",
    "                'token_idx': neuron_comp['token_idx'],\n",
    "                'difference': neuron_comp['difference'],\n",
    "                'layer': layer_name\n",
    "            })\n",
    "    \n",
    "    if all_token_diffs:\n",
    "        token_df = pd.DataFrame(all_token_diffs)\n",
    "        token_summary = token_df.groupby('token_idx')['difference'].mean()\n",
    "        axes[1, 0].plot(token_summary.index, token_summary.values, marker='o')\n",
    "        axes[1, 0].set_title('Mean Differences by Token Position')\n",
    "        axes[1, 0].set_xlabel('Token Position')\n",
    "        axes[1, 0].set_ylabel('Mean Difference')\n",
    "    \n",
    "    # 5. Layer depth vs difference\n",
    "    layer_numbers = []\n",
    "    layer_diffs = []\n",
    "    for i, name in enumerate(layer_names):\n",
    "        if 'layer_' in name:\n",
    "            try:\n",
    "                layer_num = int(name.split('_')[1])\n",
    "                layer_numbers.append(layer_num)\n",
    "                layer_diffs.append(mean_diffs[i])\n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "    if layer_numbers and layer_diffs:\n",
    "        axes[1, 1].plot(layer_numbers, layer_diffs, marker='o')\n",
    "        axes[1, 1].set_title('Differences by Layer Depth')\n",
    "        axes[1, 1].set_xlabel('Layer Number')\n",
    "        axes[1, 1].set_ylabel('Mean Difference')\n",
    "    \n",
    "    # 6. Distribution of all differences\n",
    "    all_diffs = []\n",
    "    for layer_name, results in layer_comparisons.items():\n",
    "        for neuron_comp in results.get('neuron_comparisons', []):\n",
    "            all_diffs.append(neuron_comp['difference'])\n",
    "    \n",
    "    if all_diffs:\n",
    "        axes[1, 2].hist(all_diffs, bins=30, alpha=0.7)\n",
    "        axes[1, 2].set_title('Distribution of All Differences')\n",
    "        axes[1, 2].set_xlabel('Difference')\n",
    "        axes[1, 2].set_ylabel('Frequency')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"Visualization saved to {save_path}\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbfda25",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_TEXTS = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Artificial intelligence is transforming the world of technology.\",\n",
    "    \"In a hole in the ground there lived a hobbit.\",\n",
    "    \"To be or not to be, that is the question Shakespeare posed.\",\n",
    "    \"Machine learning models require large datasets for training.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb31afe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing text 1/5 ===\n",
      "Processing: The quick brown fox jumps over the lazy dog....\n",
      "Input tokens: 13\n",
      "Registering hooks...\n",
      "Running models...\n",
      "Error in run_comparison: 'NoneType' object has no attribute 'detach'\n",
      "Results saved to llama_comparison_results.csv\n",
      "No layer comparisons found!\n",
      "Completed text 1\n",
      "\n",
      "=== Processing text 2/5 ===\n",
      "Processing: Artificial intelligence is transforming the world ...\n",
      "Input tokens: 13\n",
      "Registering hooks...\n",
      "Running models...\n",
      "Error in run_comparison: 'NoneType' object has no attribute 'detach'\n",
      "Results appended to llama_comparison_results.csv\n",
      "No layer comparisons found!\n",
      "Completed text 2\n",
      "\n",
      "=== Processing text 3/5 ===\n",
      "Processing: In a hole in the ground there lived a hobbit....\n",
      "Input tokens: 14\n",
      "Registering hooks...\n",
      "Running models...\n",
      "Error in run_comparison: 'NoneType' object has no attribute 'detach'\n",
      "Results appended to llama_comparison_results.csv\n",
      "No layer comparisons found!\n",
      "Completed text 3\n",
      "\n",
      "=== Processing text 4/5 ===\n",
      "Processing: To be or not to be, that is the question Shakespea...\n",
      "Input tokens: 16\n",
      "Registering hooks...\n",
      "Running models...\n",
      "Error in run_comparison: 'NoneType' object has no attribute 'detach'\n",
      "Results appended to llama_comparison_results.csv\n",
      "No layer comparisons found!\n",
      "Completed text 4\n",
      "\n",
      "=== Processing text 5/5 ===\n",
      "Processing: Machine learning models require large datasets for...\n",
      "Input tokens: 10\n",
      "Registering hooks...\n",
      "Running models...\n",
      "Error in run_comparison: 'NoneType' object has no attribute 'detach'\n",
      "Results appended to llama_comparison_results.csv\n",
      "No layer comparisons found!\n",
      "Completed text 5\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "all_results = []\n",
    "\n",
    "for i, text in enumerate(TEST_TEXTS):\n",
    "    print(f\"\\n=== Processing text {i+1}/{len(TEST_TEXTS)} ===\")\n",
    "    \n",
    "    try:\n",
    "        result = run_comparison(\n",
    "            text,\n",
    "            n_neurons=1,\n",
    "            n_tokens=3,\n",
    "            layer_range=None\n",
    "        )\n",
    "        \n",
    "        all_results.append(result)\n",
    "        \n",
    "        # Save to CSV\n",
    "        save_results_to_csv(result, input_id=i, filename=\"llama_comparison_results.csv\")\n",
    "        \n",
    "        # Create visualization\n",
    "        create_visualizations(result, save_path=f'llama_comparison_{i}.png')\n",
    "        \n",
    "        print(f\"Completed text {i+1}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing text {i+1}: {e}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d90a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "if all_results:\n",
    "    print(\"\\n=== SUMMARY STATISTICS ===\")\n",
    "    \n",
    "    all_layer_stats = defaultdict(list)\n",
    "    all_component_stats = defaultdict(list)\n",
    "    \n",
    "    for result in all_results:\n",
    "        for layer_name, layer_data in result['layer_comparisons'].items():\n",
    "            if not np.isnan(layer_data['mean_difference']):\n",
    "                all_layer_stats[layer_name].append(layer_data['mean_difference'])\n",
    "                all_component_stats[layer_data['component_type']].append(layer_data['mean_difference'])\n",
    "    \n",
    "    print(\"Average differences by layer:\")\n",
    "    for layer, diffs in all_layer_stats.items():\n",
    "        print(f\"  {layer}: {np.mean(diffs):.6f} ± {np.std(diffs):.6f}\")\n",
    "    \n",
    "    print(\"\\nAverage differences by component type:\")\n",
    "    for component, diffs in all_component_stats.items():\n",
    "        print(f\"  {component}: {np.mean(diffs):.6f} ± {np.std(diffs):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219db777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 TESTING SIMPLIFIED APPROACH\n",
      "==================================================\n",
      "\n",
      "============================================================\n",
      "🧪 SIMPLE COMPARISON: Hello...\n",
      "============================================================\n",
      "1️⃣  Tokenizing input...\n",
      "   ✅ Tokenized: 2 tokens\n",
      "2️⃣  Registering hooks...\n",
      "Registering hooks for Model_1 (max 1 layers)...\n",
      "  ✅ Registered hooks for layer 0\n",
      "✅ Successfully registered 2 hooks for Model_1\n",
      "Registering hooks for Model_2 (max 1 layers)...\n",
      "  ✅ Registered hooks for layer 0\n",
      "✅ Successfully registered 2 hooks for Model_2\n",
      "   ✅ Registered 2 + 2 hooks\n",
      "3️⃣  Running models...\n",
      "   🔄 Running Model 1...\n",
      "   ❌ Model execution error: 'NoneType' object has no attribute 'detach'\n",
      "❌ Test failed: Model execution failed: 'NoneType' object has no attribute 'detach'\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Clear everything and start fresh\n",
    "activations_model_1 = {}\n",
    "activations_model_2 = {}\n",
    "current_hooks = []\n",
    "\n",
    "def clear_activations():\n",
    "    global activations_model_1, activations_model_2\n",
    "    activations_model_1.clear()\n",
    "    activations_model_2.clear()\n",
    "\n",
    "def remove_all_hooks():\n",
    "    global current_hooks\n",
    "    for hook in current_hooks:\n",
    "        try:\n",
    "            hook.remove()\n",
    "        except:\n",
    "            pass\n",
    "    current_hooks.clear()\n",
    "\n",
    "# %%\n",
    "def safe_detach_cpu(tensor):\n",
    "    \"\"\"Safely detach and move tensor to CPU\"\"\"\n",
    "    if tensor is None:\n",
    "        return None\n",
    "    try:\n",
    "        if isinstance(tensor, torch.Tensor):\n",
    "            return tensor.detach().cpu()\n",
    "        else:\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error in safe_detach_cpu: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_simple_activation_hook(name, model_name):\n",
    "    \"\"\"Simplified hook with maximum safety\"\"\"\n",
    "    def hook(module, input, output):\n",
    "        try:\n",
    "            # Initialize with None values\n",
    "            activation_data = {\n",
    "                'output': None,\n",
    "                'input': None,\n",
    "                'weight': None,\n",
    "                'bias': None\n",
    "            }\n",
    "            \n",
    "            # Handle output safely\n",
    "            if output is not None:\n",
    "                if isinstance(output, tuple) and len(output) > 0:\n",
    "                    activation_data['output'] = safe_detach_cpu(output[0])\n",
    "                else:\n",
    "                    activation_data['output'] = safe_detach_cpu(output)\n",
    "            \n",
    "            # Handle input safely\n",
    "            if input is not None:\n",
    "                if isinstance(input, tuple) and len(input) > 0:\n",
    "                    activation_data['input'] = safe_detach_cpu(input[0])\n",
    "                else:\n",
    "                    activation_data['input'] = safe_detach_cpu(input)\n",
    "            \n",
    "            # Handle weights and bias safely\n",
    "            if hasattr(module, 'weight'):\n",
    "                activation_data['weight'] = safe_detach_cpu(module.weight)\n",
    "            \n",
    "            if hasattr(module, 'bias'):\n",
    "                activation_data['bias'] = safe_detach_cpu(module.bias)\n",
    "            \n",
    "            # Store the data\n",
    "            if model_name == \"Model_1\":\n",
    "                activations_model_1[name] = activation_data\n",
    "            else:\n",
    "                activations_model_2[name] = activation_data\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Hook error in {name}: {e}\")\n",
    "            # Store empty data to prevent KeyError\n",
    "            empty_data = {'output': None, 'input': None, 'weight': None, 'bias': None}\n",
    "            if model_name == \"Model_1\":\n",
    "                activations_model_1[name] = empty_data\n",
    "            else:\n",
    "                activations_model_2[name] = empty_data\n",
    "            \n",
    "    return hook\n",
    "\n",
    "# %%\n",
    "def register_safe_hooks(model, model_name, max_layers=3):\n",
    "    \"\"\"Register hooks with maximum safety and limited layers\"\"\"\n",
    "    global current_hooks\n",
    "    hooks = []\n",
    "    \n",
    "    print(f\"Registering hooks for {model_name} (max {max_layers} layers)...\")\n",
    "    \n",
    "    try:\n",
    "        # Only register a few key hooks first\n",
    "        for i in range(min(max_layers, len(model.model.layers))):\n",
    "            layer = model.model.layers[i]\n",
    "            layer_prefix = f\"layer_{i}\"\n",
    "            \n",
    "            # Only register the most important hooks\n",
    "            hooks.append(layer.self_attn.q_proj.register_forward_hook(\n",
    "                get_simple_activation_hook(f\"{layer_prefix}_q_proj\", model_name)\n",
    "            ))\n",
    "            \n",
    "            hooks.append(layer.mlp.gate_proj.register_forward_hook(\n",
    "                get_simple_activation_hook(f\"{layer_prefix}_gate_proj\", model_name)\n",
    "            ))\n",
    "            \n",
    "            print(f\"  ✅ Registered hooks for layer {i}\")\n",
    "        \n",
    "        current_hooks.extend(hooks)\n",
    "        print(f\"✅ Successfully registered {len(hooks)} hooks for {model_name}\")\n",
    "        return hooks\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error registering hooks for {model_name}: {e}\")\n",
    "        # Clean up any partial hooks\n",
    "        for hook in hooks:\n",
    "            try:\n",
    "                hook.remove()\n",
    "            except:\n",
    "                pass\n",
    "        return []\n",
    "\n",
    "# %%\n",
    "def validate_activations():\n",
    "    \"\"\"Check if we have valid activation data\"\"\"\n",
    "    print(\"\\n=== ACTIVATION VALIDATION ===\")\n",
    "    \n",
    "    print(f\"Model 1 captured layers: {len(activations_model_1)}\")\n",
    "    print(f\"Model 2 captured layers: {len(activations_model_2)}\")\n",
    "    \n",
    "    valid_1 = 0\n",
    "    valid_2 = 0\n",
    "    \n",
    "    for name, data in activations_model_1.items():\n",
    "        if data['output'] is not None:\n",
    "            valid_1 += 1\n",
    "            print(f\"  ✅ {name}: {data['output'].shape}\")\n",
    "        else:\n",
    "            print(f\"  ❌ {name}: No output\")\n",
    "    \n",
    "    for name, data in activations_model_2.items():\n",
    "        if data['output'] is not None:\n",
    "            valid_2 += 1\n",
    "    \n",
    "    print(f\"Valid activations - Model 1: {valid_1}, Model 2: {valid_2}\")\n",
    "    return valid_1 > 0 and valid_2 > 0\n",
    "\n",
    "# %%\n",
    "def simple_run_comparison(text_input, max_layers=2):\n",
    "    \"\"\"Simplified comparison with extensive debugging\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"🧪 SIMPLE COMPARISON: {text_input[:30]}...\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Step 1: Clear and prepare\n",
    "    clear_activations()\n",
    "    remove_all_hooks()\n",
    "    \n",
    "    # Step 2: Tokenize\n",
    "    print(\"1️⃣  Tokenizing input...\")\n",
    "    try:\n",
    "        inputs = tokenizer(text_input, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n",
    "        print(f\"   ✅ Tokenized: {inputs['input_ids'].shape[1]} tokens\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ Tokenization error: {e}\")\n",
    "        return {'error': f'Tokenization failed: {e}'}\n",
    "    \n",
    "    # Step 3: Register hooks\n",
    "    print(\"2️⃣  Registering hooks...\")\n",
    "    try:\n",
    "        hooks_1 = register_safe_hooks(model_1, \"Model_1\", max_layers)\n",
    "        hooks_2 = register_safe_hooks(model_2, \"Model_2\", max_layers)\n",
    "        \n",
    "        if len(hooks_1) == 0 or len(hooks_2) == 0:\n",
    "            print(\"   ❌ Hook registration failed\")\n",
    "            return {'error': 'Hook registration failed'}\n",
    "        \n",
    "        print(f\"   ✅ Registered {len(hooks_1)} + {len(hooks_2)} hooks\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ Hook registration error: {e}\")\n",
    "        return {'error': f'Hook registration failed: {e}'}\n",
    "    \n",
    "    # Step 4: Run models\n",
    "    print(\"3️⃣  Running models...\")\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            print(\"   🔄 Running Model 1...\")\n",
    "            outputs_1 = model_1(**inputs)\n",
    "            print(\"   🔄 Running Model 2...\")\n",
    "            outputs_2 = model_2(**inputs)\n",
    "        \n",
    "        print(\"   ✅ Both models completed\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ Model execution error: {e}\")\n",
    "        remove_all_hooks()\n",
    "        return {'error': f'Model execution failed: {e}'}\n",
    "    \n",
    "    # Step 5: Validate results\n",
    "    print(\"4️⃣  Validating activations...\")\n",
    "    validation_success = validate_activations()\n",
    "    \n",
    "    if not validation_success:\n",
    "        print(\"   ❌ Activation validation failed\")\n",
    "        remove_all_hooks()\n",
    "        return {'error': 'No valid activations captured'}\n",
    "    \n",
    "    # Step 6: Basic comparison\n",
    "    print(\"5️⃣  Performing basic comparison...\")\n",
    "    comparison_results = {}\n",
    "    \n",
    "    try:\n",
    "        for layer_name in activations_model_1:\n",
    "            if layer_name in activations_model_2:\n",
    "                data_1 = activations_model_1[layer_name]\n",
    "                data_2 = activations_model_2[layer_name]\n",
    "                \n",
    "                if data_1['output'] is not None and data_2['output'] is not None:\n",
    "                    # Simple comparison: just compute mean absolute difference\n",
    "                    diff = torch.mean(torch.abs(data_1['output'] - data_2['output']))\n",
    "                    \n",
    "                    comparison_results[layer_name] = {\n",
    "                        'mean_difference': diff.item(),\n",
    "                        'shape_1': data_1['output'].shape,\n",
    "                        'shape_2': data_2['output'].shape,\n",
    "                        'component_type': 'attention' if 'q_proj' in layer_name else 'mlp'\n",
    "                    }\n",
    "                    \n",
    "                    print(f\"   ✅ {layer_name}: diff = {diff.item():.6f}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ Comparison error: {e}\")\n",
    "        remove_all_hooks()\n",
    "        return {'error': f'Comparison failed: {e}'}\n",
    "    \n",
    "    # Cleanup\n",
    "    remove_all_hooks()\n",
    "    \n",
    "    print(f\"✅ Completed successfully! Found {len(comparison_results)} valid comparisons\")\n",
    "    \n",
    "    return {\n",
    "        'input_text': text_input,\n",
    "        'layer_comparisons': comparison_results,\n",
    "        'success': True\n",
    "    }\n",
    "\n",
    "# %%\n",
    "# Test the simplified version\n",
    "print(\"🧪 TESTING SIMPLIFIED APPROACH\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Test with very simple input first\n",
    "test_result = simple_run_comparison(\"Hello\", max_layers=1)\n",
    "\n",
    "if 'error' in test_result:\n",
    "    print(f\"❌ Test failed: {test_result['error']}\")\n",
    "else:\n",
    "    print(\"✅ Test passed! Moving to full tests...\")\n",
    "    \n",
    "    # Now test with your original texts\n",
    "    TEST_TEXTS = [\n",
    "        \"The quick brown fox jumps over the lazy dog.\",\n",
    "        \"Hello world this is a test.\"\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i, text in enumerate(TEST_TEXTS[:2]):  # Only test first 2\n",
    "        print(f\"\\n=== Testing text {i+1} ===\")\n",
    "        \n",
    "        result = simple_run_comparison(text, max_layers=3)\n",
    "        \n",
    "        if 'error' not in result:\n",
    "            results.append(result)\n",
    "            print(f\"✅ Success! {len(result['layer_comparisons'])} comparisons\")\n",
    "            \n",
    "            # Print some results\n",
    "            for layer, data in result['layer_comparisons'].items():\n",
    "                print(f\"  {layer}: {data['mean_difference']:.8f}\")\n",
    "        else:\n",
    "            print(f\"❌ Failed: {result['error']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfdf1e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔬 MINIMAL TEST: 'Hi'\n",
      "========================================\n",
      "✅ Tokenized: torch.Size([1, 2])\n",
      "🔒 Registering ultra-safe hooks for Model_1...\n",
      "   Testing q_proj hook...\n",
      "   ✅ Registered 1 ultra-safe hooks\n",
      "🔄 Running model with hooks...\n",
      "❌ Minimal test failed: 'NoneType' object has no attribute 'detach'\n",
      "\n",
      "🎯 RESULT: Failed: 'NoneType' object has no attribute 'detach'\n",
      "\n",
      "❌ Need to debug further...\n",
      "\n",
      "🧪 Testing model execution WITHOUT hooks...\n",
      "❌ Model fails even without hooks: 'NoneType' object has no attribute 'detach'\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Ultra-defensive hook function\n",
    "def ultra_safe_hook(name, model_name):\n",
    "    \"\"\"Hook that cannot fail under any circumstances\"\"\"\n",
    "    def hook(module, input, output):\n",
    "        try:\n",
    "            # Initialize result\n",
    "            result = {\n",
    "                'output': None,\n",
    "                'input': None, \n",
    "                'weight': None,\n",
    "                'bias': None,\n",
    "                'error': None\n",
    "            }\n",
    "            \n",
    "            # Handle output with maximum safety\n",
    "            try:\n",
    "                if output is not None:\n",
    "                    if hasattr(output, '__len__') and len(output) > 0:\n",
    "                        # It's a tuple/list\n",
    "                        first_item = output[0]\n",
    "                        if first_item is not None and hasattr(first_item, 'detach'):\n",
    "                            result['output'] = first_item.detach().cpu()\n",
    "                    elif hasattr(output, 'detach'):\n",
    "                        # It's a tensor\n",
    "                        result['output'] = output.detach().cpu()\n",
    "            except Exception as e:\n",
    "                result['error'] = f\"Output error: {e}\"\n",
    "            \n",
    "            # Handle input with maximum safety\n",
    "            try:\n",
    "                if input is not None:\n",
    "                    if hasattr(input, '__len__') and len(input) > 0:\n",
    "                        first_item = input[0]\n",
    "                        if first_item is not None and hasattr(first_item, 'detach'):\n",
    "                            result['input'] = first_item.detach().cpu()\n",
    "                    elif hasattr(input, 'detach'):\n",
    "                        result['input'] = input.detach().cpu()\n",
    "            except Exception as e:\n",
    "                if result['error']:\n",
    "                    result['error'] += f\"; Input error: {e}\"\n",
    "                else:\n",
    "                    result['error'] = f\"Input error: {e}\"\n",
    "            \n",
    "            # Handle module parameters with maximum safety\n",
    "            try:\n",
    "                if hasattr(module, 'weight') and module.weight is not None:\n",
    "                    if hasattr(module.weight, 'detach'):\n",
    "                        result['weight'] = module.weight.detach().cpu()\n",
    "            except Exception as e:\n",
    "                if result['error']:\n",
    "                    result['error'] += f\"; Weight error: {e}\"\n",
    "                else:\n",
    "                    result['error'] = f\"Weight error: {e}\"\n",
    "            \n",
    "            try:\n",
    "                if hasattr(module, 'bias') and module.bias is not None:\n",
    "                    if hasattr(module.bias, 'detach'):\n",
    "                        result['bias'] = module.bias.detach().cpu()\n",
    "            except Exception as e:\n",
    "                if result['error']:\n",
    "                    result['error'] += f\"; Bias error: {e}\"\n",
    "                else:\n",
    "                    result['error'] = f\"Bias error: {e}\"\n",
    "            \n",
    "            # Store result\n",
    "            if model_name == \"Model_1\":\n",
    "                activations_model_1[name] = result\n",
    "            else:\n",
    "                activations_model_2[name] = result\n",
    "                \n",
    "        except Exception as e:\n",
    "            # Ultimate fallback - store error info\n",
    "            error_result = {\n",
    "                'output': None,\n",
    "                'input': None,\n",
    "                'weight': None,\n",
    "                'bias': None,\n",
    "                'error': f\"Hook completely failed: {e}\"\n",
    "            }\n",
    "            \n",
    "            if model_name == \"Model_1\":\n",
    "                activations_model_1[name] = error_result\n",
    "            else:\n",
    "                activations_model_2[name] = error_result\n",
    "            \n",
    "            print(f\"🚨 HOOK FAILURE in {name}: {e}\")\n",
    "    \n",
    "    return hook\n",
    "\n",
    "# %%\n",
    "# Even safer hook registration\n",
    "def register_ultra_safe_hooks(model, model_name, max_layers=1):\n",
    "    \"\"\"Register hooks with ultimate safety\"\"\"\n",
    "    global current_hooks\n",
    "    hooks = []\n",
    "    \n",
    "    print(f\"🔒 Registering ultra-safe hooks for {model_name}...\")\n",
    "    \n",
    "    try:\n",
    "        # Start with just ONE hook to test\n",
    "        layer_0 = model.model.layers[0]\n",
    "        \n",
    "        # Try the safest possible hook first\n",
    "        print(\"   Testing q_proj hook...\")\n",
    "        hook = layer_0.self_attn.q_proj.register_forward_hook(\n",
    "            ultra_safe_hook(\"layer_0_q_proj\", model_name)\n",
    "        )\n",
    "        hooks.append(hook)\n",
    "        \n",
    "        current_hooks.extend(hooks)\n",
    "        print(f\"   ✅ Registered {len(hooks)} ultra-safe hooks\")\n",
    "        return hooks\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ Ultra-safe hook registration failed: {e}\")\n",
    "        for hook in hooks:\n",
    "            try:\n",
    "                hook.remove()\n",
    "            except:\n",
    "                pass\n",
    "        return []\n",
    "\n",
    "# %%\n",
    "# Test with absolute minimum\n",
    "def minimal_test(text=\"Hi\"):\n",
    "    \"\"\"Most minimal test possible\"\"\"\n",
    "    print(f\"\\n🔬 MINIMAL TEST: '{text}'\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    # Clear everything\n",
    "    clear_activations()\n",
    "    remove_all_hooks()\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Tokenize\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n",
    "        print(f\"✅ Tokenized: {inputs['input_ids'].shape}\")\n",
    "        \n",
    "        # Step 2: Register ONE hook\n",
    "        hooks_1 = register_ultra_safe_hooks(model_1, \"Model_1\", max_layers=1)\n",
    "        if len(hooks_1) == 0:\n",
    "            return \"Hook registration failed\"\n",
    "        \n",
    "        # Step 3: Run model with hooks\n",
    "        print(\"🔄 Running model with hooks...\")\n",
    "        with torch.no_grad():\n",
    "            output = model_1(**inputs)\n",
    "        \n",
    "        print(\"✅ Model execution completed!\")\n",
    "        \n",
    "        # Step 4: Check what we got\n",
    "        print(f\"📊 Captured {len(activations_model_1)} activations\")\n",
    "        for name, data in activations_model_1.items():\n",
    "            if data['error']:\n",
    "                print(f\"   ❌ {name}: {data['error']}\")\n",
    "            else:\n",
    "                print(f\"   ✅ {name}: output={data['output'] is not None}, input={data['input'] is not None}\")\n",
    "                if data['output'] is not None:\n",
    "                    print(f\"      Output shape: {data['output'].shape}\")\n",
    "        \n",
    "        return \"SUCCESS\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Minimal test failed: {e}\")\n",
    "        return f\"Failed: {e}\"\n",
    "    \n",
    "    finally:\n",
    "        remove_all_hooks()\n",
    "\n",
    "# %%\n",
    "# Run the minimal test\n",
    "result = minimal_test(\"Hi\")\n",
    "print(f\"\\n🎯 RESULT: {result}\")\n",
    "\n",
    "if result == \"SUCCESS\":\n",
    "    print(\"\\n✅ Minimal test passed! Now testing with both models...\")\n",
    "    \n",
    "    # Test with both models\n",
    "    def dual_minimal_test():\n",
    "        clear_activations()\n",
    "        remove_all_hooks()\n",
    "        \n",
    "        try:\n",
    "            inputs = tokenizer(\"Hello\", return_tensors=\"pt\")\n",
    "            inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n",
    "            \n",
    "            # Register hooks for both models\n",
    "            hooks_1 = register_ultra_safe_hooks(model_1, \"Model_1\")\n",
    "            hooks_2 = register_ultra_safe_hooks(model_2, \"Model_2\")\n",
    "            \n",
    "            if len(hooks_1) == 0 or len(hooks_2) == 0:\n",
    "                return \"Dual hook registration failed\"\n",
    "            \n",
    "            # Run both models\n",
    "            with torch.no_grad():\n",
    "                print(\"🔄 Running Model 1...\")\n",
    "                out1 = model_1(**inputs)\n",
    "                print(\"🔄 Running Model 2...\")\n",
    "                out2 = model_2(**inputs)\n",
    "            \n",
    "            print(\"✅ Both models completed!\")\n",
    "            \n",
    "            # Check results\n",
    "            print(f\"Model 1 activations: {len(activations_model_1)}\")\n",
    "            print(f\"Model 2 activations: {len(activations_model_2)}\")\n",
    "            \n",
    "            # Simple comparison\n",
    "            if len(activations_model_1) > 0 and len(activations_model_2) > 0:\n",
    "                name = list(activations_model_1.keys())[0]\n",
    "                data1 = activations_model_1[name]\n",
    "                data2 = activations_model_2[name]\n",
    "                \n",
    "                if data1['output'] is not None and data2['output'] is not None:\n",
    "                    diff = torch.mean(torch.abs(data1['output'] - data2['output']))\n",
    "                    print(f\"✅ Difference in {name}: {diff.item():.8f}\")\n",
    "                    return \"DUAL SUCCESS\"\n",
    "            \n",
    "            return \"Partial success\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"Dual test failed: {e}\"\n",
    "        finally:\n",
    "            remove_all_hooks()\n",
    "    \n",
    "    dual_result = dual_minimal_test()\n",
    "    print(f\"\\n🎯 DUAL RESULT: {dual_result}\")\n",
    "else:\n",
    "    print(\"\\n❌ Need to debug further...\")\n",
    "    \n",
    "    # Let's try without any hooks first\n",
    "    print(\"\\n🧪 Testing model execution WITHOUT hooks...\")\n",
    "    try:\n",
    "        inputs = tokenizer(\"Hi\", return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model_1(**inputs)\n",
    "        \n",
    "        print(\"✅ Model runs fine without hooks!\")\n",
    "        print(f\"Output shape: {output.logits.shape}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Model fails even without hooks: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bc6c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 COMPREHENSIVE MODEL DEBUG\n",
      "==================================================\n",
      "1️⃣ MODEL INSPECTION:\n",
      "Model 1 type: <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'>\n",
      "Model 1 device: cpu\n",
      "Model 1 dtype: torch.float32\n",
      "Model 1 training mode: False\n",
      "\n",
      "Model 2 type: <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'>\n",
      "Model 2 device: cpu\n",
      "Model 2 dtype: torch.float32\n",
      "Model 2 training mode: False\n",
      "\n",
      "2️⃣ TOKENIZER INSPECTION:\n",
      "Tokenizer type: <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>\n",
      "✅ 'Hi' -> torch.Size([1, 2]), tokens: [1, 6324]\n",
      "✅ 'Hello' -> torch.Size([1, 2]), tokens: [1, 15043]\n",
      "✅ 'Test' -> torch.Size([1, 2]), tokens: [1, 4321]\n",
      "\n",
      "3️⃣ DEVICE AND MEMORY CHECK:\n",
      "CUDA available: False\n",
      "Current device: cpu\n",
      "\n",
      "4️⃣ STEP-BY-STEP MODEL EXECUTION:\n",
      "\n",
      "🔍 Debugging Model_1:\n",
      "   ✅ Set to eval mode\n",
      "   🔄 Testing with: 'Hi'\n",
      "   ✅ Tokenized: KeysView({'input_ids': tensor([[   1, 6324]]), 'attention_mask': tensor([[1, 1]])})\n",
      "      Input IDs: tensor([[   1, 6324]])\n",
      "      Input shape: torch.Size([1, 2])\n",
      "   ✅ Moved to device: cpu\n",
      "      input_ids: torch.Size([1, 2]), device: cpu, dtype: torch.int64\n",
      "      attention_mask: torch.Size([1, 2]), device: cpu, dtype: torch.int64\n",
      "   🔄 Executing model...\n",
      "   ✅ Got embeddings: torch.Size([1, 2, 4096])\n",
      "   🔄 Testing first layer...\n",
      "   ❌ Model execution failed: 'NoneType' object has no attribute 'detach'\n",
      "   📍 Error type: <class 'AttributeError'>\n",
      "   📋 Full traceback:\n",
      "\n",
      "🔍 Debugging Model_2:\n",
      "   ✅ Set to eval mode\n",
      "   🔄 Testing with: 'Hi'\n",
      "   ✅ Tokenized: KeysView({'input_ids': tensor([[   1, 6324]]), 'attention_mask': tensor([[1, 1]])})\n",
      "      Input IDs: tensor([[   1, 6324]])\n",
      "      Input shape: torch.Size([1, 2])\n",
      "   ✅ Moved to device: cpu\n",
      "      input_ids: torch.Size([1, 2]), device: cpu, dtype: torch.int64\n",
      "      attention_mask: torch.Size([1, 2]), device: cpu, dtype: torch.int64\n",
      "   🔄 Executing model...\n",
      "   ✅ Got embeddings: torch.Size([1, 2, 4096])\n",
      "   🔄 Testing first layer...\n",
      "   ❌ Model execution failed: 'NoneType' object has no attribute 'detach'\n",
      "   📍 Error type: <class 'AttributeError'>\n",
      "   📋 Full traceback:\n",
      "\n",
      "🎯 RESULTS:\n",
      "Model 1: FAILED: 'NoneType' object has no attribute 'detach'\n",
      "Model 2: FAILED: 'NoneType' object has no attribute 'detach'\n",
      "\n",
      "5️⃣ ALTERNATIVE TOKENIZATION TEST:\n",
      "\n",
      "Approach 1: {'padding': False, 'truncation': False}\n",
      "   Input shape: torch.Size([1, 2])\n",
      "   ❌ Failed: 'NoneType' object has no attribute 'detach'\n",
      "\n",
      "Approach 2: {'padding': True, 'truncation': True, 'max_length': 512}\n",
      "   Input shape: torch.Size([1, 2])\n",
      "   ❌ Failed: 'NoneType' object has no attribute 'detach'\n",
      "\n",
      "Approach 3: {'padding': 'max_length', 'truncation': True, 'max_length': 10}\n",
      "   Input shape: torch.Size([1, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\hskay\\AppData\\Local\\Temp\\ipykernel_37844\\1340711730.py\", line 85, in debug_model_execution\n",
      "    layer_output = layer_0(hidden_states)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\hskay\\anaconda3\\Lib\\site-packages\\transformers\\modeling_layers.py\", line 94, in __call__\n",
      "    return super().__call__(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\hskay\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1751, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\hskay\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1762, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\hskay\\anaconda3\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py\", line 289, in forward\n",
      "    hidden_states, _ = self.self_attn(\n",
      "                       ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\hskay\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1751, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\hskay\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1762, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\hskay\\anaconda3\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py\", line 232, in forward\n",
      "    query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\hskay\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1751, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\hskay\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1857, in _call_impl\n",
      "    return inner()\n",
      "           ^^^^^^^\n",
      "  File \"c:\\Users\\hskay\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1818, in inner\n",
      "    hook_result = hook(self, args, result)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\hskay\\AppData\\Local\\Temp\\ipykernel_37844\\4176242025.py\", line 23, in hook\n",
      "    'bias': module.bias.detach().cpu() if hasattr(module, 'bias') else None\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'NoneType' object has no attribute 'detach'\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\hskay\\AppData\\Local\\Temp\\ipykernel_37844\\1340711730.py\", line 85, in debug_model_execution\n",
      "    layer_output = layer_0(hidden_states)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\hskay\\anaconda3\\Lib\\site-packages\\transformers\\modeling_layers.py\", line 94, in __call__\n",
      "    return super().__call__(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\hskay\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1751, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\hskay\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1762, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\hskay\\anaconda3\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py\", line 289, in forward\n",
      "    hidden_states, _ = self.self_attn(\n",
      "                       ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\hskay\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1751, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\hskay\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1762, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\hskay\\anaconda3\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py\", line 232, in forward\n",
      "    query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\hskay\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1751, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\hskay\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1857, in _call_impl\n",
      "    return inner()\n",
      "           ^^^^^^^\n",
      "  File \"c:\\Users\\hskay\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1818, in inner\n",
      "    hook_result = hook(self, args, result)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\hskay\\AppData\\Local\\Temp\\ipykernel_37844\\4176242025.py\", line 23, in hook\n",
      "    'bias': module.bias.detach().cpu() if hasattr(module, 'bias') else None\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'NoneType' object has no attribute 'detach'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ❌ Failed: 'NoneType' object has no attribute 'detach'\n",
      "\n",
      "6️⃣ MEMORY AND STATE CHECK:\n",
      "Model 1 parameters require grad: True\n",
      "Model 2 parameters require grad: True\n",
      "\n",
      "🔄 Trying to reload model...\n",
      "Model loading appears to be working (skipping actual reload)\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "print(\"🔍 COMPREHENSIVE MODEL DEBUG\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Check basic model properties\n",
    "print(\"1️⃣ MODEL INSPECTION:\")\n",
    "print(f\"Model 1 type: {type(model_1)}\")\n",
    "print(f\"Model 1 device: {next(model_1.parameters()).device}\")\n",
    "print(f\"Model 1 dtype: {next(model_1.parameters()).dtype}\")\n",
    "print(f\"Model 1 training mode: {model_1.training}\")\n",
    "\n",
    "print(f\"\\nModel 2 type: {type(model_2)}\")\n",
    "print(f\"Model 2 device: {next(model_2.parameters()).device}\")\n",
    "print(f\"Model 2 dtype: {next(model_2.parameters()).dtype}\")\n",
    "print(f\"Model 2 training mode: {model_2.training}\")\n",
    "\n",
    "# %%\n",
    "print(\"\\n2️⃣ TOKENIZER INSPECTION:\")\n",
    "print(f\"Tokenizer type: {type(tokenizer)}\")\n",
    "\n",
    "# Test tokenizer with various inputs\n",
    "test_inputs = [\"Hi\", \"Hello\", \"Test\"]\n",
    "for text in test_inputs:\n",
    "    try:\n",
    "        tokens = tokenizer(text, return_tensors=\"pt\")\n",
    "        print(f\"✅ '{text}' -> {tokens['input_ids'].shape}, tokens: {tokens['input_ids'][0].tolist()}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ '{text}' tokenization failed: {e}\")\n",
    "\n",
    "# %%\n",
    "print(\"\\n3️⃣ DEVICE AND MEMORY CHECK:\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"Current device: {DEVICE}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU memory allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "    print(f\"GPU memory cached: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n",
    "\n",
    "# %%\n",
    "print(\"\\n4️⃣ STEP-BY-STEP MODEL EXECUTION:\")\n",
    "\n",
    "def debug_model_execution(model, model_name):\n",
    "    \"\"\"Debug model execution step by step\"\"\"\n",
    "    print(f\"\\n🔍 Debugging {model_name}:\")\n",
    "    \n",
    "    try:\n",
    "        # Set model to eval mode\n",
    "        model.eval()\n",
    "        print(\"   ✅ Set to eval mode\")\n",
    "        \n",
    "        # Create simple input\n",
    "        text = \"Hi\"\n",
    "        print(f\"   🔄 Testing with: '{text}'\")\n",
    "        \n",
    "        # Tokenize\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", padding=False, truncation=False)\n",
    "        print(f\"   ✅ Tokenized: {inputs.keys()}\")\n",
    "        print(f\"      Input IDs: {inputs['input_ids']}\")\n",
    "        print(f\"      Input shape: {inputs['input_ids'].shape}\")\n",
    "        \n",
    "        # Move to device\n",
    "        inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n",
    "        print(f\"   ✅ Moved to device: {DEVICE}\")\n",
    "        \n",
    "        # Check input validity\n",
    "        for k, v in inputs.items():\n",
    "            print(f\"      {k}: {v.shape}, device: {v.device}, dtype: {v.dtype}\")\n",
    "            if v.min() < 0:\n",
    "                print(f\"      ⚠️  {k} has negative values: min={v.min()}\")\n",
    "        \n",
    "        # Try model execution with no_grad\n",
    "        print(\"   🔄 Executing model...\")\n",
    "        with torch.no_grad():\n",
    "            try:\n",
    "                # Try to get intermediate outputs to see where it fails\n",
    "                embeddings = model.model.embed_tokens(inputs['input_ids'])\n",
    "                print(f\"   ✅ Got embeddings: {embeddings.shape}\")\n",
    "                \n",
    "                # Try first layer\n",
    "                layer_0 = model.model.layers[0]\n",
    "                print(\"   🔄 Testing first layer...\")\n",
    "                \n",
    "                # This is where it might fail\n",
    "                hidden_states = embeddings\n",
    "                layer_output = layer_0(hidden_states)\n",
    "                print(f\"   ✅ First layer output: {layer_output[0].shape}\")\n",
    "                \n",
    "                # Try full forward pass\n",
    "                print(\"   🔄 Full forward pass...\")\n",
    "                output = model(**inputs)\n",
    "                print(f\"   ✅ Full output: {output.logits.shape}\")\n",
    "                \n",
    "                return \"SUCCESS\"\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   ❌ Model execution failed: {e}\")\n",
    "                print(f\"   📍 Error type: {type(e)}\")\n",
    "                \n",
    "                # Try to get more specific error location\n",
    "                import traceback\n",
    "                print(\"   📋 Full traceback:\")\n",
    "                traceback.print_exc()\n",
    "                \n",
    "                return f\"FAILED: {e}\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ Setup failed: {e}\")\n",
    "        return f\"SETUP_FAILED: {e}\"\n",
    "\n",
    "# Test both models\n",
    "result_1 = debug_model_execution(model_1, \"Model_1\")\n",
    "result_2 = debug_model_execution(model_2, \"Model_2\")\n",
    "\n",
    "print(f\"\\n🎯 RESULTS:\")\n",
    "print(f\"Model 1: {result_1}\")\n",
    "print(f\"Model 2: {result_2}\")\n",
    "\n",
    "# %%\n",
    "print(\"\\n5️⃣ ALTERNATIVE TOKENIZATION TEST:\")\n",
    "\n",
    "# Try different tokenization approaches\n",
    "alternative_approaches = [\n",
    "    {\"padding\": False, \"truncation\": False},\n",
    "    {\"padding\": True, \"truncation\": True, \"max_length\": 512},\n",
    "    {\"padding\": \"max_length\", \"truncation\": True, \"max_length\": 10},\n",
    "]\n",
    "\n",
    "for i, kwargs in enumerate(alternative_approaches):\n",
    "    print(f\"\\nApproach {i+1}: {kwargs}\")\n",
    "    try:\n",
    "        inputs = tokenizer(\"Hello\", return_tensors=\"pt\", **kwargs)\n",
    "        inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n",
    "        \n",
    "        print(f\"   Input shape: {inputs['input_ids'].shape}\")\n",
    "        \n",
    "        # Try with model_1\n",
    "        with torch.no_grad():\n",
    "            output = model_1(**inputs)\n",
    "        print(f\"   ✅ Model 1 succeeded: {output.logits.shape}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ Failed: {e}\")\n",
    "\n",
    "# %%\n",
    "print(\"\\n6️⃣ MEMORY AND STATE CHECK:\")\n",
    "\n",
    "# Clear CUDA cache if available\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"🗑️ Cleared CUDA cache\")\n",
    "\n",
    "# Check if models are in a weird state\n",
    "print(f\"Model 1 parameters require grad: {any(p.requires_grad for p in model_1.parameters())}\")\n",
    "print(f\"Model 2 parameters require grad: {any(p.requires_grad for p in model_2.parameters())}\")\n",
    "\n",
    "# Try reloading one model to see if it's a loading issue\n",
    "print(\"\\n🔄 Trying to reload model...\")\n",
    "try:\n",
    "    # Don't actually reload the big model, just test the loading process\n",
    "    print(\"Model loading appears to be working (skipping actual reload)\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Model reload would fail: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241f7819",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
