{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78b02841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install accelerate\n",
    "# !pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e10562b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "import os\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8863042",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "MODEL_1_PATH = \"meta-llama/Llama-2-7b-chat-hf\" \n",
    "MODEL_2_PATH = \"meta-llama/Llama-2-7b-hf\"       \n",
    "\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dacab73a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = torch.device('cpu')\n",
    "\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3911279d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.58s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:49<00:00, 24.82s/it]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = LlamaTokenizer.from_pretrained(MODEL_1_PATH)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model_1 = LlamaForCausalLM.from_pretrained(\n",
    "    MODEL_1_PATH,\n",
    "    torch_dtype=torch.float32, \n",
    "    device_map=DEVICE\n",
    ")\n",
    "\n",
    "model_2 = LlamaForCausalLM.from_pretrained(\n",
    "    MODEL_2_PATH,\n",
    "    torch_dtype=torch.float32, \n",
    "    device_map=DEVICE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75bb5afc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d2ea8373",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = model_1.to(DEVICE)\n",
    "model_2 = model_2.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bbdd67eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_weights(model, move_to_cpu=False,layer_range=None):\n",
    "    weights = {}\n",
    "    \n",
    "    # Embedding weights\n",
    "    embed_weight = model.model.embed_tokens.weight\n",
    "    weights['embed_tokens'] = embed_weight.detach().cpu() if move_to_cpu else embed_weight.detach()\n",
    "    \n",
    "    # Layer-specific weights\n",
    "    if layer_range is None:\n",
    "        layer_range = range(len(model.model.layers))\n",
    "    \n",
    "    for i in layer_range:\n",
    "        if i >= len(model.model.layers):\n",
    "            continue\n",
    "            \n",
    "        layer = model.model.layers[i]\n",
    "        layer_prefix = f\"layer_{i}\"\n",
    "        \n",
    "        # Self-attention weights\n",
    "        weights[f\"{layer_prefix}_q_proj\"] = layer.self_attn.q_proj.weight.detach().cpu() if move_to_cpu else layer.self_attn.q_proj.weight.detach()\n",
    "        weights[f\"{layer_prefix}_k_proj\"] = layer.self_attn.k_proj.weight.detach().cpu() if move_to_cpu else layer.self_attn.k_proj.weight.detach()\n",
    "        weights[f\"{layer_prefix}_v_proj\"] = layer.self_attn.v_proj.weight.detach().cpu() if move_to_cpu else layer.self_attn.v_proj.weight.detach()\n",
    "        weights[f\"{layer_prefix}_o_proj\"] = layer.self_attn.o_proj.weight.detach().cpu() if move_to_cpu else layer.self_attn.o_proj.weight.detach()\n",
    "        \n",
    "        # MLP weights\n",
    "        weights[f\"{layer_prefix}_gate_proj\"] = layer.mlp.gate_proj.weight.detach().cpu() if move_to_cpu else layer.mlp.gate_proj.weight.detach()\n",
    "        weights[f\"{layer_prefix}_up_proj\"] = layer.mlp.up_proj.weight.detach().cpu() if move_to_cpu else layer.mlp.up_proj.weight.detach()\n",
    "        weights[f\"{layer_prefix}_down_proj\"] = layer.mlp.down_proj.weight.detach().cpu() if move_to_cpu else layer.mlp.down_proj.weight.detach()\n",
    "        \n",
    "        # Layer norm weights\n",
    "        weights[f\"{layer_prefix}_input_layernorm\"] = layer.input_layernorm.weight.detach().cpu() if move_to_cpu else layer.input_layernorm.weight.detach()\n",
    "        weights[f\"{layer_prefix}_post_attention_layernorm\"] = layer.post_attention_layernorm.weight.detach().cpu() if move_to_cpu else layer.post_attention_layernorm.weight.detach()\n",
    "    \n",
    "    # Final layer norm and LM head\n",
    "    weights['final_norm'] = model.model.norm.weight.detach().cpu() if move_to_cpu else model.model.norm.weight.detach()\n",
    "    weights['lm_head'] = model.lm_head.weight.detach().cpu() if move_to_cpu else model.lm_head.weight.detach()\n",
    "    \n",
    "    return weights\n",
    "\n",
    "def calculate_weight_differences(weights_1, weights_2):\n",
    "    differences = {}\n",
    "    \n",
    "    common_keys = set(weights_1.keys()) & set(weights_2.keys())\n",
    "    print(f\"Comparing {len(common_keys)} weight matrices...\")\n",
    "    \n",
    "    for i, key in enumerate(common_keys):\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Processing {i+1}/{len(common_keys)}: {key}\")\n",
    "            \n",
    "        w1 = weights_1[key]\n",
    "        w2 = weights_2[key]\n",
    "        \n",
    "        if w1.shape != w2.shape:\n",
    "            print(f\"Warning: Shape mismatch for {key}: {w1.shape} vs {w2.shape}\")\n",
    "            continue\n",
    "        \n",
    "        # Calculate difference matrix\n",
    "        diff_matrix = w1 - w2\n",
    "        \n",
    "        # Calculate various norms and statistics\n",
    "        frobenius_norm = torch.norm(diff_matrix, p='fro').item()\n",
    "        frobenius_norm_relative = frobenius_norm / (torch.norm(w1, p='fro').item() + 1e-10)\n",
    "        \n",
    "        spectral_norm = torch.norm(diff_matrix, p=2).item()\n",
    "        spectral_norm_relative = spectral_norm / (torch.norm(w1, p=2).item() + 1e-10)\n",
    "        \n",
    "        # Element-wise statistics\n",
    "        abs_diff = torch.abs(diff_matrix)\n",
    "        mean_abs_diff = torch.mean(abs_diff).item()\n",
    "        max_abs_diff = torch.max(abs_diff).item()\n",
    "        std_diff = torch.std(diff_matrix).item()\n",
    "        \n",
    "        # Percentage of significantly different weights (threshold = 1e-6)\n",
    "        significant_diff_ratio = (abs_diff > 1e-6).float().mean().item()\n",
    "        \n",
    "        # Cosine similarity\n",
    "        w1_flat = w1.flatten()\n",
    "        w2_flat = w2.flatten()\n",
    "        cosine_sim = F.cosine_similarity(w1_flat.unsqueeze(0), w2_flat.unsqueeze(0)).item()\n",
    "        \n",
    "        differences[key] = {\n",
    "            'frobenius_norm': frobenius_norm,\n",
    "            'frobenius_norm_relative': frobenius_norm_relative,\n",
    "            'spectral_norm': spectral_norm,\n",
    "            'spectral_norm_relative': spectral_norm_relative,\n",
    "            'mean_abs_difference': mean_abs_diff,\n",
    "            'max_abs_difference': max_abs_diff,\n",
    "            'std_difference': std_diff,\n",
    "            'significant_diff_ratio': significant_diff_ratio,\n",
    "            'cosine_similarity': cosine_sim,\n",
    "            'weight_shape': w1.shape,\n",
    "            'total_parameters': w1.numel()\n",
    "        }\n",
    "    \n",
    "    return differences\n",
    "\n",
    "def analyze_weight_patterns(weight_differences):\n",
    "    analysis = {\n",
    "        'by_component_type': defaultdict(list),\n",
    "        'by_layer_depth': defaultdict(list),\n",
    "        'summary_stats': {}\n",
    "    }\n",
    "    \n",
    "    # Group by component type\n",
    "    for layer_name, diff_data in weight_differences.items():\n",
    "        if any(x in layer_name for x in ['q_proj', 'k_proj', 'v_proj', 'o_proj']):\n",
    "            component_type = 'attention'\n",
    "        elif any(x in layer_name for x in ['gate_proj', 'up_proj', 'down_proj']):\n",
    "            component_type = 'mlp'\n",
    "        elif 'layernorm' in layer_name or 'norm' in layer_name:\n",
    "            component_type = 'normalization'\n",
    "        elif 'embed' in layer_name:\n",
    "            component_type = 'embedding'\n",
    "        elif 'lm_head' in layer_name:\n",
    "            component_type = 'output'\n",
    "        else:\n",
    "            component_type = 'other'\n",
    "        \n",
    "        analysis['by_component_type'][component_type].append({\n",
    "            'layer_name': layer_name,\n",
    "            'frobenius_norm': diff_data['frobenius_norm'],\n",
    "            'frobenius_norm_relative': diff_data['frobenius_norm_relative'],\n",
    "            'significant_diff_ratio': diff_data['significant_diff_ratio'],\n",
    "            'cosine_similarity': diff_data['cosine_similarity']\n",
    "        })\n",
    "    \n",
    "    # Group by layer depth\n",
    "    for layer_name, diff_data in weight_differences.items():\n",
    "        if 'layer_' in layer_name:\n",
    "            try:\n",
    "                layer_num = int(layer_name.split('_')[1])\n",
    "                analysis['by_layer_depth'][layer_num].append({\n",
    "                    'layer_name': layer_name,\n",
    "                    'frobenius_norm': diff_data['frobenius_norm'],\n",
    "                    'frobenius_norm_relative': diff_data['frobenius_norm_relative'],\n",
    "                    'cosine_similarity': diff_data['cosine_similarity']\n",
    "                })\n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "    # Calculate summary statistics\n",
    "    all_frobenius = [data['frobenius_norm'] for data in weight_differences.values()]\n",
    "    all_frobenius_rel = [data['frobenius_norm_relative'] for data in weight_differences.values()]\n",
    "    all_significant_ratios = [data['significant_diff_ratio'] for data in weight_differences.values()]\n",
    "    all_cosine_sims = [data['cosine_similarity'] for data in weight_differences.values()]\n",
    "    \n",
    "    analysis['summary_stats'] = {\n",
    "        'total_layers_compared': len(weight_differences),\n",
    "        'mean_frobenius_norm': np.mean(all_frobenius),\n",
    "        'std_frobenius_norm': np.std(all_frobenius),\n",
    "        'max_frobenius_norm': np.max(all_frobenius),\n",
    "        'min_frobenius_norm': np.min(all_frobenius),\n",
    "        'mean_frobenius_norm_relative': np.mean(all_frobenius_rel),\n",
    "        'mean_significant_diff_ratio': np.mean(all_significant_ratios),\n",
    "        'mean_cosine_similarity': np.mean(all_cosine_sims),\n",
    "        'min_cosine_similarity': np.min(all_cosine_sims),\n",
    "        'total_parameters_compared': sum(data['total_parameters'] for data in weight_differences.values())\n",
    "    }\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "def print_weight_analysis_summary(analysis):\n",
    "    print(\"=\"*70)\n",
    "    print(\"LLAMA MODEL WEIGHT DIFFERENCE ANALYSIS SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Overall statistics\n",
    "    stats = analysis['summary_stats']\n",
    "    print(f\"\\n📊 OVERALL STATISTICS:\")\n",
    "    print(f\"  • Total layers compared: {stats['total_layers_compared']}\")\n",
    "    print(f\"  • Total parameters compared: {stats['total_parameters_compared']:,}\")\n",
    "    print(f\"  • Mean Frobenius norm: {stats['mean_frobenius_norm']:.2e}\")\n",
    "    print(f\"  • Mean relative Frobenius norm: {stats['mean_frobenius_norm_relative']:.8f}\")\n",
    "    print(f\"  • Max Frobenius norm: {stats['max_frobenius_norm']:.2e}\")\n",
    "    print(f\"  • Min Frobenius norm: {stats['min_frobenius_norm']:.2e}\")\n",
    "    print(f\"  • Mean cosine similarity: {stats['mean_cosine_similarity']:.8f}\")\n",
    "    print(f\"  • Min cosine similarity: {stats['min_cosine_similarity']:.8f}\")\n",
    "    print(f\"  • Mean significant difference ratio: {stats['mean_significant_diff_ratio']:.4f}\")\n",
    "    \n",
    "    # Component type analysis\n",
    "    print(f\"\\n🔧 BY COMPONENT TYPE:\")\n",
    "    for comp_type, comp_data in analysis['by_component_type'].items():\n",
    "        frob_norms = [item['frobenius_norm_relative'] for item in comp_data]\n",
    "        cosine_sims = [item['cosine_similarity'] for item in comp_data]\n",
    "        sig_ratios = [item['significant_diff_ratio'] for item in comp_data]\n",
    "        \n",
    "        print(f\"  {comp_type.upper()}:\")\n",
    "        print(f\"    - Count: {len(comp_data)} layers\")\n",
    "        print(f\"    - Mean relative Frobenius: {np.mean(frob_norms):.8f} ± {np.std(frob_norms):.8f}\")\n",
    "        print(f\"    - Mean cosine similarity: {np.mean(cosine_sims):.8f} ± {np.std(cosine_sims):.8f}\")\n",
    "        print(f\"    - Mean sig. diff ratio: {np.mean(sig_ratios):.4f}\")\n",
    "    \n",
    "    # Layer depth analysis (if available)\n",
    "    if analysis['by_layer_depth']:\n",
    "        print(f\"\\n📈 BY LAYER DEPTH:\")\n",
    "        for depth in sorted(analysis['by_layer_depth'].keys())[:10]:  # Show first 10 layers\n",
    "            depth_data = analysis['by_layer_depth'][depth]\n",
    "            frob_norms = [item['frobenius_norm_relative'] for item in depth_data]\n",
    "            cosine_sims = [item['cosine_similarity'] for item in depth_data]\n",
    "            \n",
    "            print(f\"  Layer {depth}: Frob={np.mean(frob_norms):.6f}, Cosine={np.mean(cosine_sims):.6f}\")\n",
    "    \n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ec280794",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_1 = get_model_weights(model_1)\n",
    "weights_2 = get_model_weights(model_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "566bf7c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing 291 weight matrices...\n",
      "Processing 1/291: layer_22_down_proj\n",
      "Processing 11/291: layer_15_post_attention_layernorm\n",
      "Processing 21/291: layer_27_gate_proj\n",
      "Processing 31/291: layer_31_post_attention_layernorm\n",
      "Processing 41/291: layer_3_q_proj\n",
      "Processing 51/291: layer_17_up_proj\n",
      "Processing 61/291: layer_14_up_proj\n",
      "Processing 71/291: layer_15_gate_proj\n",
      "Processing 81/291: layer_17_post_attention_layernorm\n",
      "Processing 91/291: layer_8_down_proj\n",
      "Processing 101/291: layer_24_input_layernorm\n",
      "Processing 111/291: layer_21_input_layernorm\n",
      "Processing 121/291: layer_28_v_proj\n",
      "Processing 131/291: layer_6_v_proj\n",
      "Processing 141/291: layer_29_v_proj\n",
      "Processing 151/291: layer_20_q_proj\n",
      "Processing 161/291: layer_12_o_proj\n",
      "Processing 171/291: layer_12_gate_proj\n",
      "Processing 181/291: layer_20_up_proj\n",
      "Processing 191/291: layer_19_q_proj\n",
      "Processing 201/291: layer_22_post_attention_layernorm\n",
      "Processing 211/291: final_norm\n",
      "Processing 221/291: layer_14_input_layernorm\n",
      "Processing 231/291: layer_26_k_proj\n",
      "Processing 241/291: layer_8_o_proj\n",
      "Processing 251/291: layer_11_gate_proj\n",
      "Processing 261/291: layer_18_up_proj\n",
      "Processing 271/291: layer_31_input_layernorm\n",
      "Processing 281/291: layer_30_input_layernorm\n",
      "Processing 291/291: layer_25_gate_proj\n"
     ]
    }
   ],
   "source": [
    "weight_differences = calculate_weight_differences(weights_1, weights_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff68a2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis = analyze_weight_patterns(weight_differences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "06f8f2a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "LLAMA MODEL WEIGHT DIFFERENCE ANALYSIS SUMMARY\n",
      "======================================================================\n",
      "\n",
      "📊 OVERALL STATISTICS:\n",
      "  • Total layers compared: 291\n",
      "  • Total parameters compared: 6,738,415,616\n",
      "  • Mean Frobenius norm: 5.00e+00\n",
      "  • Mean relative Frobenius norm: 0.05240598\n",
      "  • Max Frobenius norm: 2.33e+01\n",
      "  • Min Frobenius norm: 6.37e-02\n",
      "  • Mean cosine similarity: 1.00115807\n",
      "  • Min cosine similarity: 0.99230218\n",
      "  • Mean significant difference ratio: 0.9642\n",
      "\n",
      "🔧 BY COMPONENT TYPE:\n",
      "  MLP:\n",
      "    - Count: 96 layers\n",
      "    - Mean relative Frobenius: 0.06610288 ± 0.00257659\n",
      "    - Mean cosine similarity: 1.00398319 ± 0.00041742\n",
      "    - Mean sig. diff ratio: 0.9729\n",
      "  ATTENTION:\n",
      "    - Count: 128 layers\n",
      "    - Mean relative Frobenius: 0.06287519 ± 0.01413015\n",
      "    - Mean cosine similarity: 0.99922307 ± 0.00117901\n",
      "    - Mean sig. diff ratio: 0.9713\n",
      "  NORMALIZATION:\n",
      "    - Count: 65 layers\n",
      "    - Mean relative Frobenius: 0.01038052 ± 0.00265356\n",
      "    - Mean cosine similarity: 0.99998514 ± 0.00004352\n",
      "    - Mean sig. diff ratio: 0.9371\n",
      "  EMBEDDING:\n",
      "    - Count: 1 layers\n",
      "    - Mean relative Frobenius: 0.05791559 ± 0.00000000\n",
      "    - Mean cosine similarity: 1.02927232 ± 0.00000000\n",
      "    - Mean sig. diff ratio: 0.9617\n",
      "  OUTPUT:\n",
      "    - Count: 1 layers\n",
      "    - Mean relative Frobenius: 0.12358926 ± 0.00000000\n",
      "    - Mean cosine similarity: 1.02575266 ± 0.00000000\n",
      "    - Mean sig. diff ratio: 0.9832\n",
      "\n",
      "📈 BY LAYER DEPTH:\n",
      "  Layer 0: Frob=0.061750, Cosine=1.000780\n",
      "  Layer 1: Frob=0.057145, Cosine=1.000603\n",
      "  Layer 2: Frob=0.048795, Cosine=1.001271\n",
      "  Layer 3: Frob=0.051349, Cosine=1.001073\n",
      "  Layer 4: Frob=0.050610, Cosine=1.001082\n",
      "  Layer 5: Frob=0.049889, Cosine=1.001132\n",
      "  Layer 6: Frob=0.052620, Cosine=1.001073\n",
      "  Layer 7: Frob=0.052589, Cosine=1.001031\n",
      "  Layer 8: Frob=0.052245, Cosine=1.001056\n",
      "  Layer 9: Frob=0.051850, Cosine=1.001013\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print_weight_analysis_summary(analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9c01ef25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'layer_22_down_proj': {'frobenius_norm': 8.437965393066406,\n",
       "  'frobenius_norm_relative': 0.06938864292926306,\n",
       "  'spectral_norm': 8.437965393066406,\n",
       "  'spectral_norm_relative': 0.06938864292926306,\n",
       "  'mean_abs_difference': 0.0010020476765930653,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0012579604517668486,\n",
       "  'significant_diff_ratio': 0.9741316437721252,\n",
       "  'cosine_similarity': 1.0036357641220093,\n",
       "  'weight_shape': torch.Size([4096, 11008]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_5_k_proj': {'frobenius_norm': 4.747844696044922,\n",
       "  'frobenius_norm_relative': 0.042555051637605416,\n",
       "  'spectral_norm': 4.747844696044922,\n",
       "  'spectral_norm_relative': 0.042555051637605416,\n",
       "  'mean_abs_difference': 0.0009188769618049264,\n",
       "  'max_abs_difference': 0.00732421875,\n",
       "  'std_difference': 0.0011603632010519505,\n",
       "  'significant_diff_ratio': 0.960820734500885,\n",
       "  'cosine_similarity': 1.000489354133606,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_26_up_proj': {'frobenius_norm': 8.555595397949219,\n",
       "  'frobenius_norm_relative': 0.06882573699419178,\n",
       "  'spectral_norm': 8.555595397949219,\n",
       "  'spectral_norm_relative': 0.06882573699419178,\n",
       "  'mean_abs_difference': 0.001015764893963933,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.001275516115128994,\n",
       "  'significant_diff_ratio': 0.9739344120025635,\n",
       "  'cosine_similarity': 1.0034339427947998,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_30_k_proj': {'frobenius_norm': 5.332021713256836,\n",
       "  'frobenius_norm_relative': 0.060236614097658896,\n",
       "  'spectral_norm': 5.332021713256836,\n",
       "  'spectral_norm_relative': 0.060236614097658896,\n",
       "  'mean_abs_difference': 0.001034602290019393,\n",
       "  'max_abs_difference': 0.0107421875,\n",
       "  'std_difference': 0.0013022474013268948,\n",
       "  'significant_diff_ratio': 0.9710012674331665,\n",
       "  'cosine_similarity': 0.9992855787277222,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_6_up_proj': {'frobenius_norm': 7.640622615814209,\n",
       "  'frobenius_norm_relative': 0.06611216395878375,\n",
       "  'spectral_norm': 7.640622615814209,\n",
       "  'spectral_norm_relative': 0.06611216395878375,\n",
       "  'mean_abs_difference': 0.0009067205828614533,\n",
       "  'max_abs_difference': 0.0068359375,\n",
       "  'std_difference': 0.0011389772407710552,\n",
       "  'significant_diff_ratio': 0.9729219675064087,\n",
       "  'cosine_similarity': 1.0043625831604004,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_0_v_proj': {'frobenius_norm': 3.7908897399902344,\n",
       "  'frobenius_norm_relative': 0.0842658171334102,\n",
       "  'spectral_norm': 3.7908897399902344,\n",
       "  'spectral_norm_relative': 0.0842658171334102,\n",
       "  'mean_abs_difference': 0.0007258172845467925,\n",
       "  'max_abs_difference': 0.00634765625,\n",
       "  'std_difference': 0.0009258080390281975,\n",
       "  'significant_diff_ratio': 0.9805576801300049,\n",
       "  'cosine_similarity': 0.9975800514221191,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_12_down_proj': {'frobenius_norm': 7.951237201690674,\n",
       "  'frobenius_norm_relative': 0.06618498389577321,\n",
       "  'spectral_norm': 7.951237201690674,\n",
       "  'spectral_norm_relative': 0.06618498389577321,\n",
       "  'mean_abs_difference': 0.0009430106147192419,\n",
       "  'max_abs_difference': 0.007568359375,\n",
       "  'std_difference': 0.0011852747993543744,\n",
       "  'significant_diff_ratio': 0.9729090929031372,\n",
       "  'cosine_similarity': 1.0039992332458496,\n",
       "  'weight_shape': torch.Size([4096, 11008]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_31_k_proj': {'frobenius_norm': 5.498818397521973,\n",
       "  'frobenius_norm_relative': 0.0600335496510609,\n",
       "  'spectral_norm': 5.498818397521973,\n",
       "  'spectral_norm_relative': 0.0600335496510609,\n",
       "  'mean_abs_difference': 0.0010643609566614032,\n",
       "  'max_abs_difference': 0.015625,\n",
       "  'std_difference': 0.0013429520186036825,\n",
       "  'significant_diff_ratio': 0.9710589647293091,\n",
       "  'cosine_similarity': 0.9993984699249268,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_27_post_attention_layernorm': {'frobenius_norm': 0.2920877933502197,\n",
       "  'frobenius_norm_relative': 0.010211433745091822,\n",
       "  'spectral_norm': 0.2920877933502197,\n",
       "  'spectral_norm_relative': 0.010211433745091822,\n",
       "  'mean_abs_difference': 0.004338681697845459,\n",
       "  'max_abs_difference': 0.009765625,\n",
       "  'std_difference': 0.001433488680049777,\n",
       "  'significant_diff_ratio': 0.994873046875,\n",
       "  'cosine_similarity': 0.9999985098838806,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_7_input_layernorm': {'frobenius_norm': 0.20509512722492218,\n",
       "  'frobenius_norm_relative': 0.009349520847390927,\n",
       "  'spectral_norm': 0.20509512722492218,\n",
       "  'spectral_norm_relative': 0.009349520847390927,\n",
       "  'mean_abs_difference': 0.0029208073392510414,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0013231367338448763,\n",
       "  'significant_diff_ratio': 0.9501953125,\n",
       "  'cosine_similarity': 0.9999926686286926,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_15_post_attention_layernorm': {'frobenius_norm': 0.14409592747688293,\n",
       "  'frobenius_norm_relative': 0.008050540092084484,\n",
       "  'spectral_norm': 0.14409592747688293,\n",
       "  'spectral_norm_relative': 0.008050540092084484,\n",
       "  'mean_abs_difference': 0.001860499382019043,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0013425260549411178,\n",
       "  'significant_diff_ratio': 0.771240234375,\n",
       "  'cosine_similarity': 0.9999885559082031,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_18_q_proj': {'frobenius_norm': 5.296039581298828,\n",
       "  'frobenius_norm_relative': 0.05700274041800103,\n",
       "  'spectral_norm': 5.296039581298828,\n",
       "  'spectral_norm_relative': 0.05700274041800103,\n",
       "  'mean_abs_difference': 0.0010259905830025673,\n",
       "  'max_abs_difference': 0.0080718994140625,\n",
       "  'std_difference': 0.0012935962295159698,\n",
       "  'significant_diff_ratio': 0.9693618416786194,\n",
       "  'cosine_similarity': 0.9996270537376404,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_4_v_proj': {'frobenius_norm': 4.426141262054443,\n",
       "  'frobenius_norm_relative': 0.075854659575615,\n",
       "  'spectral_norm': 4.426141262054443,\n",
       "  'spectral_norm_relative': 0.075854659575615,\n",
       "  'mean_abs_difference': 0.0008593168458901346,\n",
       "  'max_abs_difference': 0.0062255859375,\n",
       "  'std_difference': 0.0010808942606672645,\n",
       "  'significant_diff_ratio': 0.9765108227729797,\n",
       "  'cosine_similarity': 0.9981894493103027,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_8_k_proj': {'frobenius_norm': 5.111876487731934,\n",
       "  'frobenius_norm_relative': 0.049849899489023006,\n",
       "  'spectral_norm': 5.111876487731934,\n",
       "  'spectral_norm_relative': 0.049849899489023006,\n",
       "  'mean_abs_difference': 0.0009909870568662882,\n",
       "  'max_abs_difference': 0.00732421875,\n",
       "  'std_difference': 0.0012488323263823986,\n",
       "  'significant_diff_ratio': 0.966234564781189,\n",
       "  'cosine_similarity': 1.000373125076294,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_25_v_proj': {'frobenius_norm': 4.972454071044922,\n",
       "  'frobenius_norm_relative': 0.0631493340706661,\n",
       "  'spectral_norm': 4.972454071044922,\n",
       "  'spectral_norm_relative': 0.0631493340706661,\n",
       "  'mean_abs_difference': 0.0009660234209150076,\n",
       "  'max_abs_difference': 0.006591796875,\n",
       "  'std_difference': 0.001214389456436038,\n",
       "  'significant_diff_ratio': 0.9716722965240479,\n",
       "  'cosine_similarity': 0.999299943447113,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_13_v_proj': {'frobenius_norm': 4.355114459991455,\n",
       "  'frobenius_norm_relative': 0.06997270667139532,\n",
       "  'spectral_norm': 4.355114459991455,\n",
       "  'spectral_norm_relative': 0.06997270667139532,\n",
       "  'mean_abs_difference': 0.0008440351230092347,\n",
       "  'max_abs_difference': 0.006561279296875,\n",
       "  'std_difference': 0.001063698553480208,\n",
       "  'significant_diff_ratio': 0.9743920564651489,\n",
       "  'cosine_similarity': 0.9984908699989319,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_14_v_proj': {'frobenius_norm': 4.285957336425781,\n",
       "  'frobenius_norm_relative': 0.07013915424705598,\n",
       "  'spectral_norm': 4.285957336425781,\n",
       "  'spectral_norm_relative': 0.07013915424705598,\n",
       "  'mean_abs_difference': 0.0008306287927553058,\n",
       "  'max_abs_difference': 0.005889892578125,\n",
       "  'std_difference': 0.0010468580294400454,\n",
       "  'significant_diff_ratio': 0.9744570851325989,\n",
       "  'cosine_similarity': 0.9984819889068604,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_7_v_proj': {'frobenius_norm': 4.458589553833008,\n",
       "  'frobenius_norm_relative': 0.0792613309233287,\n",
       "  'spectral_norm': 4.458589553833008,\n",
       "  'spectral_norm_relative': 0.0792613309233287,\n",
       "  'mean_abs_difference': 0.0008649220690131187,\n",
       "  'max_abs_difference': 0.006103515625,\n",
       "  'std_difference': 0.0010887174867093563,\n",
       "  'significant_diff_ratio': 0.9774587154388428,\n",
       "  'cosine_similarity': 0.9978500604629517,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_5_v_proj': {'frobenius_norm': 4.350338459014893,\n",
       "  'frobenius_norm_relative': 0.07268901985842877,\n",
       "  'spectral_norm': 4.350338459014893,\n",
       "  'spectral_norm_relative': 0.07268901985842877,\n",
       "  'mean_abs_difference': 0.0008440296514891088,\n",
       "  'max_abs_difference': 0.006092071533203125,\n",
       "  'std_difference': 0.0010624458082020283,\n",
       "  'significant_diff_ratio': 0.9755011796951294,\n",
       "  'cosine_similarity': 0.9983773827552795,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_11_up_proj': {'frobenius_norm': 8.037643432617188,\n",
       "  'frobenius_norm_relative': 0.06670278987859041,\n",
       "  'spectral_norm': 8.037643432617188,\n",
       "  'spectral_norm_relative': 0.06670278987859041,\n",
       "  'mean_abs_difference': 0.0009530227980576456,\n",
       "  'max_abs_difference': 0.0075531005859375,\n",
       "  'std_difference': 0.0011981845600530505,\n",
       "  'significant_diff_ratio': 0.9730544090270996,\n",
       "  'cosine_similarity': 1.0039528608322144,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_27_gate_proj': {'frobenius_norm': 8.808809280395508,\n",
       "  'frobenius_norm_relative': 0.06587299436480276,\n",
       "  'spectral_norm': 8.808809280395508,\n",
       "  'spectral_norm_relative': 0.06587299436480276,\n",
       "  'mean_abs_difference': 0.0010461758356541395,\n",
       "  'max_abs_difference': 0.013671875,\n",
       "  'std_difference': 0.001313277636654675,\n",
       "  'significant_diff_ratio': 0.9730355739593506,\n",
       "  'cosine_similarity': 1.0044147968292236,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_11_o_proj': {'frobenius_norm': 4.428964138031006,\n",
       "  'frobenius_norm_relative': 0.07354737572315648,\n",
       "  'spectral_norm': 4.428964138031006,\n",
       "  'spectral_norm_relative': 0.07354737572315648,\n",
       "  'mean_abs_difference': 0.0008569533238187432,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0010816043941304088,\n",
       "  'significant_diff_ratio': 0.9754784107208252,\n",
       "  'cosine_similarity': 0.9982882738113403,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_22_up_proj': {'frobenius_norm': 8.469330787658691,\n",
       "  'frobenius_norm_relative': 0.06921897576134509,\n",
       "  'spectral_norm': 8.469330787658691,\n",
       "  'spectral_norm_relative': 0.06921897576134509,\n",
       "  'mean_abs_difference': 0.0010055670281872153,\n",
       "  'max_abs_difference': 0.01025390625,\n",
       "  'std_difference': 0.001262644655071199,\n",
       "  'significant_diff_ratio': 0.9740433096885681,\n",
       "  'cosine_similarity': 1.0035878419876099,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_2_q_proj': {'frobenius_norm': 4.333070278167725,\n",
       "  'frobenius_norm_relative': 0.04011743892081072,\n",
       "  'spectral_norm': 4.333070278167725,\n",
       "  'spectral_norm_relative': 0.04011743892081072,\n",
       "  'mean_abs_difference': 0.0008362624794244766,\n",
       "  'max_abs_difference': 0.015625,\n",
       "  'std_difference': 0.0010591002646833658,\n",
       "  'significant_diff_ratio': 0.9600001573562622,\n",
       "  'cosine_similarity': 1.0010275840759277,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_4_k_proj': {'frobenius_norm': 4.679013729095459,\n",
       "  'frobenius_norm_relative': 0.04297787407947939,\n",
       "  'spectral_norm': 4.679013729095459,\n",
       "  'spectral_norm_relative': 0.04297787407947939,\n",
       "  'mean_abs_difference': 0.0009050709777511656,\n",
       "  'max_abs_difference': 0.0068359375,\n",
       "  'std_difference': 0.001143536064773798,\n",
       "  'significant_diff_ratio': 0.9615519046783447,\n",
       "  'cosine_similarity': 1.0005054473876953,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_5_post_attention_layernorm': {'frobenius_norm': 0.13624075055122375,\n",
       "  'frobenius_norm_relative': 0.010863133032225511,\n",
       "  'spectral_norm': 0.13624075055122375,\n",
       "  'spectral_norm_relative': 0.010863133032225511,\n",
       "  'mean_abs_difference': 0.001877129077911377,\n",
       "  'max_abs_difference': 0.005859375,\n",
       "  'std_difference': 0.001049379468895495,\n",
       "  'significant_diff_ratio': 0.92724609375,\n",
       "  'cosine_similarity': 0.9999881982803345,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_25_input_layernorm': {'frobenius_norm': 0.3556511402130127,\n",
       "  'frobenius_norm_relative': 0.010235893340148896,\n",
       "  'spectral_norm': 0.3556511402130127,\n",
       "  'spectral_norm_relative': 0.010235893340148896,\n",
       "  'mean_abs_difference': 0.005169880576431751,\n",
       "  'max_abs_difference': 0.01171875,\n",
       "  'std_difference': 0.002038171049207449,\n",
       "  'significant_diff_ratio': 0.974609375,\n",
       "  'cosine_similarity': 0.999993085861206,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_11_q_proj': {'frobenius_norm': 5.268446922302246,\n",
       "  'frobenius_norm_relative': 0.05438147991427175,\n",
       "  'spectral_norm': 5.268446922302246,\n",
       "  'spectral_norm_relative': 0.05438147991427175,\n",
       "  'mean_abs_difference': 0.0010217539966106415,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0012868057237938046,\n",
       "  'significant_diff_ratio': 0.9690695405006409,\n",
       "  'cosine_similarity': 0.9999977946281433,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_19_gate_proj': {'frobenius_norm': 8.692544937133789,\n",
       "  'frobenius_norm_relative': 0.06669191773144695,\n",
       "  'spectral_norm': 8.692544937133789,\n",
       "  'spectral_norm_relative': 0.06669191773144695,\n",
       "  'mean_abs_difference': 0.0010321848094463348,\n",
       "  'max_abs_difference': 0.0087890625,\n",
       "  'std_difference': 0.0012959794839844108,\n",
       "  'significant_diff_ratio': 0.9733238220214844,\n",
       "  'cosine_similarity': 1.00387704372406,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_30_v_proj': {'frobenius_norm': 4.893336296081543,\n",
       "  'frobenius_norm_relative': 0.05758839753103985,\n",
       "  'spectral_norm': 4.893336296081543,\n",
       "  'spectral_norm_relative': 0.05758839753103985,\n",
       "  'mean_abs_difference': 0.0009484292240813375,\n",
       "  'max_abs_difference': 0.00701904296875,\n",
       "  'std_difference': 0.001195305958390236,\n",
       "  'significant_diff_ratio': 0.9689257740974426,\n",
       "  'cosine_similarity': 0.9994920492172241,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_31_post_attention_layernorm': {'frobenius_norm': 0.2960869073867798,\n",
       "  'frobenius_norm_relative': 0.010916757888783442,\n",
       "  'spectral_norm': 0.2960869073867798,\n",
       "  'spectral_norm_relative': 0.010916757888783442,\n",
       "  'mean_abs_difference': 0.0044422149658203125,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0012973180273547769,\n",
       "  'significant_diff_ratio': 0.9990234375,\n",
       "  'cosine_similarity': 0.999998927116394,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_13_down_proj': {'frobenius_norm': 7.961238861083984,\n",
       "  'frobenius_norm_relative': 0.06574574966223729,\n",
       "  'spectral_norm': 7.961238861083984,\n",
       "  'spectral_norm_relative': 0.06574574966223729,\n",
       "  'mean_abs_difference': 0.000944170169532299,\n",
       "  'max_abs_difference': 0.015625,\n",
       "  'std_difference': 0.001186797278933227,\n",
       "  'significant_diff_ratio': 0.9727351069450378,\n",
       "  'cosine_similarity': 1.003955602645874,\n",
       "  'weight_shape': torch.Size([4096, 11008]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_1_v_proj': {'frobenius_norm': 3.9947245121002197,\n",
       "  'frobenius_norm_relative': 0.0976634902037834,\n",
       "  'spectral_norm': 3.9947245121002197,\n",
       "  'spectral_norm_relative': 0.0976634902037834,\n",
       "  'mean_abs_difference': 0.000772513507399708,\n",
       "  'max_abs_difference': 0.006591796875,\n",
       "  'std_difference': 0.0009754625498317182,\n",
       "  'significant_diff_ratio': 0.9829115867614746,\n",
       "  'cosine_similarity': 0.9964909553527832,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_17_k_proj': {'frobenius_norm': 5.334137439727783,\n",
       "  'frobenius_norm_relative': 0.054930137429767975,\n",
       "  'spectral_norm': 5.334137439727783,\n",
       "  'spectral_norm_relative': 0.054930137429767975,\n",
       "  'mean_abs_difference': 0.0010334529215469956,\n",
       "  'max_abs_difference': 0.00762939453125,\n",
       "  'std_difference': 0.0013029290130361915,\n",
       "  'significant_diff_ratio': 0.9687454700469971,\n",
       "  'cosine_similarity': 0.9999521970748901,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_31_v_proj': {'frobenius_norm': 4.631101131439209,\n",
       "  'frobenius_norm_relative': 0.059758749746542106,\n",
       "  'spectral_norm': 4.631101131439209,\n",
       "  'spectral_norm_relative': 0.059758749746542106,\n",
       "  'mean_abs_difference': 0.0008942301501519978,\n",
       "  'max_abs_difference': 0.00789642333984375,\n",
       "  'std_difference': 0.0011312909191474319,\n",
       "  'significant_diff_ratio': 0.9698655009269714,\n",
       "  'cosine_similarity': 0.9993642568588257,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_0_q_proj': {'frobenius_norm': 2.478200674057007,\n",
       "  'frobenius_norm_relative': 0.04529428611679507,\n",
       "  'spectral_norm': 2.478200674057007,\n",
       "  'spectral_norm_relative': 0.04529428611679507,\n",
       "  'mean_abs_difference': 0.00043242849642410874,\n",
       "  'max_abs_difference': 0.01171875,\n",
       "  'std_difference': 0.0006054203258827329,\n",
       "  'significant_diff_ratio': 0.9693558216094971,\n",
       "  'cosine_similarity': 1.0020240545272827,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_21_down_proj': {'frobenius_norm': 8.404852867126465,\n",
       "  'frobenius_norm_relative': 0.06913933601079915,\n",
       "  'spectral_norm': 8.404852867126465,\n",
       "  'spectral_norm_relative': 0.06913933601079915,\n",
       "  'mean_abs_difference': 0.000997869879938662,\n",
       "  'max_abs_difference': 0.0096435546875,\n",
       "  'std_difference': 0.001253011403605342,\n",
       "  'significant_diff_ratio': 0.9740197658538818,\n",
       "  'cosine_similarity': 1.0036449432373047,\n",
       "  'weight_shape': torch.Size([4096, 11008]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_27_up_proj': {'frobenius_norm': 8.54171085357666,\n",
       "  'frobenius_norm_relative': 0.06837060127795463,\n",
       "  'spectral_norm': 8.54171085357666,\n",
       "  'spectral_norm_relative': 0.06837060127795463,\n",
       "  'mean_abs_difference': 0.0010138978250324726,\n",
       "  'max_abs_difference': 0.009765625,\n",
       "  'std_difference': 0.0012734532356262207,\n",
       "  'significant_diff_ratio': 0.9737817645072937,\n",
       "  'cosine_similarity': 1.0034101009368896,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_15_q_proj': {'frobenius_norm': 5.316622257232666,\n",
       "  'frobenius_norm_relative': 0.05511993738158327,\n",
       "  'spectral_norm': 5.316622257232666,\n",
       "  'spectral_norm_relative': 0.05511993738158327,\n",
       "  'mean_abs_difference': 0.0010304109891876578,\n",
       "  'max_abs_difference': 0.00830078125,\n",
       "  'std_difference': 0.001298686140216887,\n",
       "  'significant_diff_ratio': 0.9687067270278931,\n",
       "  'cosine_similarity': 0.9998506307601929,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_9_k_proj': {'frobenius_norm': 5.166133403778076,\n",
       "  'frobenius_norm_relative': 0.049375397631209864,\n",
       "  'spectral_norm': 5.166133403778076,\n",
       "  'spectral_norm_relative': 0.049375397631209864,\n",
       "  'mean_abs_difference': 0.00100141076836735,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0012621300993487239,\n",
       "  'significant_diff_ratio': 0.9657313227653503,\n",
       "  'cosine_similarity': 1.0002989768981934,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_3_q_proj': {'frobenius_norm': 4.599825859069824,\n",
       "  'frobenius_norm_relative': 0.04500039260034916,\n",
       "  'spectral_norm': 4.599825859069824,\n",
       "  'spectral_norm_relative': 0.04500039260034916,\n",
       "  'mean_abs_difference': 0.0008899049134925008,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0011240741005167365,\n",
       "  'significant_diff_ratio': 0.9630357623100281,\n",
       "  'cosine_similarity': 1.0005450248718262,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_31_q_proj': {'frobenius_norm': 5.514733791351318,\n",
       "  'frobenius_norm_relative': 0.06240377657276829,\n",
       "  'spectral_norm': 5.514733791351318,\n",
       "  'spectral_norm_relative': 0.06240377657276829,\n",
       "  'mean_abs_difference': 0.0010688756592571735,\n",
       "  'max_abs_difference': 0.01171875,\n",
       "  'std_difference': 0.0013467506505548954,\n",
       "  'significant_diff_ratio': 0.971977710723877,\n",
       "  'cosine_similarity': 0.999191403388977,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_18_down_proj': {'frobenius_norm': 8.232666015625,\n",
       "  'frobenius_norm_relative': 0.06786050421198273,\n",
       "  'spectral_norm': 8.232666015625,\n",
       "  'spectral_norm_relative': 0.06786050421198273,\n",
       "  'mean_abs_difference': 0.0009769403841346502,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0012273507891222835,\n",
       "  'significant_diff_ratio': 0.9735403656959534,\n",
       "  'cosine_similarity': 1.0037798881530762,\n",
       "  'weight_shape': torch.Size([4096, 11008]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_9_input_layernorm': {'frobenius_norm': 0.22860167920589447,\n",
       "  'frobenius_norm_relative': 0.010403072978888222,\n",
       "  'spectral_norm': 0.22860167920589447,\n",
       "  'spectral_norm_relative': 0.010403072978888222,\n",
       "  'mean_abs_difference': 0.003312712535262108,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0013359826989471912,\n",
       "  'significant_diff_ratio': 0.974365234375,\n",
       "  'cosine_similarity': 0.9999927878379822,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_17_input_layernorm': {'frobenius_norm': 0.29475247859954834,\n",
       "  'frobenius_norm_relative': 0.011123795088578284,\n",
       "  'spectral_norm': 0.29475247859954834,\n",
       "  'spectral_norm_relative': 0.011123795088578284,\n",
       "  'mean_abs_difference': 0.004403494764119387,\n",
       "  'max_abs_difference': 0.009765625,\n",
       "  'std_difference': 0.00134936289396137,\n",
       "  'significant_diff_ratio': 0.996337890625,\n",
       "  'cosine_similarity': 0.9999978542327881,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_29_down_proj': {'frobenius_norm': 8.359954833984375,\n",
       "  'frobenius_norm_relative': 0.06656437202723305,\n",
       "  'spectral_norm': 8.359954833984375,\n",
       "  'spectral_norm_relative': 0.06656437202723305,\n",
       "  'mean_abs_difference': 0.0009864402236416936,\n",
       "  'max_abs_difference': 0.01171875,\n",
       "  'std_difference': 0.0012465473264455795,\n",
       "  'significant_diff_ratio': 0.9727200269699097,\n",
       "  'cosine_similarity': 1.003531575202942,\n",
       "  'weight_shape': torch.Size([4096, 11008]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_28_down_proj': {'frobenius_norm': 8.511173248291016,\n",
       "  'frobenius_norm_relative': 0.06799768622570507,\n",
       "  'spectral_norm': 8.511173248291016,\n",
       "  'spectral_norm_relative': 0.06799768622570507,\n",
       "  'mean_abs_difference': 0.0010072553995996714,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0012689855648204684,\n",
       "  'significant_diff_ratio': 0.9734148383140564,\n",
       "  'cosine_similarity': 1.003453254699707,\n",
       "  'weight_shape': torch.Size([4096, 11008]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_2_up_proj': {'frobenius_norm': 7.382428169250488,\n",
       "  'frobenius_norm_relative': 0.06366556946267243,\n",
       "  'spectral_norm': 7.382428169250488,\n",
       "  'spectral_norm_relative': 0.06366556946267243,\n",
       "  'mean_abs_difference': 0.0008769890409894288,\n",
       "  'max_abs_difference': 0.0062255859375,\n",
       "  'std_difference': 0.001100563327781856,\n",
       "  'significant_diff_ratio': 0.9719051122665405,\n",
       "  'cosine_similarity': 1.0044782161712646,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_4_o_proj': {'frobenius_norm': 4.485839366912842,\n",
       "  'frobenius_norm_relative': 0.07858351865384036,\n",
       "  'spectral_norm': 4.485839366912842,\n",
       "  'spectral_norm_relative': 0.07858351865384036,\n",
       "  'mean_abs_difference': 0.0008706428925506771,\n",
       "  'max_abs_difference': 0.006072998046875,\n",
       "  'std_difference': 0.001095354906283319,\n",
       "  'significant_diff_ratio': 0.9771450161933899,\n",
       "  'cosine_similarity': 0.9980177283287048,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_15_o_proj': {'frobenius_norm': 4.4074859619140625,\n",
       "  'frobenius_norm_relative': 0.070286654839327,\n",
       "  'spectral_norm': 4.4074859619140625,\n",
       "  'spectral_norm_relative': 0.070286654839327,\n",
       "  'mean_abs_difference': 0.0008548776968382299,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.001076446264050901,\n",
       "  'significant_diff_ratio': 0.9745209813117981,\n",
       "  'cosine_similarity': 0.9983840584754944,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_17_up_proj': {'frobenius_norm': 8.256293296813965,\n",
       "  'frobenius_norm_relative': 0.06708535831406712,\n",
       "  'spectral_norm': 8.256293296813965,\n",
       "  'spectral_norm_relative': 0.06708535831406712,\n",
       "  'mean_abs_difference': 0.0009796591475605965,\n",
       "  'max_abs_difference': 0.00927734375,\n",
       "  'std_difference': 0.0012308789882808924,\n",
       "  'significant_diff_ratio': 0.973271906375885,\n",
       "  'cosine_similarity': 1.0036778450012207,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_24_post_attention_layernorm': {'frobenius_norm': 0.23234659433364868,\n",
       "  'frobenius_norm_relative': 0.008882295892381476,\n",
       "  'spectral_norm': 0.23234659433364868,\n",
       "  'spectral_norm_relative': 0.008882295892381476,\n",
       "  'mean_abs_difference': 0.003332793712615967,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0014449930749833584,\n",
       "  'significant_diff_ratio': 0.965576171875,\n",
       "  'cosine_similarity': 0.9999967813491821,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_28_o_proj': {'frobenius_norm': 5.024733543395996,\n",
       "  'frobenius_norm_relative': 0.06104172467120421,\n",
       "  'spectral_norm': 5.024733543395996,\n",
       "  'spectral_norm_relative': 0.06104172467120421,\n",
       "  'mean_abs_difference': 0.0009754984639585018,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0012272208696231246,\n",
       "  'significant_diff_ratio': 0.9707567691802979,\n",
       "  'cosine_similarity': 0.9993414878845215,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_11_v_proj': {'frobenius_norm': 4.336440086364746,\n",
       "  'frobenius_norm_relative': 0.07132196361360946,\n",
       "  'spectral_norm': 4.336440086364746,\n",
       "  'spectral_norm_relative': 0.07132196361360946,\n",
       "  'mean_abs_difference': 0.0008401564555242658,\n",
       "  'max_abs_difference': 0.006855010986328125,\n",
       "  'std_difference': 0.001059110858477652,\n",
       "  'significant_diff_ratio': 0.9749435186386108,\n",
       "  'cosine_similarity': 0.9983332753181458,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_28_k_proj': {'frobenius_norm': 5.324228763580322,\n",
       "  'frobenius_norm_relative': 0.05906270767149764,\n",
       "  'spectral_norm': 5.324228763580322,\n",
       "  'spectral_norm_relative': 0.05906270767149764,\n",
       "  'mean_abs_difference': 0.0010338198626413941,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.001300386036746204,\n",
       "  'significant_diff_ratio': 0.9705520868301392,\n",
       "  'cosine_similarity': 0.999366044998169,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_24_up_proj': {'frobenius_norm': 8.566575050354004,\n",
       "  'frobenius_norm_relative': 0.06945619311327554,\n",
       "  'spectral_norm': 8.566575050354004,\n",
       "  'spectral_norm_relative': 0.06945619311327554,\n",
       "  'mean_abs_difference': 0.0010171601315960288,\n",
       "  'max_abs_difference': 0.009765625,\n",
       "  'std_difference': 0.0012771247420459986,\n",
       "  'significant_diff_ratio': 0.9741190671920776,\n",
       "  'cosine_similarity': 1.003485918045044,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_15_k_proj': {'frobenius_norm': 5.4003987312316895,\n",
       "  'frobenius_norm_relative': 0.05412723877470095,\n",
       "  'spectral_norm': 5.4003987312316895,\n",
       "  'spectral_norm_relative': 0.05412723877470095,\n",
       "  'mean_abs_difference': 0.001046835328452289,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.001319154049269855,\n",
       "  'significant_diff_ratio': 0.9685633778572083,\n",
       "  'cosine_similarity': 0.9999767541885376,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_3_up_proj': {'frobenius_norm': 7.513704776763916,\n",
       "  'frobenius_norm_relative': 0.06433475996233581,\n",
       "  'spectral_norm': 7.513704776763916,\n",
       "  'spectral_norm_relative': 0.06433475996233581,\n",
       "  'mean_abs_difference': 0.000892461568582803,\n",
       "  'max_abs_difference': 0.006591796875,\n",
       "  'std_difference': 0.0011201227316632867,\n",
       "  'significant_diff_ratio': 0.9722139239311218,\n",
       "  'cosine_similarity': 1.0043563842773438,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_3_input_layernorm': {'frobenius_norm': 0.20425836741924286,\n",
       "  'frobenius_norm_relative': 0.0111941802228016,\n",
       "  'spectral_norm': 0.20425836741924286,\n",
       "  'spectral_norm_relative': 0.0111941802228016,\n",
       "  'mean_abs_difference': 0.002934732474386692,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0012544470373541117,\n",
       "  'significant_diff_ratio': 0.958251953125,\n",
       "  'cosine_similarity': 0.9999905824661255,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_4_q_proj': {'frobenius_norm': 4.761052131652832,\n",
       "  'frobenius_norm_relative': 0.04465244023290847,\n",
       "  'spectral_norm': 4.761052131652832,\n",
       "  'spectral_norm_relative': 0.04465244023290847,\n",
       "  'mean_abs_difference': 0.0009224803070537746,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0011635096743702888,\n",
       "  'significant_diff_ratio': 0.9624634385108948,\n",
       "  'cosine_similarity': 1.0003162622451782,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_14_up_proj': {'frobenius_norm': 8.155856132507324,\n",
       "  'frobenius_norm_relative': 0.06632002394950386,\n",
       "  'spectral_norm': 8.155856132507324,\n",
       "  'spectral_norm_relative': 0.06632002394950386,\n",
       "  'mean_abs_difference': 0.0009674763423390687,\n",
       "  'max_abs_difference': 0.00732421875,\n",
       "  'std_difference': 0.001215900294482708,\n",
       "  'significant_diff_ratio': 0.9729374647140503,\n",
       "  'cosine_similarity': 1.0037484169006348,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_4_post_attention_layernorm': {'frobenius_norm': 0.13436259329319,\n",
       "  'frobenius_norm_relative': 0.011349244036208662,\n",
       "  'spectral_norm': 0.13436259329319,\n",
       "  'spectral_norm_relative': 0.011349244036208662,\n",
       "  'mean_abs_difference': 0.0018381178379058838,\n",
       "  'max_abs_difference': 0.0048828125,\n",
       "  'std_difference': 0.001046623452566564,\n",
       "  'significant_diff_ratio': 0.914794921875,\n",
       "  'cosine_similarity': 0.99998539686203,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_12_up_proj': {'frobenius_norm': 8.063419342041016,\n",
       "  'frobenius_norm_relative': 0.06631053670767212,\n",
       "  'spectral_norm': 8.063419342041016,\n",
       "  'spectral_norm_relative': 0.06631053670767212,\n",
       "  'mean_abs_difference': 0.0009561135084368289,\n",
       "  'max_abs_difference': 0.007568359375,\n",
       "  'std_difference': 0.0012020444264635444,\n",
       "  'significant_diff_ratio': 0.9728776812553406,\n",
       "  'cosine_similarity': 1.00386381149292,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_20_gate_proj': {'frobenius_norm': 8.702253341674805,\n",
       "  'frobenius_norm_relative': 0.06641569879728096,\n",
       "  'spectral_norm': 8.702253341674805,\n",
       "  'spectral_norm_relative': 0.06641569879728096,\n",
       "  'mean_abs_difference': 0.0010334814433008432,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0012974096462130547,\n",
       "  'significant_diff_ratio': 0.973181962966919,\n",
       "  'cosine_similarity': 1.0040143728256226,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_30_down_proj': {'frobenius_norm': 8.022528648376465,\n",
       "  'frobenius_norm_relative': 0.06426361420737257,\n",
       "  'spectral_norm': 8.022528648376465,\n",
       "  'spectral_norm_relative': 0.06426361420737257,\n",
       "  'mean_abs_difference': 0.000939988880418241,\n",
       "  'max_abs_difference': 0.015625,\n",
       "  'std_difference': 0.0011962716234847903,\n",
       "  'significant_diff_ratio': 0.9713287949562073,\n",
       "  'cosine_similarity': 1.003781795501709,\n",
       "  'weight_shape': torch.Size([4096, 11008]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_16_up_proj': {'frobenius_norm': 8.210671424865723,\n",
       "  'frobenius_norm_relative': 0.06646132275667017,\n",
       "  'spectral_norm': 8.210671424865723,\n",
       "  'spectral_norm_relative': 0.06646132275667017,\n",
       "  'mean_abs_difference': 0.0009741014800965786,\n",
       "  'max_abs_difference': 0.00726318359375,\n",
       "  'std_difference': 0.0012241010554134846,\n",
       "  'significant_diff_ratio': 0.9729949831962585,\n",
       "  'cosine_similarity': 1.003686785697937,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_17_o_proj': {'frobenius_norm': 4.663743019104004,\n",
       "  'frobenius_norm_relative': 0.07111127765763556,\n",
       "  'spectral_norm': 4.663743019104004,\n",
       "  'spectral_norm_relative': 0.07111127765763556,\n",
       "  'mean_abs_difference': 0.0009037334239110351,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0011388639686629176,\n",
       "  'significant_diff_ratio': 0.9747008085250854,\n",
       "  'cosine_similarity': 0.9984526634216309,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_29_post_attention_layernorm': {'frobenius_norm': 0.3236376643180847,\n",
       "  'frobenius_norm_relative': 0.010769506954186917,\n",
       "  'spectral_norm': 0.3236376643180847,\n",
       "  'spectral_norm_relative': 0.010769506954186917,\n",
       "  'mean_abs_difference': 0.004855513572692871,\n",
       "  'max_abs_difference': 0.009765625,\n",
       "  'std_difference': 0.0014218149008229375,\n",
       "  'significant_diff_ratio': 0.998046875,\n",
       "  'cosine_similarity': 0.9999992251396179,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_5_down_proj': {'frobenius_norm': 7.689506530761719,\n",
       "  'frobenius_norm_relative': 0.0664140302248536,\n",
       "  'spectral_norm': 7.689506530761719,\n",
       "  'spectral_norm_relative': 0.0664140302248536,\n",
       "  'mean_abs_difference': 0.0009127834346145391,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0011462501715868711,\n",
       "  'significant_diff_ratio': 0.9730415940284729,\n",
       "  'cosine_similarity': 1.0043097734451294,\n",
       "  'weight_shape': torch.Size([4096, 11008]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_8_up_proj': {'frobenius_norm': 7.824586868286133,\n",
       "  'frobenius_norm_relative': 0.06663277261360907,\n",
       "  'spectral_norm': 7.824586868286133,\n",
       "  'spectral_norm_relative': 0.06663277261360907,\n",
       "  'mean_abs_difference': 0.0009281010716222227,\n",
       "  'max_abs_difference': 0.006988525390625,\n",
       "  'std_difference': 0.0011663895566016436,\n",
       "  'significant_diff_ratio': 0.9730927348136902,\n",
       "  'cosine_similarity': 1.0041662454605103,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_15_gate_proj': {'frobenius_norm': 8.518645286560059,\n",
       "  'frobenius_norm_relative': 0.06724467948953927,\n",
       "  'spectral_norm': 8.518645286560059,\n",
       "  'spectral_norm_relative': 0.06724467948953927,\n",
       "  'mean_abs_difference': 0.0010109497234225273,\n",
       "  'max_abs_difference': 0.00927734375,\n",
       "  'std_difference': 0.0012700851075351238,\n",
       "  'significant_diff_ratio': 0.973667323589325,\n",
       "  'cosine_similarity': 1.003510594367981,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_30_o_proj': {'frobenius_norm': 5.0764875411987305,\n",
       "  'frobenius_norm_relative': 0.059449914044043824,\n",
       "  'spectral_norm': 5.0764875411987305,\n",
       "  'spectral_norm_relative': 0.059449914044043824,\n",
       "  'mean_abs_difference': 0.0009837074903771281,\n",
       "  'max_abs_difference': 0.008056640625,\n",
       "  'std_difference': 0.001239931327290833,\n",
       "  'significant_diff_ratio': 0.9698613286018372,\n",
       "  'cosine_similarity': 0.9993709325790405,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_16_o_proj': {'frobenius_norm': 4.5589375495910645,\n",
       "  'frobenius_norm_relative': 0.06956888442199269,\n",
       "  'spectral_norm': 4.5589375495910645,\n",
       "  'spectral_norm_relative': 0.06956888442199269,\n",
       "  'mean_abs_difference': 0.0008826879784464836,\n",
       "  'max_abs_difference': 0.0068359375,\n",
       "  'std_difference': 0.0011133627267554402,\n",
       "  'significant_diff_ratio': 0.9740788340568542,\n",
       "  'cosine_similarity': 0.9985560774803162,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_3_gate_proj': {'frobenius_norm': 7.588742733001709,\n",
       "  'frobenius_norm_relative': 0.06012720269671633,\n",
       "  'spectral_norm': 7.588742733001709,\n",
       "  'spectral_norm_relative': 0.06012720269671633,\n",
       "  'mean_abs_difference': 0.0009015626274049282,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0011314955772832036,\n",
       "  'significant_diff_ratio': 0.9703407883644104,\n",
       "  'cosine_similarity': 1.0038514137268066,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_27_input_layernorm': {'frobenius_norm': 0.3605331480503082,\n",
       "  'frobenius_norm_relative': 0.010371656038457562,\n",
       "  'spectral_norm': 0.3605331480503082,\n",
       "  'spectral_norm_relative': 0.010371656038457562,\n",
       "  'mean_abs_difference': 0.0052474793046712875,\n",
       "  'max_abs_difference': 0.01171875,\n",
       "  'std_difference': 0.0020492717158049345,\n",
       "  'significant_diff_ratio': 0.97705078125,\n",
       "  'cosine_similarity': 0.999993085861206,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_9_q_proj': {'frobenius_norm': 5.138644218444824,\n",
       "  'frobenius_norm_relative': 0.050340476977520685,\n",
       "  'spectral_norm': 5.138644218444824,\n",
       "  'spectral_norm_relative': 0.050340476977520685,\n",
       "  'mean_abs_difference': 0.0009961165487766266,\n",
       "  'max_abs_difference': 0.00732421875,\n",
       "  'std_difference': 0.0012553904671221972,\n",
       "  'significant_diff_ratio': 0.9659645557403564,\n",
       "  'cosine_similarity': 1.0002446174621582,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_24_down_proj': {'frobenius_norm': 8.58829402923584,\n",
       "  'frobenius_norm_relative': 0.06995147047641545,\n",
       "  'spectral_norm': 8.58829402923584,\n",
       "  'spectral_norm_relative': 0.06995147047641545,\n",
       "  'mean_abs_difference': 0.0010201759869232774,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0012803543359041214,\n",
       "  'significant_diff_ratio': 0.9743436574935913,\n",
       "  'cosine_similarity': 1.0034899711608887,\n",
       "  'weight_shape': torch.Size([4096, 11008]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_15_up_proj': {'frobenius_norm': 8.154401779174805,\n",
       "  'frobenius_norm_relative': 0.06595016815590297,\n",
       "  'spectral_norm': 8.154401779174805,\n",
       "  'spectral_norm_relative': 0.06595016815590297,\n",
       "  'mean_abs_difference': 0.0009672019514255226,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.00121569714974612,\n",
       "  'significant_diff_ratio': 0.9728357195854187,\n",
       "  'cosine_similarity': 1.0037199258804321,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_10_gate_proj': {'frobenius_norm': 8.398812294006348,\n",
       "  'frobenius_norm_relative': 0.06561947191194913,\n",
       "  'spectral_norm': 8.398812294006348,\n",
       "  'spectral_norm_relative': 0.06561947191194913,\n",
       "  'mean_abs_difference': 0.0009967676596716046,\n",
       "  'max_abs_difference': 0.0074710845947265625,\n",
       "  'std_difference': 0.0012522529577836394,\n",
       "  'significant_diff_ratio': 0.9731269478797913,\n",
       "  'cosine_similarity': 1.0037037134170532,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_25_up_proj': {'frobenius_norm': 8.57492733001709,\n",
       "  'frobenius_norm_relative': 0.06922494869558873,\n",
       "  'spectral_norm': 8.57492733001709,\n",
       "  'spectral_norm_relative': 0.06922494869558873,\n",
       "  'mean_abs_difference': 0.0010182401165366173,\n",
       "  'max_abs_difference': 0.0076751708984375,\n",
       "  'std_difference': 0.0012783874990418553,\n",
       "  'significant_diff_ratio': 0.9740738868713379,\n",
       "  'cosine_similarity': 1.0034421682357788,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_17_post_attention_layernorm': {'frobenius_norm': 0.17321394383907318,\n",
       "  'frobenius_norm_relative': 0.008561483445090095,\n",
       "  'spectral_norm': 0.17321394383907318,\n",
       "  'spectral_norm_relative': 0.008561483445090095,\n",
       "  'mean_abs_difference': 0.0023543834686279297,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0013578921789303422,\n",
       "  'significant_diff_ratio': 0.869384765625,\n",
       "  'cosine_similarity': 0.9999908804893494,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_8_v_proj': {'frobenius_norm': 4.413210391998291,\n",
       "  'frobenius_norm_relative': 0.07639175667863583,\n",
       "  'spectral_norm': 4.413210391998291,\n",
       "  'spectral_norm_relative': 0.07639175667863583,\n",
       "  'mean_abs_difference': 0.0008565316675230861,\n",
       "  'max_abs_difference': 0.006103515625,\n",
       "  'std_difference': 0.001077725668437779,\n",
       "  'significant_diff_ratio': 0.9766912460327148,\n",
       "  'cosine_similarity': 0.9982579350471497,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_2_down_proj': {'frobenius_norm': 7.458620071411133,\n",
       "  'frobenius_norm_relative': 0.06392231488560686,\n",
       "  'spectral_norm': 7.458620071411133,\n",
       "  'spectral_norm_relative': 0.06392231488560686,\n",
       "  'mean_abs_difference': 0.0008861838141456246,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0011119086993858218,\n",
       "  'significant_diff_ratio': 0.9720113277435303,\n",
       "  'cosine_similarity': 1.0043858289718628,\n",
       "  'weight_shape': torch.Size([4096, 11008]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_16_input_layernorm': {'frobenius_norm': 0.3085192143917084,\n",
       "  'frobenius_norm_relative': 0.012009522646199868,\n",
       "  'spectral_norm': 0.3085192143917084,\n",
       "  'spectral_norm_relative': 0.012009522646199868,\n",
       "  'mean_abs_difference': 0.004629045724868774,\n",
       "  'max_abs_difference': 0.009765625,\n",
       "  'std_difference': 0.0013472562422975898,\n",
       "  'significant_diff_ratio': 0.998046875,\n",
       "  'cosine_similarity': 0.9999971389770508,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_7_k_proj': {'frobenius_norm': 5.020422458648682,\n",
       "  'frobenius_norm_relative': 0.04948270623522803,\n",
       "  'spectral_norm': 5.020422458648682,\n",
       "  'spectral_norm_relative': 0.04948270623522803,\n",
       "  'mean_abs_difference': 0.0009731253958307207,\n",
       "  'max_abs_difference': 0.00714111328125,\n",
       "  'std_difference': 0.0012265217956155539,\n",
       "  'significant_diff_ratio': 0.9661608934402466,\n",
       "  'cosine_similarity': 1.0002515316009521,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_0_input_layernorm': {'frobenius_norm': 0.06370905786752701,\n",
       "  'frobenius_norm_relative': 0.026121502108325926,\n",
       "  'spectral_norm': 0.06370905786752701,\n",
       "  'spectral_norm_relative': 0.026121502108325926,\n",
       "  'mean_abs_difference': 0.0007808239897713065,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0009826107416301966,\n",
       "  'significant_diff_ratio': 0.964599609375,\n",
       "  'cosine_similarity': 0.9996707439422607,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_3_o_proj': {'frobenius_norm': 4.515036582946777,\n",
       "  'frobenius_norm_relative': 0.08347618774559598,\n",
       "  'spectral_norm': 4.515036582946777,\n",
       "  'spectral_norm_relative': 0.08347618774559598,\n",
       "  'mean_abs_difference': 0.0008771020220592618,\n",
       "  'max_abs_difference': 0.00595855712890625,\n",
       "  'std_difference': 0.0011023984989151359,\n",
       "  'significant_diff_ratio': 0.9786558151245117,\n",
       "  'cosine_similarity': 0.9977954030036926,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_8_q_proj': {'frobenius_norm': 5.1102094650268555,\n",
       "  'frobenius_norm_relative': 0.05012070814943925,\n",
       "  'spectral_norm': 5.1102094650268555,\n",
       "  'spectral_norm_relative': 0.05012070814943925,\n",
       "  'mean_abs_difference': 0.0009908040519803762,\n",
       "  'max_abs_difference': 0.007183074951171875,\n",
       "  'std_difference': 0.001248408225364983,\n",
       "  'significant_diff_ratio': 0.9663439989089966,\n",
       "  'cosine_similarity': 1.0002453327178955,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_14_q_proj': {'frobenius_norm': 5.366689682006836,\n",
       "  'frobenius_norm_relative': 0.054965457429190664,\n",
       "  'spectral_norm': 5.366689682006836,\n",
       "  'spectral_norm_relative': 0.054965457429190664,\n",
       "  'mean_abs_difference': 0.0010410810355097055,\n",
       "  'max_abs_difference': 0.007568359375,\n",
       "  'std_difference': 0.0013108383864164352,\n",
       "  'significant_diff_ratio': 0.968817412853241,\n",
       "  'cosine_similarity': 1.0000410079956055,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_29_o_proj': {'frobenius_norm': 5.178296089172363,\n",
       "  'frobenius_norm_relative': 0.062164558900403497,\n",
       "  'spectral_norm': 5.178296089172363,\n",
       "  'spectral_norm_relative': 0.062164558900403497,\n",
       "  'mean_abs_difference': 0.0010051049757748842,\n",
       "  'max_abs_difference': 0.01953125,\n",
       "  'std_difference': 0.0012646657414734364,\n",
       "  'significant_diff_ratio': 0.9712818264961243,\n",
       "  'cosine_similarity': 0.9992499947547913,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_8_down_proj': {'frobenius_norm': 7.804276466369629,\n",
       "  'frobenius_norm_relative': 0.06696756519796132,\n",
       "  'spectral_norm': 7.804276466369629,\n",
       "  'spectral_norm_relative': 0.06696756519796132,\n",
       "  'mean_abs_difference': 0.0009260291699320078,\n",
       "  'max_abs_difference': 0.01171875,\n",
       "  'std_difference': 0.0011633418034762144,\n",
       "  'significant_diff_ratio': 0.9732714891433716,\n",
       "  'cosine_similarity': 1.00424325466156,\n",
       "  'weight_shape': torch.Size([4096, 11008]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_19_v_proj': {'frobenius_norm': 4.668221473693848,\n",
       "  'frobenius_norm_relative': 0.06699923482386681,\n",
       "  'spectral_norm': 4.668221473693848,\n",
       "  'spectral_norm_relative': 0.06699923482386681,\n",
       "  'mean_abs_difference': 0.0009058689465746284,\n",
       "  'max_abs_difference': 0.0067138671875,\n",
       "  'std_difference': 0.0011400870280340314,\n",
       "  'significant_diff_ratio': 0.9733886122703552,\n",
       "  'cosine_similarity': 0.9988657236099243,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_4_input_layernorm': {'frobenius_norm': 0.17321208119392395,\n",
       "  'frobenius_norm_relative': 0.010367540051380061,\n",
       "  'spectral_norm': 0.17321208119392395,\n",
       "  'spectral_norm_relative': 0.010367540051380061,\n",
       "  'mean_abs_difference': 0.0024109738878905773,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0012447371846064925,\n",
       "  'significant_diff_ratio': 0.907958984375,\n",
       "  'cosine_similarity': 0.9999895691871643,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_28_post_attention_layernorm': {'frobenius_norm': 0.3042753338813782,\n",
       "  'frobenius_norm_relative': 0.01041382498939895,\n",
       "  'spectral_norm': 0.3042753338813782,\n",
       "  'spectral_norm_relative': 0.01041382498939895,\n",
       "  'mean_abs_difference': 0.004540622234344482,\n",
       "  'max_abs_difference': 0.009765625,\n",
       "  'std_difference': 0.0014182807644829154,\n",
       "  'significant_diff_ratio': 0.997802734375,\n",
       "  'cosine_similarity': 0.9999989867210388,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_2_k_proj': {'frobenius_norm': 4.2913689613342285,\n",
       "  'frobenius_norm_relative': 0.037777744289514595,\n",
       "  'spectral_norm': 4.2913689613342285,\n",
       "  'spectral_norm_relative': 0.037777744289514595,\n",
       "  'mean_abs_difference': 0.0008263499476015568,\n",
       "  'max_abs_difference': 0.0062255859375,\n",
       "  'std_difference': 0.001048975856974721,\n",
       "  'significant_diff_ratio': 0.9589781165122986,\n",
       "  'cosine_similarity': 1.0011476278305054,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_2_v_proj': {'frobenius_norm': 4.275660991668701,\n",
       "  'frobenius_norm_relative': 0.07336246621539821,\n",
       "  'spectral_norm': 4.275660991668701,\n",
       "  'spectral_norm_relative': 0.07336246621539821,\n",
       "  'mean_abs_difference': 0.0008289013057947159,\n",
       "  'max_abs_difference': 0.006072998046875,\n",
       "  'std_difference': 0.0010442895581945777,\n",
       "  'significant_diff_ratio': 0.9756855964660645,\n",
       "  'cosine_similarity': 0.998313307762146,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_12_v_proj': {'frobenius_norm': 4.320816993713379,\n",
       "  'frobenius_norm_relative': 0.07194933687314177,\n",
       "  'spectral_norm': 4.320816993713379,\n",
       "  'spectral_norm_relative': 0.07194933687314177,\n",
       "  'mean_abs_difference': 0.0008379047503694892,\n",
       "  'max_abs_difference': 0.0059814453125,\n",
       "  'std_difference': 0.0010553072206676006,\n",
       "  'significant_diff_ratio': 0.9752076864242554,\n",
       "  'cosine_similarity': 0.9984627366065979,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_10_v_proj': {'frobenius_norm': 4.3216423988342285,\n",
       "  'frobenius_norm_relative': 0.07410424787087458,\n",
       "  'spectral_norm': 4.3216423988342285,\n",
       "  'spectral_norm_relative': 0.07410424787087458,\n",
       "  'mean_abs_difference': 0.0008387219277210534,\n",
       "  'max_abs_difference': 0.006000518798828125,\n",
       "  'std_difference': 0.0010554458713158965,\n",
       "  'significant_diff_ratio': 0.9758978486061096,\n",
       "  'cosine_similarity': 0.9984674453735352,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_10_input_layernorm': {'frobenius_norm': 0.24399803578853607,\n",
       "  'frobenius_norm_relative': 0.011048059870821136,\n",
       "  'spectral_norm': 0.24399803578853607,\n",
       "  'spectral_norm_relative': 0.011048059870821136,\n",
       "  'mean_abs_difference': 0.003567485371604562,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0013447717064991593,\n",
       "  'significant_diff_ratio': 0.985595703125,\n",
       "  'cosine_similarity': 0.9999926686286926,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_23_gate_proj': {'frobenius_norm': 8.81283187866211,\n",
       "  'frobenius_norm_relative': 0.06644594820329307,\n",
       "  'spectral_norm': 8.81283187866211,\n",
       "  'spectral_norm_relative': 0.06644594820329307,\n",
       "  'mean_abs_difference': 0.0010470787528902292,\n",
       "  'max_abs_difference': 0.008571624755859375,\n",
       "  'std_difference': 0.0013138602953404188,\n",
       "  'significant_diff_ratio': 0.9731373190879822,\n",
       "  'cosine_similarity': 1.0041559934616089,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_24_input_layernorm': {'frobenius_norm': 0.3438488245010376,\n",
       "  'frobenius_norm_relative': 0.010801459111587135,\n",
       "  'spectral_norm': 0.3438488245010376,\n",
       "  'spectral_norm_relative': 0.010801459111587135,\n",
       "  'mean_abs_difference': 0.005087739787995815,\n",
       "  'max_abs_difference': 0.01171875,\n",
       "  'std_difference': 0.0017265193164348602,\n",
       "  'significant_diff_ratio': 0.99267578125,\n",
       "  'cosine_similarity': 0.9999963045120239,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_26_down_proj': {'frobenius_norm': 8.623748779296875,\n",
       "  'frobenius_norm_relative': 0.0696823651705728,\n",
       "  'spectral_norm': 8.623748779296875,\n",
       "  'spectral_norm_relative': 0.0696823651705728,\n",
       "  'mean_abs_difference': 0.0010234675137326121,\n",
       "  'max_abs_difference': 0.01171875,\n",
       "  'std_difference': 0.0012856641551479697,\n",
       "  'significant_diff_ratio': 0.9742385745048523,\n",
       "  'cosine_similarity': 1.0034387111663818,\n",
       "  'weight_shape': torch.Size([4096, 11008]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_29_gate_proj': {'frobenius_norm': 8.810039520263672,\n",
       "  'frobenius_norm_relative': 0.06599283556659254,\n",
       "  'spectral_norm': 8.810039520263672,\n",
       "  'spectral_norm_relative': 0.06599283556659254,\n",
       "  'mean_abs_difference': 0.0010458707110956311,\n",
       "  'max_abs_difference': 0.009765625,\n",
       "  'std_difference': 0.0013134951004758477,\n",
       "  'significant_diff_ratio': 0.973149836063385,\n",
       "  'cosine_similarity': 1.004440426826477,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_29_up_proj': {'frobenius_norm': 8.521784782409668,\n",
       "  'frobenius_norm_relative': 0.06693721046333605,\n",
       "  'spectral_norm': 8.521784782409668,\n",
       "  'spectral_norm_relative': 0.06693721046333605,\n",
       "  'mean_abs_difference': 0.0010106184054166079,\n",
       "  'max_abs_difference': 0.0107421875,\n",
       "  'std_difference': 0.0012705420376732945,\n",
       "  'significant_diff_ratio': 0.9732277393341064,\n",
       "  'cosine_similarity': 1.003395915031433,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_9_o_proj': {'frobenius_norm': 4.373073101043701,\n",
       "  'frobenius_norm_relative': 0.07525901380026284,\n",
       "  'spectral_norm': 4.373073101043701,\n",
       "  'spectral_norm_relative': 0.07525901380026284,\n",
       "  'mean_abs_difference': 0.0008473646594211459,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0010679556289687753,\n",
       "  'significant_diff_ratio': 0.9760482907295227,\n",
       "  'cosine_similarity': 0.9982420802116394,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_17_down_proj': {'frobenius_norm': 8.140670776367188,\n",
       "  'frobenius_norm_relative': 0.06703736734450512,\n",
       "  'spectral_norm': 8.140670776367188,\n",
       "  'spectral_norm_relative': 0.06703736734450512,\n",
       "  'mean_abs_difference': 0.0009659070638008416,\n",
       "  'max_abs_difference': 0.015625,\n",
       "  'std_difference': 0.001213588286191225,\n",
       "  'significant_diff_ratio': 0.9732857346534729,\n",
       "  'cosine_similarity': 1.0038080215454102,\n",
       "  'weight_shape': torch.Size([4096, 11008]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_22_k_proj': {'frobenius_norm': 5.244569778442383,\n",
       "  'frobenius_norm_relative': 0.05616871804830197,\n",
       "  'spectral_norm': 5.244569778442383,\n",
       "  'spectral_norm_relative': 0.05616871804830197,\n",
       "  'mean_abs_difference': 0.0010171675821766257,\n",
       "  'max_abs_difference': 0.007568359375,\n",
       "  'std_difference': 0.0012810220941901207,\n",
       "  'significant_diff_ratio': 0.96922767162323,\n",
       "  'cosine_similarity': 0.9999568462371826,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_15_down_proj': {'frobenius_norm': 8.011927604675293,\n",
       "  'frobenius_norm_relative': 0.0658570445236215,\n",
       "  'spectral_norm': 8.011927604675293,\n",
       "  'spectral_norm_relative': 0.0658570445236215,\n",
       "  'mean_abs_difference': 0.0009500262094661593,\n",
       "  'max_abs_difference': 0.0089111328125,\n",
       "  'std_difference': 0.0011943490244448185,\n",
       "  'significant_diff_ratio': 0.9727612733840942,\n",
       "  'cosine_similarity': 1.0039008855819702,\n",
       "  'weight_shape': torch.Size([4096, 11008]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_5_input_layernorm': {'frobenius_norm': 0.1740250289440155,\n",
       "  'frobenius_norm_relative': 0.010161847419590505,\n",
       "  'spectral_norm': 0.1740250289440155,\n",
       "  'spectral_norm_relative': 0.010161847419590505,\n",
       "  'mean_abs_difference': 0.002398568904027343,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0012908573262393475,\n",
       "  'significant_diff_ratio': 0.891357421875,\n",
       "  'cosine_similarity': 0.9999886751174927,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_14_gate_proj': {'frobenius_norm': 8.529053688049316,\n",
       "  'frobenius_norm_relative': 0.06768678330811323,\n",
       "  'spectral_norm': 8.529053688049316,\n",
       "  'spectral_norm_relative': 0.06768678330811323,\n",
       "  'mean_abs_difference': 0.001012250897474587,\n",
       "  'max_abs_difference': 0.00726318359375,\n",
       "  'std_difference': 0.0012716222554445267,\n",
       "  'significant_diff_ratio': 0.9738004803657532,\n",
       "  'cosine_similarity': 1.0035146474838257,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_21_input_layernorm': {'frobenius_norm': 0.29096195101737976,\n",
       "  'frobenius_norm_relative': 0.00973043599180845,\n",
       "  'spectral_norm': 0.29096195101737976,\n",
       "  'spectral_norm_relative': 0.00973043599180845,\n",
       "  'mean_abs_difference': 0.004318595863878727,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.001420882879756391,\n",
       "  'significant_diff_ratio': 0.993896484375,\n",
       "  'cosine_similarity': 0.9999991059303284,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_1_post_attention_layernorm': {'frobenius_norm': 0.08544612675905228,\n",
       "  'frobenius_norm_relative': 0.013116989526030529,\n",
       "  'spectral_norm': 0.08544612675905228,\n",
       "  'spectral_norm_relative': 0.013116989526030529,\n",
       "  'mean_abs_difference': 0.0010895447339862585,\n",
       "  'max_abs_difference': 0.0048828125,\n",
       "  'std_difference': 0.00096520921215415,\n",
       "  'significant_diff_ratio': 0.8798828125,\n",
       "  'cosine_similarity': 0.9999590516090393,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_4_down_proj': {'frobenius_norm': 7.636986255645752,\n",
       "  'frobenius_norm_relative': 0.0660218432795391,\n",
       "  'spectral_norm': 7.636986255645752,\n",
       "  'spectral_norm_relative': 0.0660218432795391,\n",
       "  'mean_abs_difference': 0.0009064741898328066,\n",
       "  'max_abs_difference': 0.01171875,\n",
       "  'std_difference': 0.001138423103839159,\n",
       "  'significant_diff_ratio': 0.9728655219078064,\n",
       "  'cosine_similarity': 1.0043408870697021,\n",
       "  'weight_shape': torch.Size([4096, 11008]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_24_q_proj': {'frobenius_norm': 5.339209079742432,\n",
       "  'frobenius_norm_relative': 0.0599680470560544,\n",
       "  'spectral_norm': 5.339209079742432,\n",
       "  'spectral_norm_relative': 0.0599680470560544,\n",
       "  'mean_abs_difference': 0.0010366415372118354,\n",
       "  'max_abs_difference': 0.0087890625,\n",
       "  'std_difference': 0.0013040164485573769,\n",
       "  'significant_diff_ratio': 0.9713507294654846,\n",
       "  'cosine_similarity': 0.9992771148681641,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_12_q_proj': {'frobenius_norm': 5.287425994873047,\n",
       "  'frobenius_norm_relative': 0.05349028461161979,\n",
       "  'spectral_norm': 5.287425994873047,\n",
       "  'spectral_norm_relative': 0.05349028461161979,\n",
       "  'mean_abs_difference': 0.00102561479434371,\n",
       "  'max_abs_difference': 0.0072021484375,\n",
       "  'std_difference': 0.0012915695551782846,\n",
       "  'significant_diff_ratio': 0.9679966568946838,\n",
       "  'cosine_similarity': 1.0000038146972656,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_8_gate_proj': {'frobenius_norm': 8.190411567687988,\n",
       "  'frobenius_norm_relative': 0.0631909509507113,\n",
       "  'spectral_norm': 8.190411567687988,\n",
       "  'spectral_norm_relative': 0.0631909509507113,\n",
       "  'mean_abs_difference': 0.0009723561233840883,\n",
       "  'max_abs_difference': 0.007568359375,\n",
       "  'std_difference': 0.0012212072033435106,\n",
       "  'significant_diff_ratio': 0.9719893932342529,\n",
       "  'cosine_similarity': 1.0041007995605469,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_8_post_attention_layernorm': {'frobenius_norm': 0.13295693695545197,\n",
       "  'frobenius_norm_relative': 0.009164523625714686,\n",
       "  'spectral_norm': 0.13295693695545197,\n",
       "  'spectral_norm_relative': 0.009164523625714686,\n",
       "  'mean_abs_difference': 0.001799464225769043,\n",
       "  'max_abs_difference': 0.005859375,\n",
       "  'std_difference': 0.0010933233425021172,\n",
       "  'significant_diff_ratio': 0.902587890625,\n",
       "  'cosine_similarity': 0.9999920129776001,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_20_v_proj': {'frobenius_norm': 4.7370758056640625,\n",
       "  'frobenius_norm_relative': 0.0669852639495586,\n",
       "  'spectral_norm': 4.7370758056640625,\n",
       "  'spectral_norm_relative': 0.0669852639495586,\n",
       "  'mean_abs_difference': 0.0009192785946652293,\n",
       "  'max_abs_difference': 0.00701904296875,\n",
       "  'std_difference': 0.0011569190537557006,\n",
       "  'significant_diff_ratio': 0.9733082056045532,\n",
       "  'cosine_similarity': 0.9989830255508423,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_2_post_attention_layernorm': {'frobenius_norm': 0.09992062300443649,\n",
       "  'frobenius_norm_relative': 0.01159489469484401,\n",
       "  'spectral_norm': 0.09992062300443649,\n",
       "  'spectral_norm_relative': 0.01159489469484401,\n",
       "  'mean_abs_difference': 0.0012758595403283834,\n",
       "  'max_abs_difference': 0.0048828125,\n",
       "  'std_difference': 0.001034511486068368,\n",
       "  'significant_diff_ratio': 0.81591796875,\n",
       "  'cosine_similarity': 0.9999710917472839,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_24_v_proj': {'frobenius_norm': 4.964504718780518,\n",
       "  'frobenius_norm_relative': 0.06541390451019384,\n",
       "  'spectral_norm': 4.964504718780518,\n",
       "  'spectral_norm_relative': 0.06541390451019384,\n",
       "  'mean_abs_difference': 0.0009646923863328993,\n",
       "  'max_abs_difference': 0.0068359375,\n",
       "  'std_difference': 0.001212327741086483,\n",
       "  'significant_diff_ratio': 0.9727107286453247,\n",
       "  'cosine_similarity': 0.9992544651031494,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_28_v_proj': {'frobenius_norm': 4.937997817993164,\n",
       "  'frobenius_norm_relative': 0.05972126862045856,\n",
       "  'spectral_norm': 4.937997817993164,\n",
       "  'spectral_norm_relative': 0.05972126862045856,\n",
       "  'mean_abs_difference': 0.0009586571832187474,\n",
       "  'max_abs_difference': 0.0069732666015625,\n",
       "  'std_difference': 0.001206120359711349,\n",
       "  'significant_diff_ratio': 0.9701546430587769,\n",
       "  'cosine_similarity': 0.9993994235992432,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_9_down_proj': {'frobenius_norm': 7.857545852661133,\n",
       "  'frobenius_norm_relative': 0.06695896321238916,\n",
       "  'spectral_norm': 7.857545852661133,\n",
       "  'spectral_norm_relative': 0.06695896321238916,\n",
       "  'mean_abs_difference': 0.000932212162297219,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0011712838895618916,\n",
       "  'significant_diff_ratio': 0.9732651710510254,\n",
       "  'cosine_similarity': 1.004188060760498,\n",
       "  'weight_shape': torch.Size([4096, 11008]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_7_o_proj': {'frobenius_norm': 4.47178840637207,\n",
       "  'frobenius_norm_relative': 0.08103328782975419,\n",
       "  'spectral_norm': 4.47178840637207,\n",
       "  'spectral_norm_relative': 0.08103328782975419,\n",
       "  'mean_abs_difference': 0.0008664465858601034,\n",
       "  'max_abs_difference': 0.006103515625,\n",
       "  'std_difference': 0.0010918958578258753,\n",
       "  'significant_diff_ratio': 0.9777671098709106,\n",
       "  'cosine_similarity': 0.9979553818702698,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_13_o_proj': {'frobenius_norm': 4.409083366394043,\n",
       "  'frobenius_norm_relative': 0.07154843116362551,\n",
       "  'spectral_norm': 4.409083366394043,\n",
       "  'spectral_norm_relative': 0.07154843116362551,\n",
       "  'mean_abs_difference': 0.0008541274000890553,\n",
       "  'max_abs_difference': 0.0074462890625,\n",
       "  'std_difference': 0.0010768012143671513,\n",
       "  'significant_diff_ratio': 0.9749303460121155,\n",
       "  'cosine_similarity': 0.9983645081520081,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_19_o_proj': {'frobenius_norm': 4.870305061340332,\n",
       "  'frobenius_norm_relative': 0.07062704341751912,\n",
       "  'spectral_norm': 4.870305061340332,\n",
       "  'spectral_norm_relative': 0.07062704341751912,\n",
       "  'mean_abs_difference': 0.0009452314116060734,\n",
       "  'max_abs_difference': 0.0067138671875,\n",
       "  'std_difference': 0.0011892324546352029,\n",
       "  'significant_diff_ratio': 0.9746370911598206,\n",
       "  'cosine_similarity': 0.998685359954834,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_18_input_layernorm': {'frobenius_norm': 0.27791568636894226,\n",
       "  'frobenius_norm_relative': 0.009929976612621252,\n",
       "  'spectral_norm': 0.27791568636894226,\n",
       "  'spectral_norm_relative': 0.009929976612621252,\n",
       "  'mean_abs_difference': 0.004119643941521645,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0013732697116211057,\n",
       "  'significant_diff_ratio': 0.993896484375,\n",
       "  'cosine_similarity': 0.9999984502792358,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_24_o_proj': {'frobenius_norm': 5.148160457611084,\n",
       "  'frobenius_norm_relative': 0.06902081871906426,\n",
       "  'spectral_norm': 5.148160457611084,\n",
       "  'spectral_norm_relative': 0.06902081871906426,\n",
       "  'mean_abs_difference': 0.0009990442777052522,\n",
       "  'max_abs_difference': 0.01171875,\n",
       "  'std_difference': 0.0012570625403895974,\n",
       "  'significant_diff_ratio': 0.9740133285522461,\n",
       "  'cosine_similarity': 0.9989004731178284,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_5_q_proj': {'frobenius_norm': 4.787556171417236,\n",
       "  'frobenius_norm_relative': 0.04466815304352225,\n",
       "  'spectral_norm': 4.787556171417236,\n",
       "  'spectral_norm_relative': 0.04466815304352225,\n",
       "  'mean_abs_difference': 0.00092760642291978,\n",
       "  'max_abs_difference': 0.0072021484375,\n",
       "  'std_difference': 0.0011700175236910582,\n",
       "  'significant_diff_ratio': 0.9619293808937073,\n",
       "  'cosine_similarity': 1.0003111362457275,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_6_post_attention_layernorm': {'frobenius_norm': 0.13462889194488525,\n",
       "  'frobenius_norm_relative': 0.010031056644220257,\n",
       "  'spectral_norm': 0.13462889194488525,\n",
       "  'spectral_norm_relative': 0.010031056644220257,\n",
       "  'mean_abs_difference': 0.0018409490585327148,\n",
       "  'max_abs_difference': 0.005859375,\n",
       "  'std_difference': 0.0010605325223878026,\n",
       "  'significant_diff_ratio': 0.913818359375,\n",
       "  'cosine_similarity': 0.9999902248382568,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_12_post_attention_layernorm': {'frobenius_norm': 0.13483990728855133,\n",
       "  'frobenius_norm_relative': 0.0084103488845568,\n",
       "  'spectral_norm': 0.13483990728855133,\n",
       "  'spectral_norm_relative': 0.0084103488845568,\n",
       "  'mean_abs_difference': 0.001734018325805664,\n",
       "  'max_abs_difference': 0.005859375,\n",
       "  'std_difference': 0.0012826650636270642,\n",
       "  'significant_diff_ratio': 0.789306640625,\n",
       "  'cosine_similarity': 0.999988317489624,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_6_v_proj': {'frobenius_norm': 4.4694013595581055,\n",
       "  'frobenius_norm_relative': 0.08047265616534091,\n",
       "  'spectral_norm': 4.4694013595581055,\n",
       "  'spectral_norm_relative': 0.08047265616534091,\n",
       "  'mean_abs_difference': 0.0008670289535075426,\n",
       "  'max_abs_difference': 0.00592041015625,\n",
       "  'std_difference': 0.001091293292120099,\n",
       "  'significant_diff_ratio': 0.9778518676757812,\n",
       "  'cosine_similarity': 0.9979537129402161,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_15_input_layernorm': {'frobenius_norm': 0.2879183292388916,\n",
       "  'frobenius_norm_relative': 0.011583339670334331,\n",
       "  'spectral_norm': 0.2879183292388916,\n",
       "  'spectral_norm_relative': 0.011583339670334331,\n",
       "  'mean_abs_difference': 0.004293352365493774,\n",
       "  'max_abs_difference': 0.01171875,\n",
       "  'std_difference': 0.0013439058093354106,\n",
       "  'significant_diff_ratio': 0.9970703125,\n",
       "  'cosine_similarity': 0.9999963045120239,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_10_up_proj': {'frobenius_norm': 7.992169380187988,\n",
       "  'frobenius_norm_relative': 0.06689061402796855,\n",
       "  'spectral_norm': 7.992169380187988,\n",
       "  'spectral_norm_relative': 0.06689061402796855,\n",
       "  'mean_abs_difference': 0.0009477834100835025,\n",
       "  'max_abs_difference': 0.00665283203125,\n",
       "  'std_difference': 0.0011913508642464876,\n",
       "  'significant_diff_ratio': 0.9731636643409729,\n",
       "  'cosine_similarity': 1.0040122270584106,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_13_post_attention_layernorm': {'frobenius_norm': 0.14650492370128632,\n",
       "  'frobenius_norm_relative': 0.008883903714886378,\n",
       "  'spectral_norm': 0.14650492370128632,\n",
       "  'spectral_norm_relative': 0.008883903714886378,\n",
       "  'mean_abs_difference': 0.001902759075164795,\n",
       "  'max_abs_difference': 0.005859375,\n",
       "  'std_difference': 0.0013206887524574995,\n",
       "  'significant_diff_ratio': 0.784912109375,\n",
       "  'cosine_similarity': 0.9999873638153076,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_29_input_layernorm': {'frobenius_norm': 0.33343014121055603,\n",
       "  'frobenius_norm_relative': 0.009740164936833044,\n",
       "  'spectral_norm': 0.33343014121055603,\n",
       "  'spectral_norm_relative': 0.009740164936833044,\n",
       "  'mean_abs_difference': 0.004798755049705505,\n",
       "  'max_abs_difference': 0.015625,\n",
       "  'std_difference': 0.0020286571234464645,\n",
       "  'significant_diff_ratio': 0.954833984375,\n",
       "  'cosine_similarity': 0.999993085861206,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_31_down_proj': {'frobenius_norm': 7.2006378173828125,\n",
       "  'frobenius_norm_relative': 0.057758148942714964,\n",
       "  'spectral_norm': 7.2006378173828125,\n",
       "  'spectral_norm_relative': 0.057758148942714964,\n",
       "  'mean_abs_difference': 0.0008355370373465121,\n",
       "  'max_abs_difference': 0.015625,\n",
       "  'std_difference': 0.0010742039885371923,\n",
       "  'significant_diff_ratio': 0.96783047914505,\n",
       "  'cosine_similarity': 1.004289984703064,\n",
       "  'weight_shape': torch.Size([4096, 11008]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_3_v_proj': {'frobenius_norm': 4.3687310218811035,\n",
       "  'frobenius_norm_relative': 0.07902701786041318,\n",
       "  'spectral_norm': 4.3687310218811035,\n",
       "  'spectral_norm_relative': 0.07902701786041318,\n",
       "  'mean_abs_difference': 0.0008480151882395148,\n",
       "  'max_abs_difference': 0.00604248046875,\n",
       "  'std_difference': 0.0010669007897377014,\n",
       "  'significant_diff_ratio': 0.9774608016014099,\n",
       "  'cosine_similarity': 0.9982520341873169,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_0_k_proj': {'frobenius_norm': 2.53912091255188,\n",
       "  'frobenius_norm_relative': 0.04142570042731183,\n",
       "  'spectral_norm': 2.53912091255188,\n",
       "  'spectral_norm_relative': 0.04142570042731183,\n",
       "  'mean_abs_difference': 0.0004406938096508384,\n",
       "  'max_abs_difference': 0.0166015625,\n",
       "  'std_difference': 0.0006204194505698979,\n",
       "  'significant_diff_ratio': 0.9621654748916626,\n",
       "  'cosine_similarity': 1.0013927221298218,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_1_down_proj': {'frobenius_norm': 7.356190204620361,\n",
       "  'frobenius_norm_relative': 0.06365712666394607,\n",
       "  'spectral_norm': 7.356190204620361,\n",
       "  'spectral_norm_relative': 0.06365712666394607,\n",
       "  'mean_abs_difference': 0.0008733381982892752,\n",
       "  'max_abs_difference': 0.015625,\n",
       "  'std_difference': 0.0010966553818434477,\n",
       "  'significant_diff_ratio': 0.9719694256782532,\n",
       "  'cosine_similarity': 1.0045015811920166,\n",
       "  'weight_shape': torch.Size([4096, 11008]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_25_down_proj': {'frobenius_norm': 8.635223388671875,\n",
       "  'frobenius_norm_relative': 0.06998791791448096,\n",
       "  'spectral_norm': 8.635223388671875,\n",
       "  'spectral_norm_relative': 0.06998791791448096,\n",
       "  'mean_abs_difference': 0.0010255755623802543,\n",
       "  'max_abs_difference': 0.015625,\n",
       "  'std_difference': 0.0012873420491814613,\n",
       "  'significant_diff_ratio': 0.9743975400924683,\n",
       "  'cosine_similarity': 1.003444790840149,\n",
       "  'weight_shape': torch.Size([4096, 11008]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_29_v_proj': {'frobenius_norm': 4.954885482788086,\n",
       "  'frobenius_norm_relative': 0.05968456670407185,\n",
       "  'spectral_norm': 4.954885482788086,\n",
       "  'spectral_norm_relative': 0.05968456670407185,\n",
       "  'mean_abs_difference': 0.0009610253619030118,\n",
       "  'max_abs_difference': 0.007568359375,\n",
       "  'std_difference': 0.0012102395994588733,\n",
       "  'significant_diff_ratio': 0.9701020121574402,\n",
       "  'cosine_similarity': 0.9994542598724365,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_12_k_proj': {'frobenius_norm': 5.351505279541016,\n",
       "  'frobenius_norm_relative': 0.052515864274516294,\n",
       "  'spectral_norm': 5.351505279541016,\n",
       "  'spectral_norm_relative': 0.052515864274516294,\n",
       "  'mean_abs_difference': 0.0010375857818871737,\n",
       "  'max_abs_difference': 0.0076904296875,\n",
       "  'std_difference': 0.0013072373112663627,\n",
       "  'significant_diff_ratio': 0.9677895903587341,\n",
       "  'cosine_similarity': 1.0001232624053955,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_9_post_attention_layernorm': {'frobenius_norm': 0.12474033236503601,\n",
       "  'frobenius_norm_relative': 0.008401572841351181,\n",
       "  'spectral_norm': 0.12474033236503601,\n",
       "  'spectral_norm_relative': 0.008401572841351181,\n",
       "  'mean_abs_difference': 0.0016632080078125,\n",
       "  'max_abs_difference': 0.005859375,\n",
       "  'std_difference': 0.0010933420853689313,\n",
       "  'significant_diff_ratio': 0.88232421875,\n",
       "  'cosine_similarity': 0.9999925494194031,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_23_q_proj': {'frobenius_norm': 5.275055408477783,\n",
       "  'frobenius_norm_relative': 0.05761193948695812,\n",
       "  'spectral_norm': 5.275055408477783,\n",
       "  'spectral_norm_relative': 0.05761193948695812,\n",
       "  'mean_abs_difference': 0.0010234846267849207,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0012884729076176882,\n",
       "  'significant_diff_ratio': 0.9697124361991882,\n",
       "  'cosine_similarity': 0.9996523857116699,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_21_k_proj': {'frobenius_norm': 5.294196128845215,\n",
       "  'frobenius_norm_relative': 0.05799056292352896,\n",
       "  'spectral_norm': 5.294196128845215,\n",
       "  'spectral_norm_relative': 0.05799056292352896,\n",
       "  'mean_abs_difference': 0.0010271577630192041,\n",
       "  'max_abs_difference': 0.0074462890625,\n",
       "  'std_difference': 0.0012930878438055515,\n",
       "  'significant_diff_ratio': 0.9703312516212463,\n",
       "  'cosine_similarity': 0.9996341466903687,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_23_post_attention_layernorm': {'frobenius_norm': 0.21492044627666473,\n",
       "  'frobenius_norm_relative': 0.008484464336143124,\n",
       "  'spectral_norm': 0.21492044627666473,\n",
       "  'spectral_norm_relative': 0.008484464336143124,\n",
       "  'mean_abs_difference': 0.003056943416595459,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0013998475624248385,\n",
       "  'significant_diff_ratio': 0.947509765625,\n",
       "  'cosine_similarity': 0.9999964833259583,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_28_up_proj': {'frobenius_norm': 8.573166847229004,\n",
       "  'frobenius_norm_relative': 0.0679579144936056,\n",
       "  'spectral_norm': 8.573166847229004,\n",
       "  'spectral_norm_relative': 0.0679579144936056,\n",
       "  'mean_abs_difference': 0.0010173505870625377,\n",
       "  'max_abs_difference': 0.01025390625,\n",
       "  'std_difference': 0.0012781458208337426,\n",
       "  'significant_diff_ratio': 0.973608136177063,\n",
       "  'cosine_similarity': 1.0033454895019531,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_16_k_proj': {'frobenius_norm': 5.430888652801514,\n",
       "  'frobenius_norm_relative': 0.055255770578686095,\n",
       "  'spectral_norm': 5.430888652801514,\n",
       "  'spectral_norm_relative': 0.055255770578686095,\n",
       "  'mean_abs_difference': 0.0010526154655963182,\n",
       "  'max_abs_difference': 0.00789642333984375,\n",
       "  'std_difference': 0.001326500321738422,\n",
       "  'significant_diff_ratio': 0.9693059325218201,\n",
       "  'cosine_similarity': 1.0000600814819336,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_27_o_proj': {'frobenius_norm': 5.042657375335693,\n",
       "  'frobenius_norm_relative': 0.06302448395957762,\n",
       "  'spectral_norm': 5.042657375335693,\n",
       "  'spectral_norm_relative': 0.06302448395957762,\n",
       "  'mean_abs_difference': 0.000979797332547605,\n",
       "  'max_abs_difference': 0.01171875,\n",
       "  'std_difference': 0.0012315205531194806,\n",
       "  'significant_diff_ratio': 0.9716857671737671,\n",
       "  'cosine_similarity': 0.9992443323135376,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_16_gate_proj': {'frobenius_norm': 8.571089744567871,\n",
       "  'frobenius_norm_relative': 0.06719484905420826,\n",
       "  'spectral_norm': 8.571089744567871,\n",
       "  'spectral_norm_relative': 0.06719484905420826,\n",
       "  'mean_abs_difference': 0.0010173255577683449,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0012779023963958025,\n",
       "  'significant_diff_ratio': 0.9736019372940063,\n",
       "  'cosine_similarity': 1.003460168838501,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_20_q_proj': {'frobenius_norm': 5.3425421714782715,\n",
       "  'frobenius_norm_relative': 0.057834771485102326,\n",
       "  'spectral_norm': 5.3425421714782715,\n",
       "  'spectral_norm_relative': 0.057834771485102326,\n",
       "  'mean_abs_difference': 0.0010362555040046573,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0013049375265836716,\n",
       "  'significant_diff_ratio': 0.9701040387153625,\n",
       "  'cosine_similarity': 0.9996099472045898,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_4_gate_proj': {'frobenius_norm': 7.7505693435668945,\n",
       "  'frobenius_norm_relative': 0.06016439243461498,\n",
       "  'spectral_norm': 7.7505693435668945,\n",
       "  'spectral_norm_relative': 0.06016439243461498,\n",
       "  'mean_abs_difference': 0.0009204488596878946,\n",
       "  'max_abs_difference': 0.00732421875,\n",
       "  'std_difference': 0.0011556372046470642,\n",
       "  'significant_diff_ratio': 0.9704627394676208,\n",
       "  'cosine_similarity': 1.0040373802185059,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_25_q_proj': {'frobenius_norm': 5.274148464202881,\n",
       "  'frobenius_norm_relative': 0.05874496535043081,\n",
       "  'spectral_norm': 5.274148464202881,\n",
       "  'spectral_norm_relative': 0.05874496535043081,\n",
       "  'mean_abs_difference': 0.0010241870768368244,\n",
       "  'max_abs_difference': 0.0133056640625,\n",
       "  'std_difference': 0.0012881677830591798,\n",
       "  'significant_diff_ratio': 0.9704422354698181,\n",
       "  'cosine_similarity': 0.9994786381721497,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_6_k_proj': {'frobenius_norm': 4.999655723571777,\n",
       "  'frobenius_norm_relative': 0.04846495563293143,\n",
       "  'spectral_norm': 4.999655723571777,\n",
       "  'spectral_norm_relative': 0.04846495563293143,\n",
       "  'mean_abs_difference': 0.0009691443992778659,\n",
       "  'max_abs_difference': 0.00738525390625,\n",
       "  'std_difference': 0.0012214655289426446,\n",
       "  'significant_diff_ratio': 0.9655052423477173,\n",
       "  'cosine_similarity': 1.0003392696380615,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_14_k_proj': {'frobenius_norm': 5.416794300079346,\n",
       "  'frobenius_norm_relative': 0.05443003538149676,\n",
       "  'spectral_norm': 5.416794300079346,\n",
       "  'spectral_norm_relative': 0.05443003538149676,\n",
       "  'mean_abs_difference': 0.0010505927493795753,\n",
       "  'max_abs_difference': 0.00732421875,\n",
       "  'std_difference': 0.0013230841141194105,\n",
       "  'significant_diff_ratio': 0.9687387347221375,\n",
       "  'cosine_similarity': 1.0001171827316284,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_15_v_proj': {'frobenius_norm': 4.346723556518555,\n",
       "  'frobenius_norm_relative': 0.06840682709389885,\n",
       "  'spectral_norm': 4.346723556518555,\n",
       "  'spectral_norm_relative': 0.06840682709389885,\n",
       "  'mean_abs_difference': 0.0008423661347478628,\n",
       "  'max_abs_difference': 0.0058441162109375,\n",
       "  'std_difference': 0.0010617045918479562,\n",
       "  'significant_diff_ratio': 0.9737943410873413,\n",
       "  'cosine_similarity': 0.9984422922134399,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_16_post_attention_layernorm': {'frobenius_norm': 0.15516746044158936,\n",
       "  'frobenius_norm_relative': 0.008151916048659981,\n",
       "  'spectral_norm': 0.15516746044158936,\n",
       "  'spectral_norm_relative': 0.008151916048659981,\n",
       "  'mean_abs_difference': 0.0020401477813720703,\n",
       "  'max_abs_difference': 0.005859375,\n",
       "  'std_difference': 0.0013472323771566153,\n",
       "  'significant_diff_ratio': 0.805908203125,\n",
       "  'cosine_similarity': 0.999989926815033,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_21_v_proj': {'frobenius_norm': 4.879202842712402,\n",
       "  'frobenius_norm_relative': 0.06693156375590155,\n",
       "  'spectral_norm': 4.879202842712402,\n",
       "  'spectral_norm_relative': 0.06693156375590155,\n",
       "  'mean_abs_difference': 0.0009478670544922352,\n",
       "  'max_abs_difference': 0.007080078125,\n",
       "  'std_difference': 0.0011915052309632301,\n",
       "  'significant_diff_ratio': 0.9733640551567078,\n",
       "  'cosine_similarity': 0.9991537928581238,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_22_v_proj': {'frobenius_norm': 4.870222091674805,\n",
       "  'frobenius_norm_relative': 0.066567108817231,\n",
       "  'spectral_norm': 4.870222091674805,\n",
       "  'spectral_norm_relative': 0.066567108817231,\n",
       "  'mean_abs_difference': 0.0009458037093281746,\n",
       "  'max_abs_difference': 0.00665283203125,\n",
       "  'std_difference': 0.0011893679620698094,\n",
       "  'significant_diff_ratio': 0.9731869697570801,\n",
       "  'cosine_similarity': 0.9990348815917969,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_22_o_proj': {'frobenius_norm': 5.014476776123047,\n",
       "  'frobenius_norm_relative': 0.06962438114711783,\n",
       "  'spectral_norm': 5.014476776123047,\n",
       "  'spectral_norm_relative': 0.06962438114711783,\n",
       "  'mean_abs_difference': 0.0009726113057695329,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0012244214303791523,\n",
       "  'significant_diff_ratio': 0.9743739366531372,\n",
       "  'cosine_similarity': 0.9988519549369812,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_12_o_proj': {'frobenius_norm': 4.416309833526611,\n",
       "  'frobenius_norm_relative': 0.07417611945010123,\n",
       "  'spectral_norm': 4.416309833526611,\n",
       "  'spectral_norm_relative': 0.07417611945010123,\n",
       "  'mean_abs_difference': 0.0008556984830647707,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0010785047197714448,\n",
       "  'significant_diff_ratio': 0.9757921695709229,\n",
       "  'cosine_similarity': 0.998267650604248,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_26_q_proj': {'frobenius_norm': 5.321805953979492,\n",
       "  'frobenius_norm_relative': 0.060231475149193754,\n",
       "  'spectral_norm': 5.321805953979492,\n",
       "  'spectral_norm_relative': 0.060231475149193754,\n",
       "  'mean_abs_difference': 0.0010325564071536064,\n",
       "  'max_abs_difference': 0.008056640625,\n",
       "  'std_difference': 0.0012996926670894027,\n",
       "  'significant_diff_ratio': 0.9711175560951233,\n",
       "  'cosine_similarity': 0.9995778203010559,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_31_gate_proj': {'frobenius_norm': 8.413201332092285,\n",
       "  'frobenius_norm_relative': 0.059064352598252,\n",
       "  'spectral_norm': 8.413201332092285,\n",
       "  'spectral_norm_relative': 0.059064352598252,\n",
       "  'mean_abs_difference': 0.0009964585769921541,\n",
       "  'max_abs_difference': 0.013671875,\n",
       "  'std_difference': 0.0012546507641673088,\n",
       "  'significant_diff_ratio': 0.9700521230697632,\n",
       "  'cosine_similarity': 1.0054513216018677,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_17_q_proj': {'frobenius_norm': 5.312648773193359,\n",
       "  'frobenius_norm_relative': 0.055923305813844096,\n",
       "  'spectral_norm': 5.312648773193359,\n",
       "  'spectral_norm_relative': 0.055923305813844096,\n",
       "  'mean_abs_difference': 0.0010297062108293176,\n",
       "  'max_abs_difference': 0.00830078125,\n",
       "  'std_difference': 0.0012976530706509948,\n",
       "  'significant_diff_ratio': 0.9690259695053101,\n",
       "  'cosine_similarity': 0.9998317956924438,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_16_down_proj': {'frobenius_norm': 8.063284873962402,\n",
       "  'frobenius_norm_relative': 0.06632824928254634,\n",
       "  'spectral_norm': 8.063284873962402,\n",
       "  'spectral_norm_relative': 0.06632824928254634,\n",
       "  'mean_abs_difference': 0.0009563038474880159,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0012020273134112358,\n",
       "  'significant_diff_ratio': 0.9729512929916382,\n",
       "  'cosine_similarity': 1.003871202468872,\n",
       "  'weight_shape': torch.Size([4096, 11008]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_1_gate_proj': {'frobenius_norm': 7.311300754547119,\n",
       "  'frobenius_norm_relative': 0.06043434170450168,\n",
       "  'spectral_norm': 7.311300754547119,\n",
       "  'spectral_norm_relative': 0.06043434170450168,\n",
       "  'mean_abs_difference': 0.0008682101033627987,\n",
       "  'max_abs_difference': 0.01171875,\n",
       "  'std_difference': 0.0010900963097810745,\n",
       "  'significant_diff_ratio': 0.9706401228904724,\n",
       "  'cosine_similarity': 1.0042961835861206,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_11_post_attention_layernorm': {'frobenius_norm': 0.1279473900794983,\n",
       "  'frobenius_norm_relative': 0.008222338833141364,\n",
       "  'spectral_norm': 0.1279473900794983,\n",
       "  'spectral_norm_relative': 0.008222338833141364,\n",
       "  'mean_abs_difference': 0.0016783475875854492,\n",
       "  'max_abs_difference': 0.0048828125,\n",
       "  'std_difference': 0.0011687172809615731,\n",
       "  'significant_diff_ratio': 0.8505859375,\n",
       "  'cosine_similarity': 0.9999914169311523,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_3_down_proj': {'frobenius_norm': 7.577291488647461,\n",
       "  'frobenius_norm_relative': 0.0647440627417357,\n",
       "  'spectral_norm': 7.577291488647461,\n",
       "  'spectral_norm_relative': 0.0647440627417357,\n",
       "  'mean_abs_difference': 0.0008998644771054387,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0011295769363641739,\n",
       "  'significant_diff_ratio': 0.9723770022392273,\n",
       "  'cosine_similarity': 1.0043100118637085,\n",
       "  'weight_shape': torch.Size([4096, 11008]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_11_down_proj': {'frobenius_norm': 7.91891622543335,\n",
       "  'frobenius_norm_relative': 0.06647033165094346,\n",
       "  'spectral_norm': 7.91891622543335,\n",
       "  'spectral_norm_relative': 0.06647033165094346,\n",
       "  'mean_abs_difference': 0.0009391416097059846,\n",
       "  'max_abs_difference': 0.01171875,\n",
       "  'std_difference': 0.001180442632175982,\n",
       "  'significant_diff_ratio': 0.9730153679847717,\n",
       "  'cosine_similarity': 1.0040627717971802,\n",
       "  'weight_shape': torch.Size([4096, 11008]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_3_k_proj': {'frobenius_norm': 4.5795769691467285,\n",
       "  'frobenius_norm_relative': 0.04306733296842791,\n",
       "  'spectral_norm': 4.5795769691467285,\n",
       "  'spectral_norm_relative': 0.04306733296842791,\n",
       "  'mean_abs_difference': 0.0008852165192365646,\n",
       "  'max_abs_difference': 0.0068359375,\n",
       "  'std_difference': 0.0011191990924999118,\n",
       "  'significant_diff_ratio': 0.9619109034538269,\n",
       "  'cosine_similarity': 1.0005735158920288,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_12_gate_proj': {'frobenius_norm': 8.49283218383789,\n",
       "  'frobenius_norm_relative': 0.06702222679800034,\n",
       "  'spectral_norm': 8.49283218383789,\n",
       "  'spectral_norm_relative': 0.06702222679800034,\n",
       "  'mean_abs_difference': 0.0010079869534820318,\n",
       "  'max_abs_difference': 0.00726318359375,\n",
       "  'std_difference': 0.0012662364169955254,\n",
       "  'significant_diff_ratio': 0.9735888242721558,\n",
       "  'cosine_similarity': 1.0035734176635742,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_6_gate_proj': {'frobenius_norm': 7.926518440246582,\n",
       "  'frobenius_norm_relative': 0.06016749490658261,\n",
       "  'spectral_norm': 7.926518440246582,\n",
       "  'spectral_norm_relative': 0.06016749490658261,\n",
       "  'mean_abs_difference': 0.000941154663451016,\n",
       "  'max_abs_difference': 0.0068359375,\n",
       "  'std_difference': 0.0011818904895335436,\n",
       "  'significant_diff_ratio': 0.9705483913421631,\n",
       "  'cosine_similarity': 1.0045710802078247,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_22_gate_proj': {'frobenius_norm': 8.77707290649414,\n",
       "  'frobenius_norm_relative': 0.06616109090398017,\n",
       "  'spectral_norm': 8.77707290649414,\n",
       "  'spectral_norm_relative': 0.06616109090398017,\n",
       "  'mean_abs_difference': 0.001042781863361597,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0013085457030683756,\n",
       "  'significant_diff_ratio': 0.9730570316314697,\n",
       "  'cosine_similarity': 1.0041927099227905,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_13_input_layernorm': {'frobenius_norm': 0.2945097088813782,\n",
       "  'frobenius_norm_relative': 0.011838565976030533,\n",
       "  'spectral_norm': 0.2945097088813782,\n",
       "  'spectral_norm_relative': 0.011838565976030533,\n",
       "  'mean_abs_difference': 0.004400855395942926,\n",
       "  'max_abs_difference': 0.009765625,\n",
       "  'std_difference': 0.0013448738027364016,\n",
       "  'significant_diff_ratio': 0.99853515625,\n",
       "  'cosine_similarity': 0.9999964833259583,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_25_k_proj': {'frobenius_norm': 5.252642631530762,\n",
       "  'frobenius_norm_relative': 0.058312784052417195,\n",
       "  'spectral_norm': 5.252642631530762,\n",
       "  'spectral_norm_relative': 0.058312784052417195,\n",
       "  'mean_abs_difference': 0.001020154682919383,\n",
       "  'max_abs_difference': 0.0076904296875,\n",
       "  'std_difference': 0.0012829340994358063,\n",
       "  'significant_diff_ratio': 0.9702855348587036,\n",
       "  'cosine_similarity': 0.9995087385177612,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_2_input_layernorm': {'frobenius_norm': 0.12187516689300537,\n",
       "  'frobenius_norm_relative': 0.010813652281523412,\n",
       "  'spectral_norm': 0.12187516689300537,\n",
       "  'spectral_norm_relative': 0.010813652281523412,\n",
       "  'mean_abs_difference': 0.0016280289273709059,\n",
       "  'max_abs_difference': 0.0048828125,\n",
       "  'std_difference': 0.0010340465232729912,\n",
       "  'significant_diff_ratio': 0.88330078125,\n",
       "  'cosine_similarity': 0.9999831914901733,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_27_v_proj': {'frobenius_norm': 4.876420021057129,\n",
       "  'frobenius_norm_relative': 0.06083455883710045,\n",
       "  'spectral_norm': 4.876420021057129,\n",
       "  'spectral_norm_relative': 0.06083455883710045,\n",
       "  'mean_abs_difference': 0.0009474256657995284,\n",
       "  'max_abs_difference': 0.006500244140625,\n",
       "  'std_difference': 0.0011910725152119994,\n",
       "  'significant_diff_ratio': 0.9706944823265076,\n",
       "  'cosine_similarity': 0.9992872476577759,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_26_gate_proj': {'frobenius_norm': 8.811647415161133,\n",
       "  'frobenius_norm_relative': 0.0659610090199579,\n",
       "  'spectral_norm': 8.811647415161133,\n",
       "  'spectral_norm_relative': 0.0659610090199579,\n",
       "  'mean_abs_difference': 0.001046657213009894,\n",
       "  'max_abs_difference': 0.008056640625,\n",
       "  'std_difference': 0.001313704182393849,\n",
       "  'significant_diff_ratio': 0.9729995131492615,\n",
       "  'cosine_similarity': 1.0043638944625854,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_5_gate_proj': {'frobenius_norm': 7.8381195068359375,\n",
       "  'frobenius_norm_relative': 0.06049924458235837,\n",
       "  'spectral_norm': 7.8381195068359375,\n",
       "  'spectral_norm_relative': 0.06049924458235837,\n",
       "  'mean_abs_difference': 0.0009307627333328128,\n",
       "  'max_abs_difference': 0.00688934326171875,\n",
       "  'std_difference': 0.001168685033917427,\n",
       "  'significant_diff_ratio': 0.9706669449806213,\n",
       "  'cosine_similarity': 1.004170298576355,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_14_post_attention_layernorm': {'frobenius_norm': 0.14219404757022858,\n",
       "  'frobenius_norm_relative': 0.00826507629826469,\n",
       "  'spectral_norm': 0.14219404757022858,\n",
       "  'spectral_norm_relative': 0.00826507629826469,\n",
       "  'mean_abs_difference': 0.0018096566200256348,\n",
       "  'max_abs_difference': 0.005859375,\n",
       "  'std_difference': 0.001361010828986764,\n",
       "  'significant_diff_ratio': 0.7490234375,\n",
       "  'cosine_similarity': 0.9999873638153076,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_20_up_proj': {'frobenius_norm': 8.37235164642334,\n",
       "  'frobenius_norm_relative': 0.06827905321541552,\n",
       "  'spectral_norm': 8.37235164642334,\n",
       "  'spectral_norm_relative': 0.06827905321541552,\n",
       "  'mean_abs_difference': 0.0009936142014339566,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0012481885496526957,\n",
       "  'significant_diff_ratio': 0.9736646413803101,\n",
       "  'cosine_similarity': 1.0036191940307617,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_14_down_proj': {'frobenius_norm': 8.035466194152832,\n",
       "  'frobenius_norm_relative': 0.06628801968347714,\n",
       "  'spectral_norm': 8.035466194152832,\n",
       "  'spectral_norm_relative': 0.06628801968347714,\n",
       "  'mean_abs_difference': 0.0009529832168482244,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0011978618567809463,\n",
       "  'significant_diff_ratio': 0.9729366898536682,\n",
       "  'cosine_similarity': 1.0039082765579224,\n",
       "  'weight_shape': torch.Size([4096, 11008]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_26_post_attention_layernorm': {'frobenius_norm': 0.27005064487457275,\n",
       "  'frobenius_norm_relative': 0.009713736340225,\n",
       "  'spectral_norm': 0.27005064487457275,\n",
       "  'spectral_norm_relative': 0.009713736340225,\n",
       "  'mean_abs_difference': 0.003977775573730469,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.001420350163243711,\n",
       "  'significant_diff_ratio': 0.991455078125,\n",
       "  'cosine_similarity': 0.9999982714653015,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_6_input_layernorm': {'frobenius_norm': 0.21336494386196136,\n",
       "  'frobenius_norm_relative': 0.01007518330977558,\n",
       "  'spectral_norm': 0.21336494386196136,\n",
       "  'spectral_norm_relative': 0.01007518330977558,\n",
       "  'mean_abs_difference': 0.0030593944247812033,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0013291712384670973,\n",
       "  'significant_diff_ratio': 0.95947265625,\n",
       "  'cosine_similarity': 0.9999922513961792,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_16_q_proj': {'frobenius_norm': 5.3896074295043945,\n",
       "  'frobenius_norm_relative': 0.05629930536251091,\n",
       "  'spectral_norm': 5.3896074295043945,\n",
       "  'spectral_norm_relative': 0.05629930536251091,\n",
       "  'mean_abs_difference': 0.0010451240232214332,\n",
       "  'max_abs_difference': 0.0080413818359375,\n",
       "  'std_difference': 0.001316390698775649,\n",
       "  'significant_diff_ratio': 0.969587504863739,\n",
       "  'cosine_similarity': 0.9998968839645386,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_20_k_proj': {'frobenius_norm': 5.356678009033203,\n",
       "  'frobenius_norm_relative': 0.05695513987905455,\n",
       "  'spectral_norm': 5.356678009033203,\n",
       "  'spectral_norm_relative': 0.05695513987905455,\n",
       "  'mean_abs_difference': 0.001038905931636691,\n",
       "  'max_abs_difference': 0.00732421875,\n",
       "  'std_difference': 0.0013084152014926076,\n",
       "  'significant_diff_ratio': 0.9698905348777771,\n",
       "  'cosine_similarity': 0.9998575448989868,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_10_post_attention_layernorm': {'frobenius_norm': 0.12156364321708679,\n",
       "  'frobenius_norm_relative': 0.00806576181934551,\n",
       "  'spectral_norm': 0.12156364321708679,\n",
       "  'spectral_norm_relative': 0.00806576181934551,\n",
       "  'mean_abs_difference': 0.0016049742698669434,\n",
       "  'max_abs_difference': 0.005859375,\n",
       "  'std_difference': 0.0011078525567427278,\n",
       "  'significant_diff_ratio': 0.86962890625,\n",
       "  'cosine_similarity': 0.9999924898147583,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_20_down_proj': {'frobenius_norm': 8.306987762451172,\n",
       "  'frobenius_norm_relative': 0.0683307950171292,\n",
       "  'spectral_norm': 8.306987762451172,\n",
       "  'spectral_norm_relative': 0.0683307950171292,\n",
       "  'mean_abs_difference': 0.0009861363796517253,\n",
       "  'max_abs_difference': 0.015625,\n",
       "  'std_difference': 0.0012384267756715417,\n",
       "  'significant_diff_ratio': 0.9737603664398193,\n",
       "  'cosine_similarity': 1.0037128925323486,\n",
       "  'weight_shape': torch.Size([4096, 11008]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_21_q_proj': {'frobenius_norm': 5.3157196044921875,\n",
       "  'frobenius_norm_relative': 0.05886154094391835,\n",
       "  'spectral_norm': 5.3157196044921875,\n",
       "  'spectral_norm_relative': 0.05886154094391835,\n",
       "  'mean_abs_difference': 0.0010316901607438922,\n",
       "  'max_abs_difference': 0.013671875,\n",
       "  'std_difference': 0.0012983259512111545,\n",
       "  'significant_diff_ratio': 0.9706134796142578,\n",
       "  'cosine_similarity': 0.9994752407073975,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_9_gate_proj': {'frobenius_norm': 8.3024263381958,\n",
       "  'frobenius_norm_relative': 0.06466808343709984,\n",
       "  'spectral_norm': 8.3024263381958,\n",
       "  'spectral_norm_relative': 0.06466808343709984,\n",
       "  'mean_abs_difference': 0.0009854938834905624,\n",
       "  'max_abs_difference': 0.007568359375,\n",
       "  'std_difference': 0.0012378881219774485,\n",
       "  'significant_diff_ratio': 0.9726201295852661,\n",
       "  'cosine_similarity': 1.0037719011306763,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_19_q_proj': {'frobenius_norm': 5.330599308013916,\n",
       "  'frobenius_norm_relative': 0.05808797848278871,\n",
       "  'spectral_norm': 5.330599308013916,\n",
       "  'spectral_norm_relative': 0.05808797848278871,\n",
       "  'mean_abs_difference': 0.0010338914580643177,\n",
       "  'max_abs_difference': 0.01171875,\n",
       "  'std_difference': 0.0013019619509577751,\n",
       "  'significant_diff_ratio': 0.9701717495918274,\n",
       "  'cosine_similarity': 0.9995095729827881,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_7_q_proj': {'frobenius_norm': 5.00929069519043,\n",
       "  'frobenius_norm_relative': 0.049606009466210355,\n",
       "  'spectral_norm': 5.00929069519043,\n",
       "  'spectral_norm_relative': 0.049606009466210355,\n",
       "  'mean_abs_difference': 0.0009708308498375118,\n",
       "  'max_abs_difference': 0.007080078125,\n",
       "  'std_difference': 0.0012238039635121822,\n",
       "  'significant_diff_ratio': 0.9661020040512085,\n",
       "  'cosine_similarity': 1.0001767873764038,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_30_q_proj': {'frobenius_norm': 5.335108757019043,\n",
       "  'frobenius_norm_relative': 0.06108547575074326,\n",
       "  'spectral_norm': 5.335108757019043,\n",
       "  'spectral_norm_relative': 0.06108547575074326,\n",
       "  'mean_abs_difference': 0.0010352940298616886,\n",
       "  'max_abs_difference': 0.009765625,\n",
       "  'std_difference': 0.0013029645197093487,\n",
       "  'significant_diff_ratio': 0.9713774919509888,\n",
       "  'cosine_similarity': 0.9992645382881165,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_1_k_proj': {'frobenius_norm': 3.8670010566711426,\n",
       "  'frobenius_norm_relative': 0.036090438541867366,\n",
       "  'spectral_norm': 3.8670010566711426,\n",
       "  'spectral_norm_relative': 0.036090438541867366,\n",
       "  'mean_abs_difference': 0.000740374147426337,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0009452746598981321,\n",
       "  'significant_diff_ratio': 0.9603727459907532,\n",
       "  'cosine_similarity': 1.0010056495666504,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_13_gate_proj': {'frobenius_norm': 8.498128890991211,\n",
       "  'frobenius_norm_relative': 0.06730882475302699,\n",
       "  'spectral_norm': 8.498128890991211,\n",
       "  'spectral_norm_relative': 0.06730882475302699,\n",
       "  'mean_abs_difference': 0.001008506747893989,\n",
       "  'max_abs_difference': 0.00732421875,\n",
       "  'std_difference': 0.0012670363066717982,\n",
       "  'significant_diff_ratio': 0.9737247228622437,\n",
       "  'cosine_similarity': 1.003535270690918,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_27_q_proj': {'frobenius_norm': 5.1969733238220215,\n",
       "  'frobenius_norm_relative': 0.05670091953052372,\n",
       "  'spectral_norm': 5.1969733238220215,\n",
       "  'spectral_norm_relative': 0.05670091953052372,\n",
       "  'mean_abs_difference': 0.0010088107082992792,\n",
       "  'max_abs_difference': 0.00927734375,\n",
       "  'std_difference': 0.0012694415636360645,\n",
       "  'significant_diff_ratio': 0.9690819978713989,\n",
       "  'cosine_similarity': 0.9995550513267517,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_30_up_proj': {'frobenius_norm': 8.418402671813965,\n",
       "  'frobenius_norm_relative': 0.06504543708575124,\n",
       "  'spectral_norm': 8.418402671813965,\n",
       "  'spectral_norm_relative': 0.06504543708575124,\n",
       "  'mean_abs_difference': 0.0009966341312974691,\n",
       "  'max_abs_difference': 0.0185546875,\n",
       "  'std_difference': 0.0012551960535347462,\n",
       "  'significant_diff_ratio': 0.9726055860519409,\n",
       "  'cosine_similarity': 1.0038806200027466,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_9_v_proj': {'frobenius_norm': 4.381922721862793,\n",
       "  'frobenius_norm_relative': 0.07438238926223756,\n",
       "  'spectral_norm': 4.381922721862793,\n",
       "  'spectral_norm_relative': 0.07438238926223756,\n",
       "  'mean_abs_difference': 0.0008501394768245518,\n",
       "  'max_abs_difference': 0.00640869140625,\n",
       "  'std_difference': 0.0010701532009989023,\n",
       "  'significant_diff_ratio': 0.9759843945503235,\n",
       "  'cosine_similarity': 0.9983159303665161,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_6_q_proj': {'frobenius_norm': 4.9914374351501465,\n",
       "  'frobenius_norm_relative': 0.04934326684589363,\n",
       "  'spectral_norm': 4.9914374351501465,\n",
       "  'spectral_norm_relative': 0.04934326684589363,\n",
       "  'mean_abs_difference': 0.0009676441550254822,\n",
       "  'max_abs_difference': 0.00701904296875,\n",
       "  'std_difference': 0.001219457946717739,\n",
       "  'significant_diff_ratio': 0.9659169316291809,\n",
       "  'cosine_similarity': 1.0002386569976807,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_19_down_proj': {'frobenius_norm': 8.304408073425293,\n",
       "  'frobenius_norm_relative': 0.06829426156478931,\n",
       "  'spectral_norm': 8.304408073425293,\n",
       "  'spectral_norm_relative': 0.06829426156478931,\n",
       "  'mean_abs_difference': 0.0009856141405180097,\n",
       "  'max_abs_difference': 0.01171875,\n",
       "  'std_difference': 0.0012380407424643636,\n",
       "  'significant_diff_ratio': 0.9736819863319397,\n",
       "  'cosine_similarity': 1.0037070512771606,\n",
       "  'weight_shape': torch.Size([4096, 11008]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_22_post_attention_layernorm': {'frobenius_norm': 0.2164146602153778,\n",
       "  'frobenius_norm_relative': 0.008812626756083942,\n",
       "  'spectral_norm': 0.2164146602153778,\n",
       "  'spectral_norm_relative': 0.008812626756083942,\n",
       "  'mean_abs_difference': 0.0030868053436279297,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0013908453984186053,\n",
       "  'significant_diff_ratio': 0.9541015625,\n",
       "  'cosine_similarity': 0.9999954700469971,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_6_o_proj': {'frobenius_norm': 4.4882025718688965,\n",
       "  'frobenius_norm_relative': 0.0819626140585392,\n",
       "  'spectral_norm': 4.4882025718688965,\n",
       "  'spectral_norm_relative': 0.0819626140585392,\n",
       "  'mean_abs_difference': 0.0008702456834726036,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0010958786588162184,\n",
       "  'significant_diff_ratio': 0.9779826402664185,\n",
       "  'cosine_similarity': 0.9978708028793335,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_11_input_layernorm': {'frobenius_norm': 0.2766379415988922,\n",
       "  'frobenius_norm_relative': 0.011361792669613562,\n",
       "  'spectral_norm': 0.2766379415988922,\n",
       "  'spectral_norm_relative': 0.011361792669613562,\n",
       "  'mean_abs_difference': 0.004114169627428055,\n",
       "  'max_abs_difference': 0.009765625,\n",
       "  'std_difference': 0.001325808698311448,\n",
       "  'significant_diff_ratio': 0.99755859375,\n",
       "  'cosine_similarity': 0.9999959468841553,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_20_input_layernorm': {'frobenius_norm': 0.2916102111339569,\n",
       "  'frobenius_norm_relative': 0.010376312634160978,\n",
       "  'spectral_norm': 0.2916102111339569,\n",
       "  'spectral_norm_relative': 0.010376312634160978,\n",
       "  'mean_abs_difference': 0.004353410564363003,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0013465802185237408,\n",
       "  'significant_diff_ratio': 0.998291015625,\n",
       "  'cosine_similarity': 0.9999988079071045,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_28_gate_proj': {'frobenius_norm': 8.845413208007812,\n",
       "  'frobenius_norm_relative': 0.0664297459423523,\n",
       "  'spectral_norm': 8.845413208007812,\n",
       "  'spectral_norm_relative': 0.0664297459423523,\n",
       "  'mean_abs_difference': 0.00105039041955024,\n",
       "  'max_abs_difference': 0.009765625,\n",
       "  'std_difference': 0.0013187070144340396,\n",
       "  'significant_diff_ratio': 0.9732878804206848,\n",
       "  'cosine_similarity': 1.004284143447876,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_7_up_proj': {'frobenius_norm': 7.707221508026123,\n",
       "  'frobenius_norm_relative': 0.06645271422110637,\n",
       "  'spectral_norm': 7.707221508026123,\n",
       "  'spectral_norm_relative': 0.06645271422110637,\n",
       "  'mean_abs_difference': 0.0009143789066001773,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0011488894233480096,\n",
       "  'significant_diff_ratio': 0.973019003868103,\n",
       "  'cosine_similarity': 1.0042904615402222,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_18_o_proj': {'frobenius_norm': 4.791514873504639,\n",
       "  'frobenius_norm_relative': 0.07035233975716941,\n",
       "  'spectral_norm': 4.791514873504639,\n",
       "  'spectral_norm_relative': 0.07035233975716941,\n",
       "  'mean_abs_difference': 0.0009299881057813764,\n",
       "  'max_abs_difference': 0.00665283203125,\n",
       "  'std_difference': 0.0011700257891789079,\n",
       "  'significant_diff_ratio': 0.9744356870651245,\n",
       "  'cosine_similarity': 0.9986652731895447,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_18_post_attention_layernorm': {'frobenius_norm': 0.1800546646118164,\n",
       "  'frobenius_norm_relative': 0.008440091581621647,\n",
       "  'spectral_norm': 0.1800546646118164,\n",
       "  'spectral_norm_relative': 0.008440091581621647,\n",
       "  'mean_abs_difference': 0.0024721622467041016,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0013647815212607384,\n",
       "  'significant_diff_ratio': 0.886474609375,\n",
       "  'cosine_similarity': 0.9999916553497314,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_28_input_layernorm': {'frobenius_norm': 0.3702763319015503,\n",
       "  'frobenius_norm_relative': 0.010518919637492987,\n",
       "  'spectral_norm': 0.3702763319015503,\n",
       "  'spectral_norm_relative': 0.010518919637492987,\n",
       "  'mean_abs_difference': 0.005391884595155716,\n",
       "  'max_abs_difference': 0.01171875,\n",
       "  'std_difference': 0.0020979628898203373,\n",
       "  'significant_diff_ratio': 0.977294921875,\n",
       "  'cosine_similarity': 0.9999929666519165,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'embed_tokens': {'frobenius_norm': 10.981884956359863,\n",
       "  'frobenius_norm_relative': 0.057915589966475875,\n",
       "  'spectral_norm': 10.981884956359863,\n",
       "  'spectral_norm_relative': 0.057915589966475875,\n",
       "  'mean_abs_difference': 0.0007397193694487214,\n",
       "  'max_abs_difference': 0.006591796875,\n",
       "  'std_difference': 0.0009699251968413591,\n",
       "  'significant_diff_ratio': 0.961707592010498,\n",
       "  'cosine_similarity': 1.0292723178863525,\n",
       "  'weight_shape': torch.Size([32000, 4096]),\n",
       "  'total_parameters': 131072000},\n",
       " 'final_norm': {'frobenius_norm': 0.6897059679031372,\n",
       "  'frobenius_norm_relative': 0.006069600970057737,\n",
       "  'spectral_norm': 0.6897059679031372,\n",
       "  'spectral_norm_relative': 0.006069600970057737,\n",
       "  'mean_abs_difference': 0.010121654719114304,\n",
       "  'max_abs_difference': 0.03125,\n",
       "  'std_difference': 0.0037019301671534777,\n",
       "  'significant_diff_ratio': 0.993896484375,\n",
       "  'cosine_similarity': 1.0000014305114746,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_26_input_layernorm': {'frobenius_norm': 0.36475661396980286,\n",
       "  'frobenius_norm_relative': 0.010862169839446961,\n",
       "  'spectral_norm': 0.36475661396980286,\n",
       "  'spectral_norm_relative': 0.010862169839446961,\n",
       "  'mean_abs_difference': 0.005320478230714798,\n",
       "  'max_abs_difference': 0.015625,\n",
       "  'std_difference': 0.0020434781908988953,\n",
       "  'significant_diff_ratio': 0.980224609375,\n",
       "  'cosine_similarity': 0.9999926090240479,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_9_up_proj': {'frobenius_norm': 7.905624866485596,\n",
       "  'frobenius_norm_relative': 0.06686323705602694,\n",
       "  'spectral_norm': 7.905624866485596,\n",
       "  'spectral_norm_relative': 0.06686323705602694,\n",
       "  'mean_abs_difference': 0.0009376067901030183,\n",
       "  'max_abs_difference': 0.0072021484375,\n",
       "  'std_difference': 0.001178458333015442,\n",
       "  'significant_diff_ratio': 0.9731096625328064,\n",
       "  'cosine_similarity': 1.0040717124938965,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_11_k_proj': {'frobenius_norm': 5.303076267242432,\n",
       "  'frobenius_norm_relative': 0.05489474742685855,\n",
       "  'spectral_norm': 5.303076267242432,\n",
       "  'spectral_norm_relative': 0.05489474742685855,\n",
       "  'mean_abs_difference': 0.0010285121388733387,\n",
       "  'max_abs_difference': 0.00732421875,\n",
       "  'std_difference': 0.0012952511897310615,\n",
       "  'significant_diff_ratio': 0.9695286750793457,\n",
       "  'cosine_similarity': 0.9999884366989136,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_28_q_proj': {'frobenius_norm': 5.270599365234375,\n",
       "  'frobenius_norm_relative': 0.05916211243336499,\n",
       "  'spectral_norm': 5.270599365234375,\n",
       "  'spectral_norm_relative': 0.05916211243336499,\n",
       "  'mean_abs_difference': 0.0010230974294245243,\n",
       "  'max_abs_difference': 0.015625,\n",
       "  'std_difference': 0.0012872683582827449,\n",
       "  'significant_diff_ratio': 0.9705202579498291,\n",
       "  'cosine_similarity': 0.9993804693222046,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_21_up_proj': {'frobenius_norm': 8.44128704071045,\n",
       "  'frobenius_norm_relative': 0.0689499395574666,\n",
       "  'spectral_norm': 8.44128704071045,\n",
       "  'spectral_norm_relative': 0.0689499395574666,\n",
       "  'mean_abs_difference': 0.0010019522160291672,\n",
       "  'max_abs_difference': 0.009765625,\n",
       "  'std_difference': 0.0012584526557475328,\n",
       "  'significant_diff_ratio': 0.9739883542060852,\n",
       "  'cosine_similarity': 1.0035779476165771,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_10_o_proj': {'frobenius_norm': 4.326534271240234,\n",
       "  'frobenius_norm_relative': 0.07477565490452871,\n",
       "  'spectral_norm': 4.326534271240234,\n",
       "  'spectral_norm_relative': 0.07477565490452871,\n",
       "  'mean_abs_difference': 0.0008384695975109935,\n",
       "  'max_abs_difference': 0.0062255859375,\n",
       "  'std_difference': 0.0010566383134573698,\n",
       "  'significant_diff_ratio': 0.9758691787719727,\n",
       "  'cosine_similarity': 0.9982815980911255,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_1_o_proj': {'frobenius_norm': 4.284850597381592,\n",
       "  'frobenius_norm_relative': 0.12753447212455707,\n",
       "  'spectral_norm': 4.284850597381592,\n",
       "  'spectral_norm_relative': 0.12753447212455707,\n",
       "  'mean_abs_difference': 0.0008328189142048359,\n",
       "  'max_abs_difference': 0.005859375,\n",
       "  'std_difference': 0.0010462022619321942,\n",
       "  'significant_diff_ratio': 0.9874928593635559,\n",
       "  'cosine_similarity': 0.9937450885772705,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_16_v_proj': {'frobenius_norm': 4.459264755249023,\n",
       "  'frobenius_norm_relative': 0.06735369807368205,\n",
       "  'spectral_norm': 4.459264755249023,\n",
       "  'spectral_norm_relative': 0.06735369807368205,\n",
       "  'mean_abs_difference': 0.0008642047178000212,\n",
       "  'max_abs_difference': 0.00689697265625,\n",
       "  'std_difference': 0.0010891611455008388,\n",
       "  'significant_diff_ratio': 0.9734370708465576,\n",
       "  'cosine_similarity': 0.99866783618927,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_21_o_proj': {'frobenius_norm': 5.080254077911377,\n",
       "  'frobenius_norm_relative': 0.07070374819150534,\n",
       "  'spectral_norm': 5.080254077911377,\n",
       "  'spectral_norm_relative': 0.07070374819150534,\n",
       "  'mean_abs_difference': 0.0009865862084552646,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0012404401786625385,\n",
       "  'significant_diff_ratio': 0.9746689796447754,\n",
       "  'cosine_similarity': 0.9987472891807556,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_14_input_layernorm': {'frobenius_norm': 0.30667370557785034,\n",
       "  'frobenius_norm_relative': 0.012040701862372771,\n",
       "  'spectral_norm': 0.30667370557785034,\n",
       "  'spectral_norm_relative': 0.012040701862372771,\n",
       "  'mean_abs_difference': 0.0045935227535665035,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.001364230178296566,\n",
       "  'significant_diff_ratio': 0.9990234375,\n",
       "  'cosine_similarity': 0.9999969601631165,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_18_k_proj': {'frobenius_norm': 5.291459560394287,\n",
       "  'frobenius_norm_relative': 0.055730429645839324,\n",
       "  'spectral_norm': 5.291459560394287,\n",
       "  'spectral_norm_relative': 0.055730429645839324,\n",
       "  'mean_abs_difference': 0.0010249504121020436,\n",
       "  'max_abs_difference': 0.007659912109375,\n",
       "  'std_difference': 0.0012925165938213468,\n",
       "  'significant_diff_ratio': 0.9688745141029358,\n",
       "  'cosine_similarity': 0.9998249411582947,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_25_post_attention_layernorm': {'frobenius_norm': 0.25286558270454407,\n",
       "  'frobenius_norm_relative': 0.0094085192235228,\n",
       "  'spectral_norm': 0.25286558270454407,\n",
       "  'spectral_norm_relative': 0.0094085192235228,\n",
       "  'mean_abs_difference': 0.003692150115966797,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0014186420012265444,\n",
       "  'significant_diff_ratio': 0.98193359375,\n",
       "  'cosine_similarity': 0.9999974370002747,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_0_post_attention_layernorm': {'frobenius_norm': 0.06904205679893494,\n",
       "  'frobenius_norm_relative': 0.019547686401994592,\n",
       "  'spectral_norm': 0.06904205679893494,\n",
       "  'spectral_norm_relative': 0.019547686401994592,\n",
       "  'mean_abs_difference': 0.0008602877496741712,\n",
       "  'max_abs_difference': 0.00439453125,\n",
       "  'std_difference': 0.0009471190278418362,\n",
       "  'significant_diff_ratio': 0.898681640625,\n",
       "  'cosine_similarity': 0.999854564666748,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_23_v_proj': {'frobenius_norm': 4.85582971572876,\n",
       "  'frobenius_norm_relative': 0.06339599946682227,\n",
       "  'spectral_norm': 4.85582971572876,\n",
       "  'spectral_norm_relative': 0.06339599946682227,\n",
       "  'mean_abs_difference': 0.0009431222570128739,\n",
       "  'max_abs_difference': 0.007293701171875,\n",
       "  'std_difference': 0.0011859370861202478,\n",
       "  'significant_diff_ratio': 0.9718872308731079,\n",
       "  'cosine_similarity': 0.9992371797561646,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_23_o_proj': {'frobenius_norm': 5.061686992645264,\n",
       "  'frobenius_norm_relative': 0.06690395447485839,\n",
       "  'spectral_norm': 5.061686992645264,\n",
       "  'spectral_norm_relative': 0.06690395447485839,\n",
       "  'mean_abs_difference': 0.0009831442730501294,\n",
       "  'max_abs_difference': 0.00677490234375,\n",
       "  'std_difference': 0.0012360257096588612,\n",
       "  'significant_diff_ratio': 0.973329484462738,\n",
       "  'cosine_similarity': 0.9990488290786743,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_0_down_proj': {'frobenius_norm': 7.330352783203125,\n",
       "  'frobenius_norm_relative': 0.06567294598936273,\n",
       "  'spectral_norm': 7.330352783203125,\n",
       "  'spectral_norm_relative': 0.06567294598936273,\n",
       "  'mean_abs_difference': 0.0008695260039530694,\n",
       "  'max_abs_difference': 0.006561279296875,\n",
       "  'std_difference': 0.001092742197215557,\n",
       "  'significant_diff_ratio': 0.9728407859802246,\n",
       "  'cosine_similarity': 1.0047093629837036,\n",
       "  'weight_shape': torch.Size([4096, 11008]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_5_o_proj': {'frobenius_norm': 4.421146869659424,\n",
       "  'frobenius_norm_relative': 0.07551400708216917,\n",
       "  'spectral_norm': 4.421146869659424,\n",
       "  'spectral_norm_relative': 0.07551400708216917,\n",
       "  'mean_abs_difference': 0.0008578454726375639,\n",
       "  'max_abs_difference': 0.0062255859375,\n",
       "  'std_difference': 0.0010796661954373121,\n",
       "  'significant_diff_ratio': 0.9762672781944275,\n",
       "  'cosine_similarity': 0.9982259273529053,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_7_down_proj': {'frobenius_norm': 7.75740909576416,\n",
       "  'frobenius_norm_relative': 0.0672678551427072,\n",
       "  'spectral_norm': 7.75740909576416,\n",
       "  'spectral_norm_relative': 0.0672678551427072,\n",
       "  'mean_abs_difference': 0.000920667196623981,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0011563372099772096,\n",
       "  'significant_diff_ratio': 0.973381757736206,\n",
       "  'cosine_similarity': 1.004298448562622,\n",
       "  'weight_shape': torch.Size([4096, 11008]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_14_o_proj': {'frobenius_norm': 4.389196872711182,\n",
       "  'frobenius_norm_relative': 0.07264744408065253,\n",
       "  'spectral_norm': 4.389196872711182,\n",
       "  'spectral_norm_relative': 0.07264744408065253,\n",
       "  'mean_abs_difference': 0.0008508003666065633,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0010719376150518656,\n",
       "  'significant_diff_ratio': 0.9751731157302856,\n",
       "  'cosine_similarity': 0.9983190298080444,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_26_k_proj': {'frobenius_norm': 5.332941055297852,\n",
       "  'frobenius_norm_relative': 0.05988596209850361,\n",
       "  'spectral_norm': 5.332941055297852,\n",
       "  'spectral_norm_relative': 0.05988596209850361,\n",
       "  'mean_abs_difference': 0.0010347208008170128,\n",
       "  'max_abs_difference': 0.009765625,\n",
       "  'std_difference': 0.001302430871874094,\n",
       "  'significant_diff_ratio': 0.9711325764656067,\n",
       "  'cosine_similarity': 0.9996031522750854,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_29_q_proj': {'frobenius_norm': 5.417647838592529,\n",
       "  'frobenius_norm_relative': 0.06242447503051387,\n",
       "  'spectral_norm': 5.417647838592529,\n",
       "  'spectral_norm_relative': 0.06242447503051387,\n",
       "  'mean_abs_difference': 0.0010521032381802797,\n",
       "  'max_abs_difference': 0.01318359375,\n",
       "  'std_difference': 0.0013230439508333802,\n",
       "  'significant_diff_ratio': 0.9722159504890442,\n",
       "  'cosine_similarity': 0.9993367195129395,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_13_up_proj': {'frobenius_norm': 8.09200668334961,\n",
       "  'frobenius_norm_relative': 0.06587627512996319,\n",
       "  'spectral_norm': 8.09200668334961,\n",
       "  'spectral_norm_relative': 0.06587627512996319,\n",
       "  'mean_abs_difference': 0.0009594949660822749,\n",
       "  'max_abs_difference': 0.0074462890625,\n",
       "  'std_difference': 0.0012063729809597135,\n",
       "  'significant_diff_ratio': 0.972716748714447,\n",
       "  'cosine_similarity': 1.0037996768951416,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_17_v_proj': {'frobenius_norm': 4.448672294616699,\n",
       "  'frobenius_norm_relative': 0.06742298688338008,\n",
       "  'spectral_norm': 4.448672294616699,\n",
       "  'spectral_norm_relative': 0.06742298688338008,\n",
       "  'mean_abs_difference': 0.0008623851463198662,\n",
       "  'max_abs_difference': 0.00628662109375,\n",
       "  'std_difference': 0.0010865428484976292,\n",
       "  'significant_diff_ratio': 0.9734436273574829,\n",
       "  'cosine_similarity': 0.9987910389900208,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_21_post_attention_layernorm': {'frobenius_norm': 0.20223864912986755,\n",
       "  'frobenius_norm_relative': 0.008538323896268433,\n",
       "  'spectral_norm': 0.20223864912986755,\n",
       "  'spectral_norm_relative': 0.008538323896268433,\n",
       "  'mean_abs_difference': 0.0028361082077026367,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0014009667793288827,\n",
       "  'significant_diff_ratio': 0.92919921875,\n",
       "  'cosine_similarity': 0.9999940395355225,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_23_input_layernorm': {'frobenius_norm': 0.32350799441337585,\n",
       "  'frobenius_norm_relative': 0.00997589121351512,\n",
       "  'spectral_norm': 0.32350799441337585,\n",
       "  'spectral_norm_relative': 0.00997589121351512,\n",
       "  'mean_abs_difference': 0.004676243755966425,\n",
       "  'max_abs_difference': 0.015625,\n",
       "  'std_difference': 0.001919575734063983,\n",
       "  'significant_diff_ratio': 0.9619140625,\n",
       "  'cosine_similarity': 0.9999938607215881,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_2_o_proj': {'frobenius_norm': 4.414910793304443,\n",
       "  'frobenius_norm_relative': 0.07811529437129157,\n",
       "  'spectral_norm': 4.414910793304443,\n",
       "  'spectral_norm_relative': 0.07811529437129157,\n",
       "  'mean_abs_difference': 0.0008579519344493747,\n",
       "  'max_abs_difference': 0.005950927734375,\n",
       "  'std_difference': 0.0010780743323266506,\n",
       "  'significant_diff_ratio': 0.977191150188446,\n",
       "  'cosine_similarity': 0.9981061220169067,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_18_gate_proj': {'frobenius_norm': 8.674532890319824,\n",
       "  'frobenius_norm_relative': 0.06679616217164384,\n",
       "  'spectral_norm': 8.674532890319824,\n",
       "  'spectral_norm_relative': 0.06679616217164384,\n",
       "  'mean_abs_difference': 0.0010299839777871966,\n",
       "  'max_abs_difference': 0.008056640625,\n",
       "  'std_difference': 0.0012933069374412298,\n",
       "  'significant_diff_ratio': 0.973410964012146,\n",
       "  'cosine_similarity': 1.0038163661956787,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_31_o_proj': {'frobenius_norm': 4.912502288818359,\n",
       "  'frobenius_norm_relative': 0.06277010804898855,\n",
       "  'spectral_norm': 4.912502288818359,\n",
       "  'spectral_norm_relative': 0.06277010804898855,\n",
       "  'mean_abs_difference': 0.0009500551968812943,\n",
       "  'max_abs_difference': 0.015625,\n",
       "  'std_difference': 0.0011997476685792208,\n",
       "  'significant_diff_ratio': 0.9717007875442505,\n",
       "  'cosine_similarity': 0.9993681907653809,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_22_q_proj': {'frobenius_norm': 5.24020528793335,\n",
       "  'frobenius_norm_relative': 0.056827845664843446,\n",
       "  'spectral_norm': 5.24020528793335,\n",
       "  'spectral_norm_relative': 0.056827845664843446,\n",
       "  'mean_abs_difference': 0.0010160889942198992,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0012799483956769109,\n",
       "  'significant_diff_ratio': 0.9693853855133057,\n",
       "  'cosine_similarity': 0.9998118281364441,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_8_o_proj': {'frobenius_norm': 4.410915374755859,\n",
       "  'frobenius_norm_relative': 0.07760407669125106,\n",
       "  'spectral_norm': 4.410915374755859,\n",
       "  'spectral_norm_relative': 0.07760407669125106,\n",
       "  'mean_abs_difference': 0.0008548144833184779,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0010771312518045306,\n",
       "  'significant_diff_ratio': 0.9767356514930725,\n",
       "  'cosine_similarity': 0.998130738735199,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_22_input_layernorm': {'frobenius_norm': 0.3001353144645691,\n",
       "  'frobenius_norm_relative': 0.00990765150755374,\n",
       "  'spectral_norm': 0.3001353144645691,\n",
       "  'spectral_norm_relative': 0.00990765150755374,\n",
       "  'mean_abs_difference': 0.00447250297293067,\n",
       "  'max_abs_difference': 0.01171875,\n",
       "  'std_difference': 0.0014106185408309102,\n",
       "  'significant_diff_ratio': 0.997314453125,\n",
       "  'cosine_similarity': 0.9999991059303284,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_5_up_proj': {'frobenius_norm': 7.61134147644043,\n",
       "  'frobenius_norm_relative': 0.06563303938054005,\n",
       "  'spectral_norm': 7.61134147644043,\n",
       "  'spectral_norm_relative': 0.06563303938054005,\n",
       "  'mean_abs_difference': 0.0009033440728671849,\n",
       "  'max_abs_difference': 0.00647735595703125,\n",
       "  'std_difference': 0.001134641352109611,\n",
       "  'significant_diff_ratio': 0.9727516174316406,\n",
       "  'cosine_similarity': 1.004326581954956,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_12_input_layernorm': {'frobenius_norm': 0.2822675406932831,\n",
       "  'frobenius_norm_relative': 0.011543106690251234,\n",
       "  'spectral_norm': 0.2822675406932831,\n",
       "  'spectral_norm_relative': 0.011543106690251234,\n",
       "  'mean_abs_difference': 0.004198194481432438,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0013518526684492826,\n",
       "  'significant_diff_ratio': 0.997802734375,\n",
       "  'cosine_similarity': 0.9999958276748657,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_7_post_attention_layernorm': {'frobenius_norm': 0.13879646360874176,\n",
       "  'frobenius_norm_relative': 0.009768544323890186,\n",
       "  'spectral_norm': 0.13879646360874176,\n",
       "  'spectral_norm_relative': 0.009768544323890186,\n",
       "  'mean_abs_difference': 0.0019086599349975586,\n",
       "  'max_abs_difference': 0.0048828125,\n",
       "  'std_difference': 0.0010703192092478275,\n",
       "  'significant_diff_ratio': 0.92724609375,\n",
       "  'cosine_similarity': 0.9999918341636658,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_2_gate_proj': {'frobenius_norm': 7.436445236206055,\n",
       "  'frobenius_norm_relative': 0.05978342937917467,\n",
       "  'spectral_norm': 7.436445236206055,\n",
       "  'spectral_norm_relative': 0.05978342937917467,\n",
       "  'mean_abs_difference': 0.0008836511406116188,\n",
       "  'max_abs_difference': 0.006378173828125,\n",
       "  'std_difference': 0.0011087949387729168,\n",
       "  'significant_diff_ratio': 0.9701905250549316,\n",
       "  'cosine_similarity': 1.0040292739868164,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_20_post_attention_layernorm': {'frobenius_norm': 0.1962137371301651,\n",
       "  'frobenius_norm_relative': 0.008578339949652805,\n",
       "  'spectral_norm': 0.1962137371301651,\n",
       "  'spectral_norm_relative': 0.008578339949652805,\n",
       "  'mean_abs_difference': 0.0027413368225097656,\n",
       "  'max_abs_difference': 0.0068359375,\n",
       "  'std_difference': 0.0013899286277592182,\n",
       "  'significant_diff_ratio': 0.92041015625,\n",
       "  'cosine_similarity': 0.999992847442627,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_23_k_proj': {'frobenius_norm': 5.261019229888916,\n",
       "  'frobenius_norm_relative': 0.05702244021514039,\n",
       "  'spectral_norm': 5.261019229888916,\n",
       "  'spectral_norm_relative': 0.05702244021514039,\n",
       "  'mean_abs_difference': 0.001020663185045123,\n",
       "  'max_abs_difference': 0.0077056884765625,\n",
       "  'std_difference': 0.0012850731145590544,\n",
       "  'significant_diff_ratio': 0.9694520235061646,\n",
       "  'cosine_similarity': 0.9997819066047668,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_0_o_proj': {'frobenius_norm': 4.033174514770508,\n",
       "  'frobenius_norm_relative': 0.1380827882238269,\n",
       "  'spectral_norm': 4.033174514770508,\n",
       "  'spectral_norm_relative': 0.1380827882238269,\n",
       "  'mean_abs_difference': 0.0007826546207070351,\n",
       "  'max_abs_difference': 0.005859375,\n",
       "  'std_difference': 0.0009846893372014165,\n",
       "  'significant_diff_ratio': 0.9886963963508606,\n",
       "  'cosine_similarity': 0.9923021793365479,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_19_up_proj': {'frobenius_norm': 8.35304069519043,\n",
       "  'frobenius_norm_relative': 0.06806836381789393,\n",
       "  'spectral_norm': 8.35304069519043,\n",
       "  'spectral_norm_relative': 0.06806836381789393,\n",
       "  'mean_abs_difference': 0.0009911861270666122,\n",
       "  'max_abs_difference': 0.0076904296875,\n",
       "  'std_difference': 0.001245292485691607,\n",
       "  'significant_diff_ratio': 0.9735836386680603,\n",
       "  'cosine_similarity': 1.0036407709121704,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_11_gate_proj': {'frobenius_norm': 8.468936920166016,\n",
       "  'frobenius_norm_relative': 0.06640998051740638,\n",
       "  'spectral_norm': 8.468936920166016,\n",
       "  'spectral_norm_relative': 0.06640998051740638,\n",
       "  'mean_abs_difference': 0.0010051733115687966,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.001262707868590951,\n",
       "  'significant_diff_ratio': 0.9733859300613403,\n",
       "  'cosine_similarity': 1.003607153892517,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_19_input_layernorm': {'frobenius_norm': 0.27105724811553955,\n",
       "  'frobenius_norm_relative': 0.009723762029276797,\n",
       "  'spectral_norm': 0.27105724811553955,\n",
       "  'spectral_norm_relative': 0.009723762029276797,\n",
       "  'mean_abs_difference': 0.004009516444057226,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0013658577809110284,\n",
       "  'significant_diff_ratio': 0.994140625,\n",
       "  'cosine_similarity': 0.9999986290931702,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_1_q_proj': {'frobenius_norm': 4.074953556060791,\n",
       "  'frobenius_norm_relative': 0.0379502716177451,\n",
       "  'spectral_norm': 4.074953556060791,\n",
       "  'spectral_norm_relative': 0.0379502716177451,\n",
       "  'mean_abs_difference': 0.0007840266334824264,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0009960058378055692,\n",
       "  'significant_diff_ratio': 0.9630439281463623,\n",
       "  'cosine_similarity': 1.0009145736694336,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_19_k_proj': {'frobenius_norm': 5.321815013885498,\n",
       "  'frobenius_norm_relative': 0.05690165446426253,\n",
       "  'spectral_norm': 5.321815013885498,\n",
       "  'spectral_norm_relative': 0.05690165446426253,\n",
       "  'mean_abs_difference': 0.0010318574495613575,\n",
       "  'max_abs_difference': 0.00799560546875,\n",
       "  'std_difference': 0.0012998442398384213,\n",
       "  'significant_diff_ratio': 0.9697065949440002,\n",
       "  'cosine_similarity': 0.999660074710846,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_1_input_layernorm': {'frobenius_norm': 0.07843246310949326,\n",
       "  'frobenius_norm_relative': 0.013330697491077077,\n",
       "  'spectral_norm': 0.07843246310949326,\n",
       "  'spectral_norm_relative': 0.013330697491077077,\n",
       "  'mean_abs_difference': 0.0009403752046637237,\n",
       "  'max_abs_difference': 0.005859375,\n",
       "  'std_difference': 0.0010028055403381586,\n",
       "  'significant_diff_ratio': 0.88134765625,\n",
       "  'cosine_similarity': 0.9999561309814453,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_23_up_proj': {'frobenius_norm': 8.530685424804688,\n",
       "  'frobenius_norm_relative': 0.0693740222567811,\n",
       "  'spectral_norm': 8.530685424804688,\n",
       "  'spectral_norm_relative': 0.0693740222567811,\n",
       "  'mean_abs_difference': 0.0010129038710147142,\n",
       "  'max_abs_difference': 0.00732421875,\n",
       "  'std_difference': 0.0012717884965240955,\n",
       "  'significant_diff_ratio': 0.9741165041923523,\n",
       "  'cosine_similarity': 1.0035099983215332,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_24_k_proj': {'frobenius_norm': 5.324514389038086,\n",
       "  'frobenius_norm_relative': 0.059489671274193294,\n",
       "  'spectral_norm': 5.324514389038086,\n",
       "  'spectral_norm_relative': 0.059489671274193294,\n",
       "  'mean_abs_difference': 0.0010336291743442416,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0013004497159272432,\n",
       "  'significant_diff_ratio': 0.9712492227554321,\n",
       "  'cosine_similarity': 0.9993242025375366,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_27_down_proj': {'frobenius_norm': 8.59900188446045,\n",
       "  'frobenius_norm_relative': 0.06911742890809366,\n",
       "  'spectral_norm': 8.59900188446045,\n",
       "  'spectral_norm_relative': 0.06911742890809366,\n",
       "  'mean_abs_difference': 0.0010194431524723768,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0012820209376513958,\n",
       "  'significant_diff_ratio': 0.9739643335342407,\n",
       "  'cosine_similarity': 1.0034382343292236,\n",
       "  'weight_shape': torch.Size([4096, 11008]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_30_post_attention_layernorm': {'frobenius_norm': 0.3302463889122009,\n",
       "  'frobenius_norm_relative': 0.010899518116902545,\n",
       "  'spectral_norm': 0.3302463889122009,\n",
       "  'spectral_norm_relative': 0.010899518116902545,\n",
       "  'mean_abs_difference': 0.0049591064453125,\n",
       "  'max_abs_difference': 0.009765625,\n",
       "  'std_difference': 0.0014271489344537258,\n",
       "  'significant_diff_ratio': 0.999755859375,\n",
       "  'cosine_similarity': 0.9999991655349731,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_27_k_proj': {'frobenius_norm': 5.2194647789001465,\n",
       "  'frobenius_norm_relative': 0.056404255125249325,\n",
       "  'spectral_norm': 5.2194647789001465,\n",
       "  'spectral_norm_relative': 0.056404255125249325,\n",
       "  'mean_abs_difference': 0.00101311388425529,\n",
       "  'max_abs_difference': 0.0086212158203125,\n",
       "  'std_difference': 0.0012749333400279284,\n",
       "  'significant_diff_ratio': 0.9690226316452026,\n",
       "  'cosine_similarity': 0.9996358752250671,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_18_up_proj': {'frobenius_norm': 8.304411888122559,\n",
       "  'frobenius_norm_relative': 0.06767455229969958,\n",
       "  'spectral_norm': 8.304411888122559,\n",
       "  'spectral_norm_relative': 0.06767455229969958,\n",
       "  'mean_abs_difference': 0.000985409365966916,\n",
       "  'max_abs_difference': 0.008056640625,\n",
       "  'std_difference': 0.001238072058185935,\n",
       "  'significant_diff_ratio': 0.9734672904014587,\n",
       "  'cosine_similarity': 1.0036559104919434,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_8_input_layernorm': {'frobenius_norm': 0.22120004892349243,\n",
       "  'frobenius_norm_relative': 0.010286498154862364,\n",
       "  'spectral_norm': 0.22120004892349243,\n",
       "  'spectral_norm_relative': 0.010286498154862364,\n",
       "  'mean_abs_difference': 0.003193755866959691,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0013236732920631766,\n",
       "  'significant_diff_ratio': 0.96826171875,\n",
       "  'cosine_similarity': 0.9999924898147583,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_3_post_attention_layernorm': {'frobenius_norm': 0.12203644216060638,\n",
       "  'frobenius_norm_relative': 0.011173604217130586,\n",
       "  'spectral_norm': 0.12203644216060638,\n",
       "  'spectral_norm_relative': 0.011173604217130586,\n",
       "  'mean_abs_difference': 0.0016355335246771574,\n",
       "  'max_abs_difference': 0.0048828125,\n",
       "  'std_difference': 0.0010282741859555244,\n",
       "  'significant_diff_ratio': 0.88671875,\n",
       "  'cosine_similarity': 0.9999821782112122,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_6_down_proj': {'frobenius_norm': 7.707906723022461,\n",
       "  'frobenius_norm_relative': 0.06695023417548984,\n",
       "  'spectral_norm': 7.707906723022461,\n",
       "  'spectral_norm_relative': 0.06695023417548984,\n",
       "  'mean_abs_difference': 0.0009147930541075766,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0011489731259644032,\n",
       "  'significant_diff_ratio': 0.973260760307312,\n",
       "  'cosine_similarity': 1.0043410062789917,\n",
       "  'weight_shape': torch.Size([4096, 11008]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_17_gate_proj': {'frobenius_norm': 8.62624740600586,\n",
       "  'frobenius_norm_relative': 0.06701213895445002,\n",
       "  'spectral_norm': 8.62624740600586,\n",
       "  'spectral_norm_relative': 0.06701213895445002,\n",
       "  'mean_abs_difference': 0.0010239921975880861,\n",
       "  'max_abs_difference': 0.0076904296875,\n",
       "  'std_difference': 0.0012861188733950257,\n",
       "  'significant_diff_ratio': 0.973486602306366,\n",
       "  'cosine_similarity': 1.0036693811416626,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_19_post_attention_layernorm': {'frobenius_norm': 0.17723466455936432,\n",
       "  'frobenius_norm_relative': 0.008027783878626463,\n",
       "  'spectral_norm': 0.17723466455936432,\n",
       "  'spectral_norm_relative': 0.008027783878626463,\n",
       "  'mean_abs_difference': 0.0024023056030273438,\n",
       "  'max_abs_difference': 0.005859375,\n",
       "  'std_difference': 0.0014057089574635029,\n",
       "  'significant_diff_ratio': 0.86328125,\n",
       "  'cosine_similarity': 0.999991774559021,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_20_o_proj': {'frobenius_norm': 4.88060998916626,\n",
       "  'frobenius_norm_relative': 0.06944919333632711,\n",
       "  'spectral_norm': 4.88060998916626,\n",
       "  'spectral_norm_relative': 0.06944919333632711,\n",
       "  'mean_abs_difference': 0.0009455127292312682,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0011917821830138564,\n",
       "  'significant_diff_ratio': 0.9740130305290222,\n",
       "  'cosine_similarity': 0.9988136887550354,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_21_gate_proj': {'frobenius_norm': 8.752872467041016,\n",
       "  'frobenius_norm_relative': 0.0663544808930907,\n",
       "  'spectral_norm': 8.752872467041016,\n",
       "  'spectral_norm_relative': 0.0663544808930907,\n",
       "  'mean_abs_difference': 0.0010396541329100728,\n",
       "  'max_abs_difference': 0.0126953125,\n",
       "  'std_difference': 0.001304931123740971,\n",
       "  'significant_diff_ratio': 0.973150908946991,\n",
       "  'cosine_similarity': 1.0041214227676392,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_1_up_proj': {'frobenius_norm': 7.350497245788574,\n",
       "  'frobenius_norm_relative': 0.06453092850927723,\n",
       "  'spectral_norm': 7.350497245788574,\n",
       "  'spectral_norm_relative': 0.06453092850927723,\n",
       "  'mean_abs_difference': 0.0008728926768526435,\n",
       "  'max_abs_difference': 0.006195068359375,\n",
       "  'std_difference': 0.001095786690711975,\n",
       "  'significant_diff_ratio': 0.9723474383354187,\n",
       "  'cosine_similarity': 1.0045617818832397,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_7_gate_proj': {'frobenius_norm': 8.042999267578125,\n",
       "  'frobenius_norm_relative': 0.06108231626230074,\n",
       "  'spectral_norm': 8.042999267578125,\n",
       "  'spectral_norm_relative': 0.06108231626230074,\n",
       "  'mean_abs_difference': 0.0009548623347654939,\n",
       "  'max_abs_difference': 0.0069580078125,\n",
       "  'std_difference': 0.001199236256070435,\n",
       "  'significant_diff_ratio': 0.9710086584091187,\n",
       "  'cosine_similarity': 1.0044723749160767,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_31_input_layernorm': {'frobenius_norm': 0.24491284787654877,\n",
       "  'frobenius_norm_relative': 0.008319830353624989,\n",
       "  'spectral_norm': 0.24491284787654877,\n",
       "  'spectral_norm_relative': 0.008319830353624989,\n",
       "  'mean_abs_difference': 0.0035457611083984375,\n",
       "  'max_abs_difference': 0.01171875,\n",
       "  'std_difference': 0.0014395138714462519,\n",
       "  'significant_diff_ratio': 0.9765625,\n",
       "  'cosine_similarity': 0.999998927116394,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_10_down_proj': {'frobenius_norm': 7.899662017822266,\n",
       "  'frobenius_norm_relative': 0.0667437159963591,\n",
       "  'spectral_norm': 7.899662017822266,\n",
       "  'spectral_norm_relative': 0.0667437159963591,\n",
       "  'mean_abs_difference': 0.0009368686005473137,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0011775732273235917,\n",
       "  'significant_diff_ratio': 0.9731699228286743,\n",
       "  'cosine_similarity': 1.0041122436523438,\n",
       "  'weight_shape': torch.Size([4096, 11008]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_30_gate_proj': {'frobenius_norm': 8.715604782104492,\n",
       "  'frobenius_norm_relative': 0.0638357060529505,\n",
       "  'spectral_norm': 8.715604782104492,\n",
       "  'spectral_norm_relative': 0.0638357060529505,\n",
       "  'mean_abs_difference': 0.0010338424472138286,\n",
       "  'max_abs_difference': 0.01025390625,\n",
       "  'std_difference': 0.001299481256864965,\n",
       "  'significant_diff_ratio': 0.9725122451782227,\n",
       "  'cosine_similarity': 1.0050028562545776,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_18_v_proj': {'frobenius_norm': 4.628059387207031,\n",
       "  'frobenius_norm_relative': 0.06700849402346584,\n",
       "  'spectral_norm': 4.628059387207031,\n",
       "  'spectral_norm_relative': 0.06700849402346584,\n",
       "  'mean_abs_difference': 0.0008982150466181338,\n",
       "  'max_abs_difference': 0.006591796875,\n",
       "  'std_difference': 0.0011303001083433628,\n",
       "  'significant_diff_ratio': 0.9734213948249817,\n",
       "  'cosine_similarity': 0.9989879131317139,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_26_o_proj': {'frobenius_norm': 5.141714572906494,\n",
       "  'frobenius_norm_relative': 0.064698671590432,\n",
       "  'spectral_norm': 5.141714572906494,\n",
       "  'spectral_norm_relative': 0.064698671590432,\n",
       "  'mean_abs_difference': 0.0009970241226255894,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0012556178262457252,\n",
       "  'significant_diff_ratio': 0.9725108742713928,\n",
       "  'cosine_similarity': 0.9991836547851562,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_13_k_proj': {'frobenius_norm': 5.359001636505127,\n",
       "  'frobenius_norm_relative': 0.05377419231726087,\n",
       "  'spectral_norm': 5.359001636505127,\n",
       "  'spectral_norm_relative': 0.05377419231726087,\n",
       "  'mean_abs_difference': 0.0010387334041297436,\n",
       "  'max_abs_difference': 0.008056640625,\n",
       "  'std_difference': 0.001308994134888053,\n",
       "  'significant_diff_ratio': 0.9684460163116455,\n",
       "  'cosine_similarity': 1.0001786947250366,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_31_up_proj': {'frobenius_norm': 8.08641529083252,\n",
       "  'frobenius_norm_relative': 0.06018220445178067,\n",
       "  'spectral_norm': 8.08641529083252,\n",
       "  'spectral_norm_relative': 0.06018220445178067,\n",
       "  'mean_abs_difference': 0.0009542134357616305,\n",
       "  'max_abs_difference': 0.014190673828125,\n",
       "  'std_difference': 0.0012058718129992485,\n",
       "  'significant_diff_ratio': 0.9701734185218811,\n",
       "  'cosine_similarity': 1.0048518180847168,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_13_q_proj': {'frobenius_norm': 5.308023452758789,\n",
       "  'frobenius_norm_relative': 0.05422569609304404,\n",
       "  'spectral_norm': 5.308023452758789,\n",
       "  'spectral_norm_relative': 0.05422569609304404,\n",
       "  'mean_abs_difference': 0.0010291552171111107,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0012965487549081445,\n",
       "  'significant_diff_ratio': 0.9683996438980103,\n",
       "  'cosine_similarity': 1.000087022781372,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_23_down_proj': {'frobenius_norm': 8.527939796447754,\n",
       "  'frobenius_norm_relative': 0.06970808704240139,\n",
       "  'spectral_norm': 8.527939796447754,\n",
       "  'spectral_norm_relative': 0.06970808704240139,\n",
       "  'mean_abs_difference': 0.0010130091104656458,\n",
       "  'max_abs_difference': 0.00978851318359375,\n",
       "  'std_difference': 0.001271359040401876,\n",
       "  'significant_diff_ratio': 0.9743133187294006,\n",
       "  'cosine_similarity': 1.0035464763641357,\n",
       "  'weight_shape': torch.Size([4096, 11008]),\n",
       "  'total_parameters': 45088768},\n",
       " 'lm_head': {'frobenius_norm': 23.344120025634766,\n",
       "  'frobenius_norm_relative': 0.12358925555308098,\n",
       "  'spectral_norm': 23.344120025634766,\n",
       "  'spectral_norm_relative': 0.12358925555308098,\n",
       "  'mean_abs_difference': 0.001527020474895835,\n",
       "  'max_abs_difference': 0.04156494140625,\n",
       "  'std_difference': 0.002061779610812664,\n",
       "  'significant_diff_ratio': 0.9831951856613159,\n",
       "  'cosine_similarity': 1.0257526636123657,\n",
       "  'weight_shape': torch.Size([32000, 4096]),\n",
       "  'total_parameters': 131072000},\n",
       " 'layer_30_input_layernorm': {'frobenius_norm': 0.3503587543964386,\n",
       "  'frobenius_norm_relative': 0.009797934352349177,\n",
       "  'spectral_norm': 0.3503587543964386,\n",
       "  'spectral_norm_relative': 0.009797934352349177,\n",
       "  'mean_abs_difference': 0.0050654723308980465,\n",
       "  'max_abs_difference': 0.01171875,\n",
       "  'std_difference': 0.0020763527136296034,\n",
       "  'significant_diff_ratio': 0.96484375,\n",
       "  'cosine_similarity': 0.9999931454658508,\n",
       "  'weight_shape': torch.Size([4096]),\n",
       "  'total_parameters': 4096},\n",
       " 'layer_10_q_proj': {'frobenius_norm': 5.228488445281982,\n",
       "  'frobenius_norm_relative': 0.05129184073090293,\n",
       "  'spectral_norm': 5.228488445281982,\n",
       "  'spectral_norm_relative': 0.05129184073090293,\n",
       "  'mean_abs_difference': 0.0010138285579159856,\n",
       "  'max_abs_difference': 0.0076904296875,\n",
       "  'std_difference': 0.0012773124035447836,\n",
       "  'significant_diff_ratio': 0.9667128324508667,\n",
       "  'cosine_similarity': 1.0000810623168945,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_0_up_proj': {'frobenius_norm': 7.323666095733643,\n",
       "  'frobenius_norm_relative': 0.06839279080594045,\n",
       "  'spectral_norm': 7.323666095733643,\n",
       "  'spectral_norm_relative': 0.06839279080594045,\n",
       "  'mean_abs_difference': 0.0008687714580446482,\n",
       "  'max_abs_difference': 0.00632476806640625,\n",
       "  'std_difference': 0.0010916744358837605,\n",
       "  'significant_diff_ratio': 0.9738690853118896,\n",
       "  'cosine_similarity': 1.0047249794006348,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_0_gate_proj': {'frobenius_norm': 7.291021823883057,\n",
       "  'frobenius_norm_relative': 0.06694593561687379,\n",
       "  'spectral_norm': 7.291021823883057,\n",
       "  'spectral_norm_relative': 0.06694593561687379,\n",
       "  'mean_abs_difference': 0.0008643656619824469,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.001086859731003642,\n",
       "  'significant_diff_ratio': 0.9733356833457947,\n",
       "  'cosine_similarity': 1.0047595500946045,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_4_up_proj': {'frobenius_norm': 7.59793758392334,\n",
       "  'frobenius_norm_relative': 0.06552167259082625,\n",
       "  'spectral_norm': 7.59793758392334,\n",
       "  'spectral_norm_relative': 0.06552167259082625,\n",
       "  'mean_abs_difference': 0.000902065890841186,\n",
       "  'max_abs_difference': 0.0064697265625,\n",
       "  'std_difference': 0.0011326324893161654,\n",
       "  'significant_diff_ratio': 0.9726582765579224,\n",
       "  'cosine_similarity': 1.0043582916259766,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_10_k_proj': {'frobenius_norm': 5.259161472320557,\n",
       "  'frobenius_norm_relative': 0.05006448356241329,\n",
       "  'spectral_norm': 5.259161472320557,\n",
       "  'spectral_norm_relative': 0.05006448356241329,\n",
       "  'mean_abs_difference': 0.001019523711875081,\n",
       "  'max_abs_difference': 0.0072021484375,\n",
       "  'std_difference': 0.001284815720282495,\n",
       "  'significant_diff_ratio': 0.9664433598518372,\n",
       "  'cosine_similarity': 1.000173807144165,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_29_k_proj': {'frobenius_norm': 5.408339023590088,\n",
       "  'frobenius_norm_relative': 0.061780537637739774,\n",
       "  'spectral_norm': 5.408339023590088,\n",
       "  'spectral_norm_relative': 0.061780537637739774,\n",
       "  'mean_abs_difference': 0.0010503368685021996,\n",
       "  'max_abs_difference': 0.00954437255859375,\n",
       "  'std_difference': 0.001320775132626295,\n",
       "  'significant_diff_ratio': 0.9719750285148621,\n",
       "  'cosine_similarity': 0.9993736147880554,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_25_o_proj': {'frobenius_norm': 5.151584625244141,\n",
       "  'frobenius_norm_relative': 0.06635441510434069,\n",
       "  'spectral_norm': 5.151584625244141,\n",
       "  'spectral_norm_relative': 0.06635441510434069,\n",
       "  'mean_abs_difference': 0.0010011799167841673,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.0012579842004925013,\n",
       "  'significant_diff_ratio': 0.973063051700592,\n",
       "  'cosine_similarity': 0.9990505576133728,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_26_v_proj': {'frobenius_norm': 4.972836494445801,\n",
       "  'frobenius_norm_relative': 0.06215874893946559,\n",
       "  'spectral_norm': 4.972836494445801,\n",
       "  'spectral_norm_relative': 0.06215874893946559,\n",
       "  'mean_abs_difference': 0.0009658053750172257,\n",
       "  'max_abs_difference': 0.0068359375,\n",
       "  'std_difference': 0.0012144986540079117,\n",
       "  'significant_diff_ratio': 0.9713771343231201,\n",
       "  'cosine_similarity': 0.9994522333145142,\n",
       "  'weight_shape': torch.Size([4096, 4096]),\n",
       "  'total_parameters': 16777216},\n",
       " 'layer_24_gate_proj': {'frobenius_norm': 8.827963829040527,\n",
       "  'frobenius_norm_relative': 0.06640276596391705,\n",
       "  'spectral_norm': 8.827963829040527,\n",
       "  'spectral_norm_relative': 0.06640276596391705,\n",
       "  'mean_abs_difference': 0.001048817066475749,\n",
       "  'max_abs_difference': 0.00927734375,\n",
       "  'std_difference': 0.0013160945381969213,\n",
       "  'significant_diff_ratio': 0.9731314182281494,\n",
       "  'cosine_similarity': 1.0041980743408203,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768},\n",
       " 'layer_25_gate_proj': {'frobenius_norm': 8.83199691772461,\n",
       "  'frobenius_norm_relative': 0.0663101967262198,\n",
       "  'spectral_norm': 8.83199691772461,\n",
       "  'spectral_norm_relative': 0.0663101967262198,\n",
       "  'mean_abs_difference': 0.0010491920402273536,\n",
       "  'max_abs_difference': 0.0078125,\n",
       "  'std_difference': 0.001316705485805869,\n",
       "  'significant_diff_ratio': 0.9731427431106567,\n",
       "  'cosine_similarity': 1.0042798519134521,\n",
       "  'weight_shape': torch.Size([11008, 4096]),\n",
       "  'total_parameters': 45088768}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b9908bfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0010020476765930653\n",
      "0.0009188769618049264\n",
      "0.001015764893963933\n",
      "0.001034602290019393\n",
      "0.0009067205828614533\n",
      "0.0007258172845467925\n",
      "0.0009430106147192419\n",
      "0.0010643609566614032\n",
      "0.004338681697845459\n",
      "0.0029208073392510414\n",
      "0.001860499382019043\n",
      "0.0010259905830025673\n",
      "0.0008593168458901346\n",
      "0.0009909870568662882\n",
      "0.0009660234209150076\n",
      "0.0008440351230092347\n",
      "0.0008306287927553058\n",
      "0.0008649220690131187\n",
      "0.0008440296514891088\n",
      "0.0009530227980576456\n",
      "0.0010461758356541395\n",
      "0.0008569533238187432\n",
      "0.0010055670281872153\n",
      "0.0008362624794244766\n",
      "0.0009050709777511656\n",
      "0.001877129077911377\n",
      "0.005169880576431751\n",
      "0.0010217539966106415\n",
      "0.0010321848094463348\n",
      "0.0009484292240813375\n",
      "0.0044422149658203125\n",
      "0.000944170169532299\n",
      "0.000772513507399708\n",
      "0.0010334529215469956\n",
      "0.0008942301501519978\n",
      "0.00043242849642410874\n",
      "0.000997869879938662\n",
      "0.0010138978250324726\n",
      "0.0010304109891876578\n",
      "0.00100141076836735\n",
      "0.0008899049134925008\n",
      "0.0010688756592571735\n",
      "0.0009769403841346502\n",
      "0.003312712535262108\n",
      "0.004403494764119387\n",
      "0.0009864402236416936\n",
      "0.0010072553995996714\n",
      "0.0008769890409894288\n",
      "0.0008706428925506771\n",
      "0.0008548776968382299\n",
      "0.0009796591475605965\n",
      "0.003332793712615967\n",
      "0.0009754984639585018\n",
      "0.0008401564555242658\n",
      "0.0010338198626413941\n",
      "0.0010171601315960288\n",
      "0.001046835328452289\n",
      "0.000892461568582803\n",
      "0.002934732474386692\n",
      "0.0009224803070537746\n",
      "0.0009674763423390687\n",
      "0.0018381178379058838\n",
      "0.0009561135084368289\n",
      "0.0010334814433008432\n",
      "0.000939988880418241\n",
      "0.0009741014800965786\n",
      "0.0009037334239110351\n",
      "0.004855513572692871\n",
      "0.0009127834346145391\n",
      "0.0009281010716222227\n",
      "0.0010109497234225273\n",
      "0.0009837074903771281\n",
      "0.0008826879784464836\n",
      "0.0009015626274049282\n",
      "0.0052474793046712875\n",
      "0.0009961165487766266\n",
      "0.0010201759869232774\n",
      "0.0009672019514255226\n",
      "0.0009967676596716046\n",
      "0.0010182401165366173\n",
      "0.0023543834686279297\n",
      "0.0008565316675230861\n",
      "0.0008861838141456246\n",
      "0.004629045724868774\n",
      "0.0009731253958307207\n",
      "0.0007808239897713065\n",
      "0.0008771020220592618\n",
      "0.0009908040519803762\n",
      "0.0010410810355097055\n",
      "0.0010051049757748842\n",
      "0.0009260291699320078\n",
      "0.0009058689465746284\n",
      "0.0024109738878905773\n",
      "0.004540622234344482\n",
      "0.0008263499476015568\n",
      "0.0008289013057947159\n",
      "0.0008379047503694892\n",
      "0.0008387219277210534\n",
      "0.003567485371604562\n",
      "0.0010470787528902292\n",
      "0.005087739787995815\n",
      "0.0010234675137326121\n",
      "0.0010458707110956311\n",
      "0.0010106184054166079\n",
      "0.0008473646594211459\n",
      "0.0009659070638008416\n",
      "0.0010171675821766257\n",
      "0.0009500262094661593\n",
      "0.002398568904027343\n",
      "0.001012250897474587\n",
      "0.004318595863878727\n",
      "0.0010895447339862585\n",
      "0.0009064741898328066\n",
      "0.0010366415372118354\n",
      "0.00102561479434371\n",
      "0.0009723561233840883\n",
      "0.001799464225769043\n",
      "0.0009192785946652293\n",
      "0.0012758595403283834\n",
      "0.0009646923863328993\n",
      "0.0009586571832187474\n",
      "0.000932212162297219\n",
      "0.0008664465858601034\n",
      "0.0008541274000890553\n",
      "0.0009452314116060734\n",
      "0.004119643941521645\n",
      "0.0009990442777052522\n",
      "0.00092760642291978\n",
      "0.0018409490585327148\n",
      "0.001734018325805664\n",
      "0.0008670289535075426\n",
      "0.004293352365493774\n",
      "0.0009477834100835025\n",
      "0.001902759075164795\n",
      "0.004798755049705505\n",
      "0.0008355370373465121\n",
      "0.0008480151882395148\n",
      "0.0004406938096508384\n",
      "0.0008733381982892752\n",
      "0.0010255755623802543\n",
      "0.0009610253619030118\n",
      "0.0010375857818871737\n",
      "0.0016632080078125\n",
      "0.0010234846267849207\n",
      "0.0010271577630192041\n",
      "0.003056943416595459\n",
      "0.0010173505870625377\n",
      "0.0010526154655963182\n",
      "0.000979797332547605\n",
      "0.0010173255577683449\n",
      "0.0010362555040046573\n",
      "0.0009204488596878946\n",
      "0.0010241870768368244\n",
      "0.0009691443992778659\n",
      "0.0010505927493795753\n",
      "0.0008423661347478628\n",
      "0.0020401477813720703\n",
      "0.0009478670544922352\n",
      "0.0009458037093281746\n",
      "0.0009726113057695329\n",
      "0.0008556984830647707\n",
      "0.0010325564071536064\n",
      "0.0009964585769921541\n",
      "0.0010297062108293176\n",
      "0.0009563038474880159\n",
      "0.0008682101033627987\n",
      "0.0016783475875854492\n",
      "0.0008998644771054387\n",
      "0.0009391416097059846\n",
      "0.0008852165192365646\n",
      "0.0010079869534820318\n",
      "0.000941154663451016\n",
      "0.001042781863361597\n",
      "0.004400855395942926\n",
      "0.001020154682919383\n",
      "0.0016280289273709059\n",
      "0.0009474256657995284\n",
      "0.001046657213009894\n",
      "0.0009307627333328128\n",
      "0.0018096566200256348\n",
      "0.0009936142014339566\n",
      "0.0009529832168482244\n",
      "0.003977775573730469\n",
      "0.0030593944247812033\n",
      "0.0010451240232214332\n",
      "0.001038905931636691\n",
      "0.0016049742698669434\n",
      "0.0009861363796517253\n",
      "0.0010316901607438922\n",
      "0.0009854938834905624\n",
      "0.0010338914580643177\n",
      "0.0009708308498375118\n",
      "0.0010352940298616886\n",
      "0.000740374147426337\n",
      "0.001008506747893989\n",
      "0.0010088107082992792\n",
      "0.0009966341312974691\n",
      "0.0008501394768245518\n",
      "0.0009676441550254822\n",
      "0.0009856141405180097\n",
      "0.0030868053436279297\n",
      "0.0008702456834726036\n",
      "0.004114169627428055\n",
      "0.004353410564363003\n",
      "0.00105039041955024\n",
      "0.0009143789066001773\n",
      "0.0009299881057813764\n",
      "0.0024721622467041016\n",
      "0.005391884595155716\n",
      "0.0007397193694487214\n",
      "0.010121654719114304\n",
      "0.005320478230714798\n",
      "0.0009376067901030183\n",
      "0.0010285121388733387\n",
      "0.0010230974294245243\n",
      "0.0010019522160291672\n",
      "0.0008384695975109935\n",
      "0.0008328189142048359\n",
      "0.0008642047178000212\n",
      "0.0009865862084552646\n",
      "0.0045935227535665035\n",
      "0.0010249504121020436\n",
      "0.003692150115966797\n",
      "0.0008602877496741712\n",
      "0.0009431222570128739\n",
      "0.0009831442730501294\n",
      "0.0008695260039530694\n",
      "0.0008578454726375639\n",
      "0.000920667196623981\n",
      "0.0008508003666065633\n",
      "0.0010347208008170128\n",
      "0.0010521032381802797\n",
      "0.0009594949660822749\n",
      "0.0008623851463198662\n",
      "0.0028361082077026367\n",
      "0.004676243755966425\n",
      "0.0008579519344493747\n",
      "0.0010299839777871966\n",
      "0.0009500551968812943\n",
      "0.0010160889942198992\n",
      "0.0008548144833184779\n",
      "0.00447250297293067\n",
      "0.0009033440728671849\n",
      "0.004198194481432438\n",
      "0.0019086599349975586\n",
      "0.0008836511406116188\n",
      "0.0027413368225097656\n",
      "0.001020663185045123\n",
      "0.0007826546207070351\n",
      "0.0009911861270666122\n",
      "0.0010051733115687966\n",
      "0.004009516444057226\n",
      "0.0007840266334824264\n",
      "0.0010318574495613575\n",
      "0.0009403752046637237\n",
      "0.0010129038710147142\n",
      "0.0010336291743442416\n",
      "0.0010194431524723768\n",
      "0.0049591064453125\n",
      "0.00101311388425529\n",
      "0.000985409365966916\n",
      "0.003193755866959691\n",
      "0.0016355335246771574\n",
      "0.0009147930541075766\n",
      "0.0010239921975880861\n",
      "0.0024023056030273438\n",
      "0.0009455127292312682\n",
      "0.0010396541329100728\n",
      "0.0008728926768526435\n",
      "0.0009548623347654939\n",
      "0.0035457611083984375\n",
      "0.0009368686005473137\n",
      "0.0010338424472138286\n",
      "0.0008982150466181338\n",
      "0.0009970241226255894\n",
      "0.0010387334041297436\n",
      "0.0009542134357616305\n",
      "0.0010291552171111107\n",
      "0.0010130091104656458\n",
      "0.001527020474895835\n",
      "0.0050654723308980465\n",
      "0.0010138285579159856\n",
      "0.0008687714580446482\n",
      "0.0008643656619824469\n",
      "0.000902065890841186\n",
      "0.001019523711875081\n",
      "0.0010503368685021996\n",
      "0.0010011799167841673\n",
      "0.0009658053750172257\n",
      "0.001048817066475749\n",
      "0.0010491920402273536\n"
     ]
    }
   ],
   "source": [
    "for i in weight_differences:\n",
    "    print(weight_differences[i]['mean_abs_difference'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2a3bf55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "activations_model_1 = {}\n",
    "activations_model_2 = {}\n",
    "current_hooks = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "854f9fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_activations():\n",
    "    global activations_model_1, activations_model_2\n",
    "    activations_model_1.clear()\n",
    "    activations_model_2.clear()\n",
    "\n",
    "def remove_all_hooks():\n",
    "    global current_hooks\n",
    "    for hook in current_hooks:\n",
    "        hook.remove()\n",
    "    current_hooks.clear()\n",
    "\n",
    "def get_activation_hook(name, model_name):\n",
    "    def hook(module, input, output):\n",
    "        try:\n",
    "            # Handle output\n",
    "            if isinstance(output, tuple):\n",
    "                activation = output[0] if output[0] is not None else None\n",
    "            else:\n",
    "                activation = output\n",
    "            \n",
    "            # Handle input - check for None values\n",
    "            input_tensor = None\n",
    "            if input is not None:\n",
    "                if isinstance(input, tuple) and len(input) > 0:\n",
    "                    input_tensor = input[0] if input[0] is not None else None\n",
    "                else:\n",
    "                    input_tensor = input if input is not None else None\n",
    "            \n",
    "            # Create activation data with None checks\n",
    "            activation_data = {\n",
    "                'output': activation.detach().cpu() if activation is not None else None,\n",
    "                'input': input_tensor.detach().cpu() if input_tensor is not None else None,\n",
    "                'weight': module.weight.detach().cpu() if hasattr(module, 'weight') and module.weight is not None else None,\n",
    "                'bias': module.bias.detach().cpu() if hasattr(module, 'bias') and module.bias is not None else None\n",
    "            }\n",
    "            \n",
    "            if model_name == \"Model_1\":\n",
    "                activations_model_1[name] = activation_data\n",
    "            else:\n",
    "                activations_model_2[name] = activation_data\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error in hook {name}: {e}\")\n",
    "            # Store None data to prevent missing keys\n",
    "            activation_data = {\n",
    "                'output': None,\n",
    "                'input': None, \n",
    "                'weight': None,\n",
    "                'bias': None\n",
    "            }\n",
    "            \n",
    "            if model_name == \"Model_1\":\n",
    "                activations_model_1[name] = activation_data\n",
    "            else:\n",
    "                activations_model_2[name] = activation_data\n",
    "            \n",
    "    return hook\n",
    "\n",
    "def register_llama_hooks(model, model_name, layer_range=None):\n",
    "    global current_hooks\n",
    "    hooks = []\n",
    "    layer_info = {}\n",
    "    \n",
    "    # Determine layer range\n",
    "    if layer_range is None:\n",
    "        layer_range = range(len(model.model.layers))\n",
    "    \n",
    "    for i in layer_range:\n",
    "        if i >= len(model.model.layers):\n",
    "            continue\n",
    "            \n",
    "        layer = model.model.layers[i]\n",
    "        layer_prefix = f\"layer_{i}\"\n",
    "        \n",
    "        # 1. Self-Attention Components\n",
    "        # Query, Key, Value projections\n",
    "        hooks.append(layer.self_attn.q_proj.register_forward_hook(\n",
    "            get_activation_hook(f\"{layer_prefix}_attention_q\", model_name)\n",
    "        ))\n",
    "        hooks.append(layer.self_attn.k_proj.register_forward_hook(\n",
    "            get_activation_hook(f\"{layer_prefix}_attention_k\", model_name)\n",
    "        ))\n",
    "        hooks.append(layer.self_attn.v_proj.register_forward_hook(\n",
    "            get_activation_hook(f\"{layer_prefix}_attention_v\", model_name)\n",
    "        ))\n",
    "        \n",
    "        # Output projection\n",
    "        hooks.append(layer.self_attn.o_proj.register_forward_hook(\n",
    "            get_activation_hook(f\"{layer_prefix}_attention_output\", model_name)\n",
    "        ))\n",
    "        \n",
    "        # 2. MLP Components  \n",
    "        hooks.append(layer.mlp.gate_proj.register_forward_hook(\n",
    "            get_activation_hook(f\"{layer_prefix}_mlp_gate\", model_name)\n",
    "        ))\n",
    "        hooks.append(layer.mlp.up_proj.register_forward_hook(\n",
    "            get_activation_hook(f\"{layer_prefix}_mlp_up\", model_name)\n",
    "        ))\n",
    "        hooks.append(layer.mlp.down_proj.register_forward_hook(\n",
    "            get_activation_hook(f\"{layer_prefix}_mlp_down\", model_name)\n",
    "        ))\n",
    "        \n",
    "        # 3. Layer Norms\n",
    "        hooks.append(layer.input_layernorm.register_forward_hook(\n",
    "            get_activation_hook(f\"{layer_prefix}_input_norm\", model_name)\n",
    "        ))\n",
    "        hooks.append(layer.post_attention_layernorm.register_forward_hook(\n",
    "            get_activation_hook(f\"{layer_prefix}_post_attn_norm\", model_name)\n",
    "        ))\n",
    "        \n",
    "        # Store layer info\n",
    "        layer_info[layer_prefix] = {\n",
    "            'layer_idx': i,\n",
    "            'components': ['attention_q', 'attention_k', 'attention_v', \n",
    "                         'attention_output', 'mlp_gate', 'mlp_up', 'mlp_down',\n",
    "                         'input_norm', 'post_attn_norm']\n",
    "        }\n",
    "    \n",
    "    # Final layer norm and LM head (optional)\n",
    "    hooks.append(model.model.norm.register_forward_hook(\n",
    "        get_activation_hook(\"final_norm\", model_name)\n",
    "    ))\n",
    "    hooks.append(model.lm_head.register_forward_hook(\n",
    "        get_activation_hook(\"lm_head\", model_name)\n",
    "    ))\n",
    "    \n",
    "    current_hooks.extend(hooks)\n",
    "    return hooks, layer_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bdf4d8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_random_neurons_per_layer(activations, seed=42):\n",
    "    np.random.seed(seed)\n",
    "    selected_neurons = {}\n",
    "    \n",
    "    for layer_name, layer_data in activations.items():\n",
    "        if not isinstance(layer_data, dict):\n",
    "            continue\n",
    "            \n",
    "        activation = layer_data.get('output')\n",
    "        \n",
    "        if activation is None:\n",
    "            print(f\"Skipping {layer_name}: No activation data\")\n",
    "            continue\n",
    "        \n",
    "        # Handle different activation shapes\n",
    "        if len(activation.shape) == 3:  # [batch, seq_len, hidden_size]\n",
    "            batch_size, seq_len, hidden_size = activation.shape\n",
    "            \n",
    "            if hidden_size == 0:\n",
    "                continue\n",
    "            \n",
    "            # Select one random neuron index for this layer\n",
    "            neuron_idx = np.random.randint(0, hidden_size)\n",
    "            \n",
    "            selected_neurons[layer_name] = {\n",
    "                'neuron_index': neuron_idx,\n",
    "                'sequence_length': seq_len,\n",
    "                'hidden_size': hidden_size,\n",
    "                'activation_shape': activation.shape\n",
    "            }\n",
    "            \n",
    "    return selected_neurons\n",
    "    \n",
    "def get_component_type(layer_name):\n",
    "    if 'attention' in layer_name:\n",
    "        return 'attention'\n",
    "    elif 'mlp' in layer_name:\n",
    "        return 'mlp'\n",
    "    elif 'norm' in layer_name:\n",
    "        return 'normalization'\n",
    "    elif 'lm_head' in layer_name:\n",
    "        return 'output'\n",
    "    else:\n",
    "        return 'other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6c9d201c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_neuron_outputs(layer_name, neuron_idx, input_tensor,\n",
    "                            weights_1, weights_2, \n",
    "                            actual_output_1, actual_output_2):\n",
    "    results = {\n",
    "        'neuron_index': neuron_idx,\n",
    "        'calculations': [],\n",
    "        'layer_type': get_component_type(layer_name)\n",
    "    }\n",
    "    \n",
    "    # Skip if essential data is missing\n",
    "    if input_tensor is None or weights_1 is None or weights_2 is None:\n",
    "        return results\n",
    "    \n",
    "    # Get weights and biases\n",
    "    w1 = weights_1.get('weight')\n",
    "    w2 = weights_2.get('weight')\n",
    "    b1 = weights_1.get('bias')\n",
    "    b2 = weights_2.get('bias')\n",
    "    \n",
    "    if w1 is None or w2 is None:\n",
    "        return results\n",
    "    \n",
    "    # Calculate for each token position\n",
    "    for token_idx in range(input_tensor.shape[1]):\n",
    "        try:\n",
    "            token_input = input_tensor[0, token_idx, :]  # [hidden_size]\n",
    "            \n",
    "            # Check bounds\n",
    "            if neuron_idx >= w1.shape[0] or neuron_idx >= w2.shape[0]:\n",
    "                continue\n",
    "            \n",
    "            # Model 1 calculation: input @ w1.T + b1\n",
    "            calc_1 = torch.matmul(token_input, w1[neuron_idx, :])\n",
    "            if b1 is not None and neuron_idx < b1.shape[0]:\n",
    "                calc_1 += b1[neuron_idx]\n",
    "            \n",
    "            # Model 2 calculation: input @ w2.T + b2  \n",
    "            calc_2 = torch.matmul(token_input, w2[neuron_idx, :])\n",
    "            if b2 is not None and neuron_idx < b2.shape[0]:\n",
    "                calc_2 += b2[neuron_idx]\n",
    "            \n",
    "            # Apply activation function for MLP gate/up projections\n",
    "            if 'mlp_gate' in layer_name or 'mlp_up' in layer_name:\n",
    "                calc_1 = F.silu(calc_1)\n",
    "                calc_2 = F.silu(calc_2)\n",
    "            \n",
    "            # Get actual outputs\n",
    "            actual_1 = actual_output_1[0, token_idx, neuron_idx] if actual_output_1 is not None else None\n",
    "            actual_2 = actual_output_2[0, token_idx, neuron_idx] if actual_output_2 is not None else None\n",
    "            \n",
    "            results['calculations'].append({\n",
    "                'token_position': token_idx,\n",
    "                'model_1_calculated': calc_1.item(),\n",
    "                'model_2_calculated': calc_2.item(),\n",
    "                'difference': (calc_1 - calc_2).item(),\n",
    "                'model_1_actual': actual_1.item() if actual_1 is not None else None,\n",
    "                'model_2_actual': actual_2.item() if actual_2 is not None else None\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating for token {token_idx} in {layer_name}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return results\n",
    "\n",
    "def calculate_neuron_outputs(layer_name, neuron_idx, input_tensor,\n",
    "                            weights_1, weights_2, \n",
    "                            actual_output_1, actual_output_2):\n",
    "    results = {\n",
    "        'neuron_index': neuron_idx,\n",
    "        'calculations': [],\n",
    "        'layer_type': get_component_type(layer_name)\n",
    "    }\n",
    "    \n",
    "    # Skip if essential data is missing\n",
    "    if input_tensor is None or weights_1 is None or weights_2 is None:\n",
    "        return results\n",
    "    \n",
    "    # Get weights and biases\n",
    "    w1 = weights_1.get('weight')\n",
    "    w2 = weights_2.get('weight')\n",
    "    b1 = weights_1.get('bias')\n",
    "    b2 = weights_2.get('bias')\n",
    "    \n",
    "    if w1 is None or w2 is None:\n",
    "        return results\n",
    "    \n",
    "    # Handle layer normalization differently (1D weights)\n",
    "    if 'norm' in layer_name:\n",
    "        # Layer norm: output = weight * normalized_input + bias\n",
    "        # For layer norm, we can't select individual neurons the same way\n",
    "        # Instead, we'll look at the scaling factor for the selected dimension\n",
    "        for token_idx in range(input_tensor.shape[1]):\n",
    "            try:\n",
    "                token_input = input_tensor[0, token_idx, :]  # [hidden_size]\n",
    "                \n",
    "                # For layer norm, weight is 1D, so we use it as element-wise multiplication\n",
    "                if neuron_idx < w1.shape[0] and neuron_idx < w2.shape[0]:\n",
    "                    # Get the scaling factor for this dimension\n",
    "                    scale_1 = w1[neuron_idx].item()\n",
    "                    scale_2 = w2[neuron_idx].item()\n",
    "                    \n",
    "                    # Get the normalized input value for this dimension\n",
    "                    input_val = token_input[neuron_idx].item()\n",
    "                    \n",
    "                    # Calculate scaled outputs\n",
    "                    calc_1 = scale_1 * input_val\n",
    "                    calc_2 = scale_2 * input_val\n",
    "                    \n",
    "                    if b1 is not None and neuron_idx < b1.shape[0]:\n",
    "                        calc_1 += b1[neuron_idx].item()\n",
    "                    if b2 is not None and neuron_idx < b2.shape[0]:\n",
    "                        calc_2 += b2[neuron_idx].item()\n",
    "                    \n",
    "                    # Get actual outputs\n",
    "                    actual_1 = actual_output_1[0, token_idx, neuron_idx] if actual_output_1 is not None else None\n",
    "                    actual_2 = actual_output_2[0, token_idx, neuron_idx] if actual_output_2 is not None else None\n",
    "                    \n",
    "                    results['calculations'].append({\n",
    "                        'token_position': token_idx,\n",
    "                        'model_1_calculated': calc_1,\n",
    "                        'model_2_calculated': calc_2,\n",
    "                        'difference': calc_1 - calc_2,\n",
    "                        'model_1_actual': actual_1.item() if actual_1 is not None else None,\n",
    "                        'model_2_actual': actual_2.item() if actual_2 is not None else None,\n",
    "                        'weight_diff': scale_1 - scale_2\n",
    "                    })\n",
    "                    \n",
    "            except Exception as e:\n",
    "                continue\n",
    "    \n",
    "    else:\n",
    "        # Handle regular linear layers (2D weights)\n",
    "        for token_idx in range(input_tensor.shape[1]):\n",
    "            try:\n",
    "                token_input = input_tensor[0, token_idx, :]  # [hidden_size]\n",
    "                \n",
    "                # Check bounds\n",
    "                if neuron_idx >= w1.shape[0] or neuron_idx >= w2.shape[0]:\n",
    "                    continue\n",
    "                \n",
    "                # Model 1 calculation: input @ w1.T + b1\n",
    "                calc_1 = torch.matmul(token_input, w1[neuron_idx, :])\n",
    "                if b1 is not None and neuron_idx < b1.shape[0]:\n",
    "                    calc_1 += b1[neuron_idx]\n",
    "                \n",
    "                # Model 2 calculation: input @ w2.T + b2  \n",
    "                calc_2 = torch.matmul(token_input, w2[neuron_idx, :])\n",
    "                if b2 is not None and neuron_idx < b2.shape[0]:\n",
    "                    calc_2 += b2[neuron_idx]\n",
    "                \n",
    "                # Apply activation function for MLP gate/up projections\n",
    "                if 'mlp_gate' in layer_name or 'mlp_up' in layer_name:\n",
    "                    calc_1 = F.silu(calc_1)\n",
    "                    calc_2 = F.silu(calc_2)\n",
    "                \n",
    "                # Get actual outputs\n",
    "                actual_1 = actual_output_1[0, token_idx, neuron_idx] if actual_output_1 is not None else None\n",
    "                actual_2 = actual_output_2[0, token_idx, neuron_idx] if actual_output_2 is not None else None\n",
    "                \n",
    "                results['calculations'].append({\n",
    "                    'token_position': token_idx,\n",
    "                    'model_1_calculated': calc_1.item(),\n",
    "                    'model_2_calculated': calc_2.item(),\n",
    "                    'difference': (calc_1 - calc_2).item(),\n",
    "                    'model_1_actual': actual_1.item() if actual_1 is not None else None,\n",
    "                    'model_2_actual': actual_2.item() if actual_2 is not None else None\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                continue\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "75e23253",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_neuron_comparisons(comparison_results, save_path='neuron_comparison_all_layers.png'):\n",
    "    # Filter out empty results\n",
    "    filtered_results = {k: v for k, v in comparison_results.items() \n",
    "                       if v.get('calculations', [])}\n",
    "    \n",
    "    num_layers = len(filtered_results)\n",
    "    if num_layers == 0:\n",
    "        print(\"No comparison results to visualize\")\n",
    "        return\n",
    "    \n",
    "    # Limit the number of subplots to avoid too large images\n",
    "    max_plots = 300\n",
    "    if num_layers > max_plots:\n",
    "        print(f\"Too many layers ({num_layers}). Showing first {max_plots} layers.\")\n",
    "        layer_names = list(filtered_results.keys())[:max_plots]\n",
    "        filtered_results = {k: filtered_results[k] for k in layer_names}\n",
    "        num_layers = max_plots\n",
    "    \n",
    "    # Create subplots grid\n",
    "    cols = 5\n",
    "    rows = (num_layers + cols - 1) // cols\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(20, 4*rows))\n",
    "    \n",
    "    # Flatten axes for easier iteration\n",
    "    if rows == 1 and cols == 1:\n",
    "        axes = np.array([[axes]])\n",
    "    elif rows == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    elif cols == 1:\n",
    "        axes = axes.reshape(-1, 1)\n",
    "    axes_flat = axes.flatten()\n",
    "    \n",
    "    # Hide extra subplots\n",
    "    for idx in range(num_layers, len(axes_flat)):\n",
    "        axes_flat[idx].set_visible(False)\n",
    "    \n",
    "    # Plot each layer\n",
    "    for idx, (layer_name, results) in enumerate(filtered_results.items()):\n",
    "        ax = axes_flat[idx]\n",
    "        \n",
    "        if 'calculations' in results and results['calculations']:\n",
    "            # Extract data for plotting\n",
    "            token_positions = [calc['token_position'] for calc in results['calculations']]\n",
    "            model_1_values = [calc['model_1_calculated'] for calc in results['calculations']]\n",
    "            model_2_values = [calc['model_2_calculated'] for calc in results['calculations']]\n",
    "            differences = [calc['difference'] for calc in results['calculations']]\n",
    "            \n",
    "            # Plot the calculated values\n",
    "            ax.plot(token_positions, model_1_values, 'b-', label='M1 weights', linewidth=2)\n",
    "            ax.plot(token_positions, model_2_values, 'r-', label='M2 weights', linewidth=2)\n",
    "            ax.plot(token_positions, differences, 'g--', label='Difference', linewidth=1)\n",
    "            \n",
    "            # Formatting\n",
    "            layer_short = layer_name.replace('layer_', 'L').replace('_', ' ')\n",
    "            ax.set_title(f'{layer_short} - N{results[\"neuron_index\"]}', fontsize=10)\n",
    "            ax.set_xlabel('Token', fontsize=8)\n",
    "            ax.set_ylabel('Value', fontsize=8)\n",
    "            ax.tick_params(axis='both', labelsize=6)\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Add legend only to first subplot\n",
    "            if idx == 0:\n",
    "                ax.legend(fontsize=8)\n",
    "    \n",
    "    plt.suptitle('Neuron Activation Comparison Across Layers', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=150, bbox_inches='tight')  # Reduced DPI\n",
    "    plt.close()\n",
    "    print(f\"Visualization saved to {save_path}\")\n",
    "\n",
    "def compare_neuron_calculations(model_1_activations, model_2_activations, \n",
    "                                selected_neurons):\n",
    "    comparison_results = {}\n",
    "    \n",
    "    for layer_name, neuron_info in selected_neurons.items():\n",
    "        neuron_idx = neuron_info['neuron_index']\n",
    "        \n",
    "        # Get current layer data\n",
    "        layer_1_data = model_1_activations.get(layer_name, {})\n",
    "        layer_2_data = model_2_activations.get(layer_name, {})\n",
    "        \n",
    "        # Skip if missing data\n",
    "        if not isinstance(layer_1_data, dict) or not isinstance(layer_2_data, dict):\n",
    "            continue\n",
    "            \n",
    "        # Get the input to this layer (from Model 1)\n",
    "        model_1_input = layer_1_data.get('input')\n",
    "        \n",
    "        if model_1_input is None:\n",
    "            continue\n",
    "        \n",
    "        # Get weights from both models\n",
    "        weights_1 = layer_1_data  # Contains weight and bias\n",
    "        weights_2 = layer_2_data  # Contains weight and bias\n",
    "        \n",
    "        # Calculate outputs for the selected neuron\n",
    "        results = calculate_neuron_outputs(\n",
    "            layer_name, neuron_idx, model_1_input,\n",
    "            weights_1, weights_2,\n",
    "            layer_1_data.get('output'), layer_2_data.get('output')\n",
    "        )\n",
    "        \n",
    "        if results and results['calculations']:\n",
    "            comparison_results[layer_name] = results\n",
    "            \n",
    "    return comparison_results\n",
    "\n",
    "def create_summary_visualization(all_comparison_results, save_path='activation_summary.png'):\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    fig.suptitle('Activation Difference Analysis Summary', fontsize=16)\n",
    "    \n",
    "    # Collect all differences by layer and component type\n",
    "    layer_differences = defaultdict(list)\n",
    "    component_differences = defaultdict(list)\n",
    "    token_differences = defaultdict(list)\n",
    "    \n",
    "    for result in all_comparison_results:\n",
    "        for layer_name, layer_data in result['layer_comparisons'].items():\n",
    "            if 'calculations' in layer_data:\n",
    "                for calc in layer_data['calculations']:\n",
    "                    diff = abs(calc['difference'])\n",
    "                    layer_differences[layer_name].append(diff)\n",
    "                    component_differences[layer_data['layer_type']].append(diff)\n",
    "                    token_differences[calc['token_position']].append(diff)\n",
    "    \n",
    "    # 1. Mean differences by layer\n",
    "    layer_names = sorted(layer_differences.keys(), \n",
    "                        key=lambda x: int(x.split('_')[1]) if 'layer_' in x else 999)\n",
    "    mean_diffs = [np.mean(layer_differences[layer]) for layer in layer_names]\n",
    "    \n",
    "    ax = axes[0, 0]\n",
    "    ax.bar(range(len(layer_names)), mean_diffs)\n",
    "    ax.set_title('Mean Activation Differences by Layer')\n",
    "    ax.set_xlabel('Layer')\n",
    "    ax.set_ylabel('Mean Absolute Difference')\n",
    "    ax.set_xticks(range(0, len(layer_names), 5))\n",
    "    ax.set_xticklabels([layer_names[i].replace('layer_', 'L') \n",
    "                       for i in range(0, len(layer_names), 5)], rotation=45)\n",
    "    \n",
    "    # 2. Differences by component type\n",
    "    ax = axes[0, 1]\n",
    "    component_data = []\n",
    "    component_labels = []\n",
    "    for comp_type, diffs in component_differences.items():\n",
    "        component_data.append(diffs)\n",
    "        component_labels.append(comp_type)\n",
    "    \n",
    "    ax.boxplot(component_data, labels=component_labels)\n",
    "    ax.set_title('Differences by Component Type')\n",
    "    ax.set_ylabel('Absolute Difference')\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 3. Differences by token position\n",
    "    ax = axes[0, 2]\n",
    "    token_positions = sorted(token_differences.keys())\n",
    "    token_means = [np.mean(token_differences[pos]) for pos in token_positions]\n",
    "    \n",
    "    ax.plot(token_positions, token_means, marker='o')\n",
    "    ax.set_title('Mean Differences by Token Position')\n",
    "    ax.set_xlabel('Token Position')\n",
    "    ax.set_ylabel('Mean Absolute Difference')\n",
    "    \n",
    "    # 4. Layer depth analysis\n",
    "    ax = axes[1, 0]\n",
    "    layer_depths = []\n",
    "    depth_means = []\n",
    "    \n",
    "    for layer_name in layer_names:\n",
    "        if 'layer_' in layer_name:\n",
    "            try:\n",
    "                depth = int(layer_name.split('_')[1])\n",
    "                layer_depths.append(depth)\n",
    "                depth_means.append(np.mean(layer_differences[layer_name]))\n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "    if layer_depths:\n",
    "        ax.scatter(layer_depths, depth_means, alpha=0.6)\n",
    "        z = np.polyfit(layer_depths, depth_means, 1)\n",
    "        p = np.poly1d(z)\n",
    "        ax.plot(layer_depths, p(layer_depths), \"r--\", alpha=0.8)\n",
    "        ax.set_title('Differences vs Layer Depth')\n",
    "        ax.set_xlabel('Layer Number')\n",
    "        ax.set_ylabel('Mean Absolute Difference')\n",
    "    \n",
    "    # 5. Distribution of all differences\n",
    "    ax = axes[1, 1]\n",
    "    all_diffs = []\n",
    "    for diffs in layer_differences.values():\n",
    "        all_diffs.extend(diffs)\n",
    "    \n",
    "    ax.hist(all_diffs, bins=50, alpha=0.7, edgecolor='black')\n",
    "    ax.set_title('Distribution of All Differences')\n",
    "    ax.set_xlabel('Absolute Difference')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.set_yscale('log')\n",
    "    \n",
    "    # 6. Top differing layers\n",
    "    ax = axes[1, 2]\n",
    "    top_n = 10\n",
    "    layer_mean_diffs = [(layer, np.mean(diffs)) \n",
    "                       for layer, diffs in layer_differences.items()]\n",
    "    layer_mean_diffs.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    top_layers = [x[0].replace('layer_', 'L').replace('_', ' ') \n",
    "                 for x in layer_mean_diffs[:top_n]]\n",
    "    top_values = [x[1] for x in layer_mean_diffs[:top_n]]\n",
    "    \n",
    "    ax.barh(range(top_n), top_values)\n",
    "    ax.set_yticks(range(top_n))\n",
    "    ax.set_yticklabels(top_layers)\n",
    "    ax.set_title(f'Top {top_n} Layers by Mean Difference')\n",
    "    ax.set_xlabel('Mean Absolute Difference')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Summary visualization saved to {save_path}\")\n",
    "\n",
    "def run_comparison(text_input, seed=42):\n",
    "    print(f\"Processing: {text_input[:50]}...\")\n",
    "    \n",
    "    # Clear previous data\n",
    "    clear_activations()\n",
    "    remove_all_hooks()\n",
    "    \n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(text_input, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n",
    "    \n",
    "    print(f\"Input tokens: {inputs['input_ids'].shape[1]}\")\n",
    "    \n",
    "    try:\n",
    "        # Register hooks for all layers\n",
    "        print(\"Registering hooks...\")\n",
    "        hooks_1, layers_1 = register_llama_hooks(model_1, \"Model_1\")\n",
    "        hooks_2, layers_2 = register_llama_hooks(model_2, \"Model_2\")\n",
    "        \n",
    "        # Run both models\n",
    "        print(\"Running models...\")\n",
    "        with torch.no_grad():\n",
    "            outputs_1 = model_1(**inputs)\n",
    "            outputs_2 = model_2(**inputs)\n",
    "        \n",
    "        print(f\"Captured {len(activations_model_1)} activations from Model 1\")\n",
    "        print(f\"Captured {len(activations_model_2)} activations from Model 2\")\n",
    "        \n",
    "        # Select random neurons (one per layer)\n",
    "        print(\"Selecting random neurons...\")\n",
    "        selected_neurons = select_random_neurons_per_layer(activations_model_1, seed=seed)\n",
    "        \n",
    "        print(f\"Selected neurons from {len(selected_neurons)} layers\")\n",
    "        \n",
    "        # Compare activations\n",
    "        print(\"Comparing activations...\")\n",
    "        comparison_results = compare_neuron_calculations(\n",
    "            activations_model_1,\n",
    "            activations_model_2,\n",
    "            selected_neurons\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_text': text_input,\n",
    "            'tokenized_input': inputs,\n",
    "            'model_1_output': outputs_1.logits,\n",
    "            'model_2_output': outputs_2.logits,\n",
    "            'layer_comparisons': comparison_results,\n",
    "            'selected_neurons': selected_neurons\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in run_comparison: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return {\n",
    "            'input_text': text_input,\n",
    "            'error': str(e),\n",
    "            'layer_comparisons': {},\n",
    "            'selected_neurons': {}\n",
    "        }\n",
    "    \n",
    "    finally:\n",
    "        # Always cleanup hooks\n",
    "        remove_all_hooks()\n",
    "\n",
    "def save_detailed_results(comparison_results, filename=\"detailed_activation_comparison.csv\"):\n",
    "    rows = []\n",
    "    \n",
    "    for layer_name, layer_data in comparison_results['layer_comparisons'].items():\n",
    "        if 'calculations' in layer_data:\n",
    "            for calc in layer_data['calculations']:\n",
    "                row = {\n",
    "                    'input_text': comparison_results['input_text'][:100],\n",
    "                    'layer_name': layer_name,\n",
    "                    'layer_type': layer_data['layer_type'],\n",
    "                    'neuron_index': layer_data['neuron_index'],\n",
    "                    'token_position': calc['token_position'],\n",
    "                    'model_1_calculated': calc['model_1_calculated'],\n",
    "                    'model_2_calculated': calc['model_2_calculated'],\n",
    "                    'difference': calc['difference'],\n",
    "                    'abs_difference': abs(calc['difference']),\n",
    "                    'model_1_actual': calc.get('model_1_actual'),\n",
    "                    'model_2_actual': calc.get('model_2_actual')\n",
    "                }\n",
    "                rows.append(row)\n",
    "    \n",
    "    df = pd.DataFrame(rows)\n",
    "    \n",
    "    # Save to CSV\n",
    "    if os.path.exists(filename):\n",
    "        df.to_csv(filename, mode='a', header=False, index=False)\n",
    "    else:\n",
    "        df.to_csv(filename, index=False)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cbbfda25",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_TEXTS = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Artificial intelligence is transforming the world of technology.\",\n",
    "    \"In a hole in the ground there lived a hobbit.\",\n",
    "    \"To be or not to be, that is the question Shakespeare posed.\",\n",
    "    \"Machine learning models require large datasets for training.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fb31afe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing text 1/5 ===\n",
      "Processing: The quick brown fox jumps over the lazy dog....\n",
      "Input tokens: 13\n",
      "Registering hooks...\n",
      "Running models...\n",
      "Captured 290 activations from Model 1\n",
      "Captured 290 activations from Model 2\n",
      "Selecting random neurons...\n",
      "Selected neurons from 290 layers\n",
      "Comparing activations...\n",
      "Visualization saved to neuron_comparison_text_0.png\n",
      "Completed text 1\n",
      "\n",
      "=== Processing text 2/5 ===\n",
      "Processing: Artificial intelligence is transforming the world ...\n",
      "Input tokens: 13\n",
      "Registering hooks...\n",
      "Running models...\n",
      "Captured 290 activations from Model 1\n",
      "Captured 290 activations from Model 2\n",
      "Selecting random neurons...\n",
      "Selected neurons from 290 layers\n",
      "Comparing activations...\n",
      "Visualization saved to neuron_comparison_text_1.png\n",
      "Completed text 2\n",
      "\n",
      "=== Processing text 3/5 ===\n",
      "Processing: In a hole in the ground there lived a hobbit....\n",
      "Input tokens: 14\n",
      "Registering hooks...\n",
      "Running models...\n",
      "Captured 290 activations from Model 1\n",
      "Captured 290 activations from Model 2\n",
      "Selecting random neurons...\n",
      "Selected neurons from 290 layers\n",
      "Comparing activations...\n",
      "Visualization saved to neuron_comparison_text_2.png\n",
      "Completed text 3\n",
      "\n",
      "=== Processing text 4/5 ===\n",
      "Processing: To be or not to be, that is the question Shakespea...\n",
      "Input tokens: 16\n",
      "Registering hooks...\n",
      "Running models...\n",
      "Captured 290 activations from Model 1\n",
      "Captured 290 activations from Model 2\n",
      "Selecting random neurons...\n",
      "Selected neurons from 290 layers\n",
      "Comparing activations...\n",
      "Visualization saved to neuron_comparison_text_3.png\n",
      "Completed text 4\n",
      "\n",
      "=== Processing text 5/5 ===\n",
      "Processing: Machine learning models require large datasets for...\n",
      "Input tokens: 10\n",
      "Registering hooks...\n",
      "Running models...\n",
      "Captured 290 activations from Model 1\n",
      "Captured 290 activations from Model 2\n",
      "Selecting random neurons...\n",
      "Selected neurons from 290 layers\n",
      "Comparing activations...\n",
      "Visualization saved to neuron_comparison_text_4.png\n",
      "Completed text 5\n"
     ]
    }
   ],
   "source": [
    "all_results = []\n",
    "\n",
    "for i, text in enumerate(TEST_TEXTS):\n",
    "    print(f\"\\n=== Processing text {i+1}/{len(TEST_TEXTS)} ===\")\n",
    "    \n",
    "    try:\n",
    "        # Use different seed for each text to get variety\n",
    "        result = run_comparison(text, seed=42+i)\n",
    "        \n",
    "        all_results.append(result)\n",
    "        \n",
    "        # Save detailed results\n",
    "        save_detailed_results(result, filename=\"all_layers_activation_comparison.csv\")\n",
    "        \n",
    "        # Create individual visualization\n",
    "        visualize_neuron_comparisons(result['layer_comparisons'], \n",
    "                                   save_path=f'neuron_comparison_text_{i}.png')\n",
    "        \n",
    "        print(f\"Completed text {i+1}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing text {i+1}: {e}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "72d90a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary visualization saved to activation_summary_all_layers.png\n",
      "\n",
      "=== SUMMARY STATISTICS (ALL LAYERS) ===\n",
      "\n",
      "Top 10 layers by average difference:\n",
      "  lm_head: 0.910089 ± 0.295044\n",
      "  layer_31_mlp_down: 0.101140 ± 0.032076\n",
      "  layer_30_attention_v: 0.073980 ± 0.050429\n",
      "  layer_30_mlp_down: 0.072233 ± 0.035467\n",
      "  layer_28_attention_k: 0.069618 ± 0.032996\n",
      "  layer_31_attention_k: 0.068672 ± 0.016603\n",
      "  layer_25_attention_q: 0.061632 ± 0.036315\n",
      "  layer_31_attention_q: 0.059968 ± 0.030755\n",
      "  layer_24_attention_q: 0.055806 ± 0.027219\n",
      "  layer_26_attention_q: 0.052486 ± 0.014143\n",
      "\n",
      "Average differences by component type:\n",
      "  normalization: 0.001776 ± 0.002798\n",
      "  attention: 0.026334 ± 0.021456\n",
      "  mlp: 0.011337 ± 0.014316\n",
      "  output: 0.910089 ± 0.295044\n",
      "\n",
      "Overall statistics:\n",
      "  Total comparisons: 19140\n",
      "  Mean absolute difference: 0.019002\n",
      "  Std deviation: 0.071256\n",
      "  Max difference: 3.069078\n",
      "  Min difference: 0.000000\n",
      "  Median difference: 0.005303\n"
     ]
    }
   ],
   "source": [
    "if all_results:\n",
    "    create_summary_visualization(all_results, save_path='activation_summary_all_layers.png')\n",
    "\n",
    "# %%\n",
    "# Print summary statistics\n",
    "if all_results:\n",
    "    print(\"\\n=== SUMMARY STATISTICS (ALL LAYERS) ===\")\n",
    "    \n",
    "    all_layer_stats = defaultdict(list)\n",
    "    all_component_stats = defaultdict(list)\n",
    "    \n",
    "    for result in all_results:\n",
    "        for layer_name, layer_data in result['layer_comparisons'].items():\n",
    "            if 'calculations' in layer_data:\n",
    "                diffs = [abs(calc['difference']) for calc in layer_data['calculations']]\n",
    "                if diffs:\n",
    "                    mean_diff = np.mean(diffs)\n",
    "                    all_layer_stats[layer_name].append(mean_diff)\n",
    "                    all_component_stats[layer_data['layer_type']].append(mean_diff)\n",
    "    \n",
    "    print(\"\\nTop 10 layers by average difference:\")\n",
    "    layer_avg_diffs = [(layer, np.mean(diffs)) for layer, diffs in all_layer_stats.items()]\n",
    "    layer_avg_diffs.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    for layer, avg_diff in layer_avg_diffs[:10]:\n",
    "        std_diff = np.std(all_layer_stats[layer])\n",
    "        print(f\"  {layer}: {avg_diff:.6f} ± {std_diff:.6f}\")\n",
    "    \n",
    "    print(\"\\nAverage differences by component type:\")\n",
    "    for component, diffs in all_component_stats.items():\n",
    "        print(f\"  {component}: {np.mean(diffs):.6f} ± {np.std(diffs):.6f}\")\n",
    "    \n",
    "    # Calculate overall statistics\n",
    "    all_differences = []\n",
    "    for result in all_results:\n",
    "        for layer_name, layer_data in result['layer_comparisons'].items():\n",
    "            if 'calculations' in layer_data:\n",
    "                all_differences.extend([abs(calc['difference']) \n",
    "                                      for calc in layer_data['calculations']])\n",
    "    \n",
    "    if all_differences:\n",
    "        print(f\"\\nOverall statistics:\")\n",
    "        print(f\"  Total comparisons: {len(all_differences)}\")\n",
    "        print(f\"  Mean absolute difference: {np.mean(all_differences):.6f}\")\n",
    "        print(f\"  Std deviation: {np.std(all_differences):.6f}\")\n",
    "        print(f\"  Max difference: {np.max(all_differences):.6f}\")\n",
    "        print(f\"  Min difference: {np.min(all_differences):.6f}\")\n",
    "        print(f\"  Median difference: {np.median(all_differences):.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "241f7819",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " ------------------------------------------------------------\n",
      "\n",
      "Prompt: 'The quick brown fox jumps over the lazy dog.'\n",
      "------------------------------------------------------------ \n",
      "\n",
      "\n",
      "------------------------------\n",
      "Model 1: The quick brown fox jumps over the lazy dog.\n",
      "\n",
      "The quick brown fox jumps over the lazy dog.\n",
      "\n",
      "The quick brown fox jumps over the lazy dog.\n",
      "\n",
      "The quick brown fox jumps\n",
      "------------------------------\n",
      "------------------------------\n",
      "Model 2: The quick brown fox jumps over the lazy dog.\n",
      "I'm a quick brown fox, and I jumped over the lazy dog.\n",
      "I'm a quick brown fox, and I jumped over the lazy dog\n",
      "------------------------------\n",
      "\n",
      "\n",
      " ------------------------------------------------------------\n",
      "\n",
      "Prompt: 'Artificial intelligence is transforming the world of technology.'\n",
      "------------------------------------------------------------ \n",
      "\n",
      "\n",
      "------------------------------\n",
      "Model 1: Artificial intelligence is transforming the world of technology. Here are some of the most important and interesting AI trends to watch in 2023:\n",
      "1. Edge AI: As more devices become connected to the internet\n",
      "------------------------------\n",
      "------------------------------\n",
      "Model 2: Artificial intelligence is transforming the world of technology. From self-driving cars to smart assistants, AI is becoming more and more prevalent in our everyday lives. As this technology continues to advance, it’\n",
      "------------------------------\n",
      "\n",
      "\n",
      " ------------------------------------------------------------\n",
      "\n",
      "Prompt: 'In a hole in the ground there lived a hobbit.'\n",
      "------------------------------------------------------------ \n",
      "\n",
      "\n",
      "------------------------------\n",
      "Model 1: In a hole in the ground there lived a hobbit.\n",
      "Not a nasty, dirty, wet hole, filled with the ends of worms and an oozy smell, nor yet a dry, bare, sandy\n",
      "------------------------------\n",
      "------------------------------\n",
      "Model 2: In a hole in the ground there lived a hobbit. I suppose Hobbits live on a hobbit-hole; big, round holes, like tunnels, going out below the hills into the world.\n",
      "The Shire\n",
      "------------------------------\n",
      "\n",
      "\n",
      " ------------------------------------------------------------\n",
      "\n",
      "Prompt: 'To be or not to be, that is the question Shakespeare posed.'\n",
      "------------------------------------------------------------ \n",
      "\n",
      "\n",
      "------------------------------\n",
      "Model 1: To be or not to be, that is the question Shakespeare posed.\n",
      "A question we all face at one time or another, and one that has been debated for centuries.\n",
      "To be, to exist, to live, to have\n",
      "------------------------------\n",
      "------------------------------\n",
      "Model 2: To be or not to be, that is the question Shakespeare posed. And that is the question I must ask you.\n",
      "There is a growing trend among Christians, particularly among those in the emergent church movement, who are questioning\n",
      "------------------------------\n",
      "\n",
      "\n",
      " ------------------------------------------------------------\n",
      "\n",
      "Prompt: 'Machine learning models require large datasets for training.'\n",
      "------------------------------------------------------------ \n",
      "\n",
      "\n",
      "------------------------------\n",
      "Model 1: Machine learning models require large datasets for training. These datasets can be difficult to obtain, especially for rare diseases or medical conditions. In this article, we will explore the challenges of obtaining datasets for machine learning in healthcare and discuss potential\n",
      "------------------------------\n",
      "------------------------------\n",
      "Model 2: Machine learning models require large datasets for training. The quality of the data has a huge impact on the accuracy of the model. Therefore, cleaning and preprocessing the data is essential.\n",
      "The data scientists at TigerGraph developed the following\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "def generate_text(model, tokenizer, prompt, max_length=50, temperature=0.8):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs.input_ids,\n",
    "            max_length=max_length,\n",
    "            temperature=temperature,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "for prompt in TEST_TEXTS:\n",
    "    print('\\n\\n',\"-\" * 60)\n",
    "    print(f\"\\nPrompt: '{prompt}'\")\n",
    "    print(\"-\" * 60,'\\n\\n')\n",
    "    \n",
    "    # Generate from Model 1\n",
    "    gen_1 = generate_text(model_1, tokenizer, prompt, max_length=50)\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"Model 1: {gen_1}\")\n",
    "    print(\"-\" * 30)\n",
    "    # Generate from Model 2\n",
    "    gen_2 = generate_text(model_2, tokenizer, prompt, max_length=50)\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"Model 2: {gen_2}\")\n",
    "    print(\"-\" * 30)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aa40340e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "RUNNING ANALYSIS WITH DIFFERENCE TRACKING\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "Processing Query 1/5\n",
      "Text: The quick brown fox jumps over the lazy dog....\n",
      "======================================================================\n",
      "Processing: The quick brown fox jumps over the lazy dog....\n",
      "Input tokens: 13\n",
      "Registering hooks...\n",
      "Running models...\n",
      "Captured 290 activations from Model 1\n",
      "Captured 290 activations from Model 2\n",
      "Selecting random neurons...\n",
      "Selected neurons from 290 layers\n",
      "Comparing activations...\n",
      "\n",
      "Input text: The quick brown fox jumps over the lazy dog.\n",
      "\n",
      "============================================================\n",
      "Model 1 Predictions            | Model 2 Predictions           \n",
      "============================================================\n",
      "1. '\n",
      "' (0.456)                    | 1. '\n",
      "' (0.302)\n",
      "2. 'The' (0.043)                  | 2. 'The' (0.107)\n",
      "3. '' (0.039)                     | 3. '' (0.030)\n",
      "4. 'This' (0.023)                 | 4. 'This' (0.028)\n",
      "5. 'It' (0.016)                   | 5. 'I' (0.020)\n",
      "\n",
      "KL Divergence (Model1 || Model2): 0.3582\n",
      "\n",
      "==================================================\n",
      "QUERY 1 DIFFERENCE SUMMARY\n",
      "==================================================\n",
      "Total Absolute Difference: 69.58\n",
      "Number of Layers Analyzed: 290\n",
      "Number of Tokens: 13\n",
      "Average Difference per Layer: 0.24\n",
      "Average Difference per Token: 5.35\n",
      "\n",
      "Top 5 Layers with Highest Differences:\n",
      "  lm_head: 10.31\n",
      "  layer_31_mlp_down: 1.49\n",
      "  layer_30_attention_v: 1.25\n",
      "  layer_31_attention_k: 1.24\n",
      "  layer_24_attention_q: 1.16\n",
      "\n",
      "Completed Query 1\n",
      "\n",
      "======================================================================\n",
      "Processing Query 2/5\n",
      "Text: Artificial intelligence is transforming the world of technol...\n",
      "======================================================================\n",
      "Processing: Artificial intelligence is transforming the world ...\n",
      "Input tokens: 13\n",
      "Registering hooks...\n",
      "Running models...\n",
      "Captured 290 activations from Model 1\n",
      "Captured 290 activations from Model 2\n",
      "Selecting random neurons...\n",
      "Selected neurons from 290 layers\n",
      "Comparing activations...\n",
      "\n",
      "Input text: Artificial intelligence is transforming the world of technology.\n",
      "\n",
      "============================================================\n",
      "Model 1 Predictions            | Model 2 Predictions           \n",
      "============================================================\n",
      "1. 'Here' (0.643)                 | 1. 'It' (0.094)\n",
      "2. 'It' (0.056)                   | 2. 'The' (0.072)\n",
      "3. 'However' (0.034)              | 3. '\n",
      "' (0.065)\n",
      "4. 'In' (0.033)                   | 4. 'Art' (0.051)\n",
      "5. 'With' (0.031)                 | 5. 'In' (0.049)\n",
      "\n",
      "KL Divergence (Model1 || Model2): 2.3150\n",
      "\n",
      "==================================================\n",
      "QUERY 2 DIFFERENCE SUMMARY\n",
      "==================================================\n",
      "Total Absolute Difference: 73.78\n",
      "Number of Layers Analyzed: 290\n",
      "Number of Tokens: 13\n",
      "Average Difference per Layer: 0.25\n",
      "Average Difference per Token: 5.68\n",
      "\n",
      "Top 5 Layers with Highest Differences:\n",
      "  lm_head: 11.44\n",
      "  layer_18_attention_q: 1.46\n",
      "  layer_31_mlp_down: 1.40\n",
      "  layer_25_attention_q: 1.36\n",
      "  layer_30_mlp_down: 1.06\n",
      "\n",
      "Completed Query 2\n",
      "\n",
      "======================================================================\n",
      "Processing Query 3/5\n",
      "Text: In a hole in the ground there lived a hobbit....\n",
      "======================================================================\n",
      "Processing: In a hole in the ground there lived a hobbit....\n",
      "Input tokens: 14\n",
      "Registering hooks...\n",
      "Running models...\n",
      "Captured 290 activations from Model 1\n",
      "Captured 290 activations from Model 2\n",
      "Selecting random neurons...\n",
      "Selected neurons from 290 layers\n",
      "Comparing activations...\n",
      "\n",
      "Input text: In a hole in the ground there lived a hobbit.\n",
      "\n",
      "============================================================\n",
      "Model 1 Predictions            | Model 2 Predictions           \n",
      "============================================================\n",
      "1. '\n",
      "' (0.182)                    | 1. '\n",
      "' (0.153)\n",
      "2. 'Not' (0.182)                  | 2. 'Tol' (0.075)\n",
      "3. 'Tol' (0.046)                  | 3. 'Hob' (0.052)\n",
      "4. 'Not' (0.045)                  | 4. 'A' (0.036)\n",
      "5. 'There' (0.043)                | 5. 'He' (0.030)\n",
      "\n",
      "KL Divergence (Model1 || Model2): 0.8663\n",
      "\n",
      "==================================================\n",
      "QUERY 3 DIFFERENCE SUMMARY\n",
      "==================================================\n",
      "Total Absolute Difference: 70.06\n",
      "Number of Layers Analyzed: 290\n",
      "Number of Tokens: 14\n",
      "Average Difference per Layer: 0.24\n",
      "Average Difference per Token: 5.00\n",
      "\n",
      "Top 5 Layers with Highest Differences:\n",
      "  lm_head: 7.44\n",
      "  layer_30_attention_v: 2.27\n",
      "  layer_31_mlp_down: 1.94\n",
      "  layer_27_attention_q: 1.20\n",
      "  layer_24_attention_q: 1.18\n",
      "\n",
      "Completed Query 3\n",
      "\n",
      "======================================================================\n",
      "Processing Query 4/5\n",
      "Text: To be or not to be, that is the question Shakespeare posed....\n",
      "======================================================================\n",
      "Processing: To be or not to be, that is the question Shakespea...\n",
      "Input tokens: 16\n",
      "Registering hooks...\n",
      "Running models...\n",
      "Captured 290 activations from Model 1\n",
      "Captured 290 activations from Model 2\n",
      "Selecting random neurons...\n",
      "Selected neurons from 290 layers\n",
      "Comparing activations...\n",
      "\n",
      "Input text: To be or not to be, that is the question Shakespeare posed.\n",
      "\n",
      "============================================================\n",
      "Model 1 Predictions            | Model 2 Predictions           \n",
      "============================================================\n",
      "1. '\n",
      "' (0.192)                    | 1. '\n",
      "' (0.246)\n",
      "2. 'Here' (0.109)                 | 2. 'The' (0.061)\n",
      "3. 'In' (0.065)                   | 3. 'It' (0.045)\n",
      "4. 'But' (0.057)                  | 4. 'In' (0.038)\n",
      "5. 'Is' (0.052)                   | 5. 'And' (0.026)\n",
      "\n",
      "KL Divergence (Model1 || Model2): 0.5378\n",
      "\n",
      "==================================================\n",
      "QUERY 4 DIFFERENCE SUMMARY\n",
      "==================================================\n",
      "Total Absolute Difference: 96.96\n",
      "Number of Layers Analyzed: 290\n",
      "Number of Tokens: 16\n",
      "Average Difference per Layer: 0.33\n",
      "Average Difference per Token: 6.06\n",
      "\n",
      "Top 5 Layers with Highest Differences:\n",
      "  lm_head: 22.99\n",
      "  layer_31_attention_q: 1.84\n",
      "  layer_31_mlp_down: 1.65\n",
      "  layer_28_attention_q: 1.45\n",
      "  layer_22_attention_q: 1.31\n",
      "\n",
      "Completed Query 4\n",
      "\n",
      "======================================================================\n",
      "Processing Query 5/5\n",
      "Text: Machine learning models require large datasets for training....\n",
      "======================================================================\n",
      "Processing: Machine learning models require large datasets for...\n",
      "Input tokens: 10\n",
      "Registering hooks...\n",
      "Running models...\n",
      "Captured 290 activations from Model 1\n",
      "Captured 290 activations from Model 2\n",
      "Selecting random neurons...\n",
      "Selected neurons from 290 layers\n",
      "Comparing activations...\n",
      "\n",
      "Input text: Machine learning models require large datasets for training.\n",
      "\n",
      "============================================================\n",
      "Model 1 Predictions            | Model 2 Predictions           \n",
      "============================================================\n",
      "1. 'Here' (0.237)                 | 1. 'Machine' (0.170)\n",
      "2. 'In' (0.105)                   | 2. 'They' (0.069)\n",
      "3. 'However' (0.079)              | 3. 'The' (0.059)\n",
      "4. 'This' (0.074)                 | 4. 'This' (0.050)\n",
      "5. 'They' (0.071)                 | 5. '\n",
      "' (0.046)\n",
      "\n",
      "KL Divergence (Model1 || Model2): 1.1488\n",
      "\n",
      "==================================================\n",
      "QUERY 5 DIFFERENCE SUMMARY\n",
      "==================================================\n",
      "Total Absolute Difference: 53.32\n",
      "Number of Layers Analyzed: 290\n",
      "Number of Tokens: 10\n",
      "Average Difference per Layer: 0.18\n",
      "Average Difference per Token: 5.33\n",
      "\n",
      "Top 5 Layers with Highest Differences:\n",
      "  lm_head: 9.09\n",
      "  layer_30_mlp_down: 1.33\n",
      "  layer_28_attention_k: 1.30\n",
      "  layer_25_attention_q: 1.01\n",
      "  layer_18_attention_k: 0.85\n",
      "\n",
      "Completed Query 5\n",
      "Difference summary visualization saved to difference_summary.png\n",
      "\n",
      "======================================================================\n",
      "FINAL SUMMARY TABLE WITH DIFFERENCES\n",
      "======================================================================\n",
      " Query                                        Text Total_Diff Avg_Layer_Diff KL_Div Top1_Match\n",
      "     1 The quick brown fox jumps over the lazy ...      69.58           0.24 0.3582          ✓\n",
      "     2 Artificial intelligence is transforming ...      73.78           0.25 2.3150          ✗\n",
      "     3 In a hole in the ground there lived a ho...      70.06           0.24 0.8663          ✓\n",
      "     4 To be or not to be, that is the question...      96.96           0.33 0.5378          ✓\n",
      "     5 Machine learning models require large da...      53.32           0.18 1.1488          ✗\n",
      "\n",
      "Correlation plot saved to activation_output_correlation.png\n",
      "Correlation coefficient: -0.214\n",
      "\n",
      "======================================================================\n",
      "ANALYSIS COMPLETE\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "def calculate_total_differences(result):\n",
    "    total_diff = 0\n",
    "    layer_diffs = {}\n",
    "    token_diffs = {}\n",
    "    \n",
    "    for layer_name, layer_data in result['layer_comparisons'].items():\n",
    "        if 'calculations' not in layer_data or not layer_data['calculations']:\n",
    "            continue\n",
    "        \n",
    "        layer_sum = 0\n",
    "        for calc in layer_data['calculations']:\n",
    "            diff = abs(calc['difference'])\n",
    "            layer_sum += diff\n",
    "            \n",
    "            # Track per-token differences\n",
    "            token_pos = calc['token_position']\n",
    "            if token_pos not in token_diffs:\n",
    "                token_diffs[token_pos] = 0\n",
    "            token_diffs[token_pos] += diff\n",
    "        \n",
    "        layer_diffs[layer_name] = layer_sum\n",
    "        total_diff += layer_sum\n",
    "    \n",
    "    return {\n",
    "        'total_difference': total_diff,\n",
    "        'layer_differences': layer_diffs,\n",
    "        'token_differences': token_diffs,\n",
    "        'num_layers': len(layer_diffs),\n",
    "        'num_tokens': len(token_diffs)\n",
    "    }\n",
    "\n",
    "def decode_and_compare_outputs(result, tokenizer, top_k=5):\n",
    "    input_ids = result['tokenized_input']['input_ids']\n",
    "    logits_1 = result['model_1_output']\n",
    "    logits_2 = result['model_2_output']\n",
    "    \n",
    "    # Get predictions for the last token (next token prediction)\n",
    "    last_token_logits_1 = logits_1[0, -1, :]  # [vocab_size]\n",
    "    last_token_logits_2 = logits_2[0, -1, :]  # [vocab_size]\n",
    "    \n",
    "    # Get top-k predictions\n",
    "    probs_1 = torch.softmax(last_token_logits_1, dim=-1)\n",
    "    probs_2 = torch.softmax(last_token_logits_2, dim=-1)\n",
    "    \n",
    "    top_probs_1, top_indices_1 = torch.topk(probs_1, top_k)\n",
    "    top_probs_2, top_indices_2 = torch.topk(probs_2, top_k)\n",
    "    \n",
    "    # Decode tokens\n",
    "    input_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(f\"\\nInput text: {input_text}\")\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"{'Model 1 Predictions':<30} | {'Model 2 Predictions':<30}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    for i in range(top_k):\n",
    "        token_1 = tokenizer.decode(top_indices_1[i])\n",
    "        prob_1 = top_probs_1[i].item()\n",
    "        \n",
    "        token_2 = tokenizer.decode(top_indices_2[i])\n",
    "        prob_2 = top_probs_2[i].item()\n",
    "        \n",
    "        print(f\"{i+1}. '{token_1}' ({prob_1:.3f}){' '*(20-len(token_1))} | \"\n",
    "              f\"{i+1}. '{token_2}' ({prob_2:.3f})\")\n",
    "    \n",
    "    # Calculate KL divergence between distributions\n",
    "    kl_div = torch.nn.functional.kl_div(\n",
    "        torch.log(probs_2 + 1e-10), \n",
    "        probs_1, \n",
    "        reduction='sum'\n",
    "    ).item()\n",
    "    \n",
    "    print(f\"\\nKL Divergence (Model1 || Model2): {kl_div:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'top_tokens_model_1': [tokenizer.decode(idx) for idx in top_indices_1],\n",
    "        'top_probs_model_1': top_probs_1.tolist(),\n",
    "        'top_tokens_model_2': [tokenizer.decode(idx) for idx in top_indices_2],\n",
    "        'top_probs_model_2': top_probs_2.tolist(),\n",
    "        'kl_divergence': kl_div\n",
    "    }\n",
    "\n",
    "def visualize_difference_summary(all_results, save_path='difference_summary.png'):\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    fig.suptitle('Sum of Activation Differences Analysis', fontsize=16)\n",
    "    \n",
    "    # 1. Total differences per query\n",
    "    ax = axes[0, 0]\n",
    "    total_diffs = []\n",
    "    query_labels = []\n",
    "    \n",
    "    for i, result in enumerate(all_results):\n",
    "        diff_analysis = calculate_total_differences(result)\n",
    "        total_diffs.append(diff_analysis['total_difference'])\n",
    "        query_labels.append(f\"Query {i+1}\")\n",
    "    \n",
    "    bars = ax.bar(query_labels, total_diffs, color='darkblue', alpha=0.7)\n",
    "    ax.set_ylabel('Total Absolute Difference')\n",
    "    ax.set_title('Total Activation Differences per Query')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, value in zip(bars, total_diffs):\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{value:.1f}', ha='center', va='bottom')\n",
    "    \n",
    "    # 2. Average difference per layer\n",
    "    ax = axes[0, 1]\n",
    "    avg_diffs_per_layer = []\n",
    "    \n",
    "    for result in all_results:\n",
    "        diff_analysis = calculate_total_differences(result)\n",
    "        if diff_analysis['num_layers'] > 0:\n",
    "            avg_diff = diff_analysis['total_difference'] / diff_analysis['num_layers']\n",
    "            avg_diffs_per_layer.append(avg_diff)\n",
    "    \n",
    "    ax.plot(range(len(avg_diffs_per_layer)), avg_diffs_per_layer, 'ro-', markersize=8)\n",
    "    ax.set_xlabel('Query Index')\n",
    "    ax.set_ylabel('Average Difference per Layer')\n",
    "    ax.set_title('Average Layer Difference by Query')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Difference distribution across layers (for first query)\n",
    "    ax = axes[0, 2]\n",
    "    if all_results:\n",
    "        first_result = all_results[0]\n",
    "        diff_analysis = calculate_total_differences(first_result)\n",
    "        \n",
    "        # Get layer types and their differences\n",
    "        layer_types = {'norm': 0, 'mlp': 0, 'attn': 0, 'other': 0}\n",
    "        for layer_name, diff in diff_analysis['layer_differences'].items():\n",
    "            if 'norm' in layer_name:\n",
    "                layer_types['norm'] += diff\n",
    "            elif 'mlp' in layer_name:\n",
    "                layer_types['mlp'] += diff\n",
    "            elif 'attn' in layer_name:\n",
    "                layer_types['attn'] += diff\n",
    "            else:\n",
    "                layer_types['other'] += diff\n",
    "        \n",
    "        # Create pie chart\n",
    "        non_zero_types = {k: v for k, v in layer_types.items() if v > 0}\n",
    "        if non_zero_types:\n",
    "            ax.pie(non_zero_types.values(), labels=non_zero_types.keys(), \n",
    "                   autopct='%1.1f%%', startangle=90)\n",
    "            ax.set_title('Difference Distribution by Layer Type\\n(Query 1)')\n",
    "    \n",
    "    # 4. Token-wise differences (averaged across queries)\n",
    "    ax = axes[1, 0]\n",
    "    max_tokens = max(len(calculate_total_differences(r)['token_differences']) \n",
    "                     for r in all_results)\n",
    "    \n",
    "    avg_token_diffs = []\n",
    "    for token_pos in range(max_tokens):\n",
    "        token_sum = 0\n",
    "        count = 0\n",
    "        for result in all_results:\n",
    "            diff_analysis = calculate_total_differences(result)\n",
    "            if token_pos in diff_analysis['token_differences']:\n",
    "                token_sum += diff_analysis['token_differences'][token_pos]\n",
    "                count += 1\n",
    "        if count > 0:\n",
    "            avg_token_diffs.append(token_sum / count)\n",
    "        else:\n",
    "            avg_token_diffs.append(0)\n",
    "    \n",
    "    ax.bar(range(len(avg_token_diffs)), avg_token_diffs, color='green', alpha=0.7)\n",
    "    ax.set_xlabel('Token Position')\n",
    "    ax.set_ylabel('Average Absolute Difference')\n",
    "    ax.set_title('Average Difference by Token Position')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 5. Cumulative differences\n",
    "    ax = axes[1, 1]\n",
    "    for i, result in enumerate(all_results):\n",
    "        diff_analysis = calculate_total_differences(result)\n",
    "        \n",
    "        # Sort layers by name for consistent ordering\n",
    "        sorted_layers = sorted(diff_analysis['layer_differences'].items())\n",
    "        layer_names = [l[0] for l in sorted_layers]\n",
    "        layer_diffs = [l[1] for l in sorted_layers]\n",
    "        \n",
    "        # Calculate cumulative sum\n",
    "        cumulative = np.cumsum(layer_diffs)\n",
    "        \n",
    "        # Plot every 10th layer to avoid overcrowding\n",
    "        x_points = list(range(0, len(cumulative), 10))\n",
    "        y_points = [cumulative[i] for i in x_points]\n",
    "        \n",
    "        ax.plot(x_points, y_points, '-o', label=f'Query {i+1}', \n",
    "                markersize=4, alpha=0.7)\n",
    "    \n",
    "    ax.set_xlabel('Layer Index (every 10th)')\n",
    "    ax.set_ylabel('Cumulative Difference')\n",
    "    ax.set_title('Cumulative Differences Across Layers')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 6. Summary statistics table\n",
    "    ax = axes[1, 2]\n",
    "    ax.axis('off')\n",
    "    \n",
    "    summary_text = \"Summary Statistics\\n\" + \"=\"*30 + \"\\n\\n\"\n",
    "    \n",
    "    for i, result in enumerate(all_results[:5]):  # Show first 5\n",
    "        diff_analysis = calculate_total_differences(result)\n",
    "        text_preview = result['input_text'][:30] + \"...\"\n",
    "        \n",
    "        summary_text += f\"Query {i+1}: {text_preview}\\n\"\n",
    "        summary_text += f\"  Total Diff: {diff_analysis['total_difference']:.2f}\\n\"\n",
    "        summary_text += f\"  Layers: {diff_analysis['num_layers']}\\n\"\n",
    "        summary_text += f\"  Avg/Layer: {diff_analysis['total_difference']/diff_analysis['num_layers']:.2f}\\n\\n\"\n",
    "    \n",
    "    ax.text(0.05, 0.95, summary_text, transform=ax.transAxes,\n",
    "            fontsize=9, verticalalignment='top', fontfamily='monospace')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Difference summary visualization saved to {save_path}\")\n",
    "\n",
    "# Run the analysis with difference tracking\n",
    "all_results = []\n",
    "difference_summaries = []\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RUNNING ANALYSIS WITH DIFFERENCE TRACKING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for i, text in enumerate(TEST_TEXTS):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Processing Query {i+1}/{len(TEST_TEXTS)}\")\n",
    "    print(f\"Text: {text[:60]}...\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    try:\n",
    "        # Run comparison\n",
    "        result = run_comparison(text, seed=42+i)\n",
    "        \n",
    "        # Calculate differences\n",
    "        diff_analysis = calculate_total_differences(result)\n",
    "        \n",
    "        # Add output comparison\n",
    "        output_comp = decode_and_compare_outputs(result, tokenizer, top_k=5)\n",
    "        result['output_comparison'] = output_comp\n",
    "        result['difference_analysis'] = diff_analysis\n",
    "        \n",
    "        all_results.append(result)\n",
    "        difference_summaries.append(diff_analysis)\n",
    "        \n",
    "        # Print summary for this query\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"QUERY {i+1} DIFFERENCE SUMMARY\")\n",
    "        print(f\"{'='*50}\")\n",
    "        print(f\"Total Absolute Difference: {diff_analysis['total_difference']:.2f}\")\n",
    "        print(f\"Number of Layers Analyzed: {diff_analysis['num_layers']}\")\n",
    "        print(f\"Number of Tokens: {diff_analysis['num_tokens']}\")\n",
    "        print(f\"Average Difference per Layer: {diff_analysis['total_difference']/diff_analysis['num_layers']:.2f}\")\n",
    "        print(f\"Average Difference per Token: {diff_analysis['total_difference']/diff_analysis['num_tokens']:.2f}\")\n",
    "        \n",
    "        # Show top 5 layers with highest differences\n",
    "        sorted_layers = sorted(diff_analysis['layer_differences'].items(), \n",
    "                             key=lambda x: x[1], reverse=True)[:5]\n",
    "        print(f\"\\nTop 5 Layers with Highest Differences:\")\n",
    "        for layer_name, diff in sorted_layers:\n",
    "            print(f\"  {layer_name}: {diff:.2f}\")\n",
    "        \n",
    "        print(f\"\\nCompleted Query {i+1}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing query {i+1}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        continue\n",
    "\n",
    "\n",
    "# Create visualizations\n",
    "if all_results:\n",
    "    visualize_difference_summary(all_results, save_path='difference_summary.png')\n",
    "\n",
    "\n",
    "# Create a detailed comparison table with differences\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL SUMMARY TABLE WITH DIFFERENCES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "summary_data = []\n",
    "\n",
    "for i, result in enumerate(all_results):\n",
    "    if 'difference_analysis' in result and 'output_comparison' in result:\n",
    "        diff = result['difference_analysis']\n",
    "        out = result['output_comparison']\n",
    "        \n",
    "        summary_data.append({\n",
    "            'Query': i+1,\n",
    "            'Text': result['input_text'][:40] + '...',\n",
    "            'Total_Diff': f\"{diff['total_difference']:.2f}\",\n",
    "            'Avg_Layer_Diff': f\"{diff['total_difference']/diff['num_layers']:.2f}\",\n",
    "            'KL_Div': f\"{out['kl_divergence']:.4f}\",\n",
    "            'Top1_Match': '✓' if out['top_tokens_model_1'][0] == out['top_tokens_model_2'][0] else '✗'\n",
    "        })\n",
    "\n",
    "df_summary = pd.DataFrame(summary_data)\n",
    "print(df_summary.to_string(index=False))\n",
    "\n",
    "# Plot correlation between activation differences and output differences\n",
    "if len(all_results) > 1:\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "    \n",
    "    activation_diffs = [r['difference_analysis']['total_difference'] for r in all_results \n",
    "                       if 'difference_analysis' in r]\n",
    "    kl_divs = [r['output_comparison']['kl_divergence'] for r in all_results \n",
    "               if 'output_comparison' in r]\n",
    "    \n",
    "    if len(activation_diffs) == len(kl_divs):\n",
    "        ax.scatter(activation_diffs, kl_divs, s=100, alpha=0.7, c='purple')\n",
    "        \n",
    "        # Add labels for each point\n",
    "        for i, (x, y) in enumerate(zip(activation_diffs, kl_divs)):\n",
    "            ax.annotate(f'Q{i+1}', (x, y), xytext=(5, 5), \n",
    "                       textcoords='offset points', fontsize=9)\n",
    "        \n",
    "        # Add trend line\n",
    "        z = np.polyfit(activation_diffs, kl_divs, 1)\n",
    "        p = np.poly1d(z)\n",
    "        ax.plot(activation_diffs, p(activation_diffs), \"r--\", alpha=0.8, \n",
    "                label=f'Trend: y={z[0]:.4f}x+{z[1]:.4f}')\n",
    "        \n",
    "        ax.set_xlabel('Total Activation Difference')\n",
    "        ax.set_ylabel('KL Divergence')\n",
    "        ax.set_title('Correlation: Activation Differences vs Output Differences')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Calculate correlation\n",
    "        correlation = np.corrcoef(activation_diffs, kl_divs)[0, 1]\n",
    "        ax.text(0.05, 0.95, f'Correlation: {correlation:.3f}', \n",
    "                transform=ax.transAxes, fontsize=12, \n",
    "                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('activation_output_correlation.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"\\nCorrelation plot saved to activation_output_correlation.png\")\n",
    "        print(f\"Correlation coefficient: {correlation:.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ANALYSIS COMPLETE\")\n",
    "print(\"=\"*70)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
